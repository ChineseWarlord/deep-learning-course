{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with different networks and datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import torch\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, label_encoder=None):\n",
    "        # Initialize attributes\n",
    "        self.data = data[\"uuid\"]\n",
    "        self.label = data[\"status\"]\n",
    "        self.label_encoder = label_encoder\n",
    "        self.sample_rate = {}\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract audio sample from idx\n",
    "        audio_path = self.data[idx]\n",
    "\n",
    "        # Load in audio\n",
    "        audio_sample, sample_rate = torchaudio.load(audio_path)\n",
    "        self.sample_rate[idx] = sample_rate\n",
    "        #info = torchaudio.info(audio_path)\n",
    "        \n",
    "        # Extract audio label from idx and transform\n",
    "        audio_label = [self.label[idx]]\n",
    "        audio_label = self.label_encoder.transform(audio_label)\n",
    "        \n",
    "        # Check if audio sample is stereo -> convert to mono\n",
    "        if audio_sample.shape[0] > 1:\n",
    "            #print(\"Stereo shape:\", audio_sample.shape)\n",
    "            audio_sample = audio_sample.mean(dim=0, keepdim=True)\n",
    "            #print(\"after reshaping shape:\", audio_sample.shape)\n",
    "            \n",
    "        return audio_sample, torch.tensor(audio_label, dtype=torch.int64)\n",
    "            \n",
    "\n",
    "    def __get_sample_rate__(self, idx):\n",
    "        return self.sample_rate.get(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Audio\n",
    "# from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "\n",
    "# Stolen from pytorch tutorial xd\n",
    "def plot_waveform(waveform, sr, title=\"Waveform\", ax=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(num_channels, 1)\n",
    "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim([0, time_axis[-1]])\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.imshow(\n",
    "        librosa.power_to_db(specgram),\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_fbank(fbank, title=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Filter bank\")\n",
    "    axs.imshow(fbank, aspect=\"auto\")\n",
    "    axs.set_ylabel(\"frequency bin\")\n",
    "    axs.set_xlabel(\"mel bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.0)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # A data tuple has the form:\n",
    "    # waveform, label\n",
    "\n",
    "    # Separate audio samples and labels\n",
    "    waveforms, labels = zip(*batch)\n",
    "    \n",
    "    # Print the lengths of the waveforms\n",
    "    #for i, waveform in enumerate(waveforms):\n",
    "    #    print(f\"Waveform {i} length: {len(waveform)}\")\n",
    "    \n",
    "    # Pad the audio samples\n",
    "    padded_waveforms = pad_sequence(waveforms)\n",
    "\n",
    "    # Convert labels to tensor\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    #return waveforms, labels\n",
    "    return padded_waveforms, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def undersample(data, n, normalize=False):\n",
    "\n",
    "    # Step 1: Identify majority class\n",
    "\n",
    "    class_counts = data[\"status\"].value_counts()\n",
    "\n",
    "    majority_class = class_counts.idxmax()\n",
    "\n",
    "\n",
    "    # Step 2: Calculate desired class distribution (e.g., balanced distribution)\n",
    "\n",
    "    desired_class_count = n  # Target number of samples for each class\n",
    "\n",
    "\n",
    "    # Step 3: Select subset from majority class\n",
    "\n",
    "    undersampled_data_majority = data[data[\"status\"] == majority_class].sample(\n",
    "        n=desired_class_count\n",
    "    )\n",
    "\n",
    "\n",
    "    # Combine with samples from minority classes\n",
    "\n",
    "    undersampled_data_minority = data[~(data[\"status\"] == majority_class)]\n",
    "\n",
    "\n",
    "    # Combine undersampled majority class with minority classes\n",
    "\n",
    "    undersampled_data = pd.concat(\n",
    "        [undersampled_data_majority, undersampled_data_minority]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Shuffle the undersampled dataset\n",
    "\n",
    "    undersampled_data = undersampled_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return undersampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sample(data):\n",
    "    # Find class distribution\n",
    "    class_counts = data[\"status\"].value_counts()\n",
    "    # print(class_counts)\n",
    "\n",
    "    # Check class weights\n",
    "    class_weights = 1 / class_counts\n",
    "    # print(class_weights)\n",
    "\n",
    "    # Adjust weighting to each sample\n",
    "    sample_weights = [1 / class_counts[i] for i in data[\"status\"].values]\n",
    "    # print(\"len sample weights:\",len(sample_weights))\n",
    "\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_dataset(data, test_size):\n",
    "    # Extract audio samples and labels\n",
    "    X = data.drop(columns=[\"status\"])\n",
    "    y = data[\"status\"]\n",
    "\n",
    "\n",
    "    # Perform a stratified split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Combine audio samples and target labels for training and validation sets\n",
    "    train_data = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(data, normalize, title):\n",
    "    print(f\"{title} Distribution\")\n",
    "    print(data[\"status\"].value_counts(normalize=normalize))\n",
    "    print(\"Total samples\", len(data))\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"Histogram of Patient Status\\n- {title}\")\n",
    "    plt.bar(data[\"status\"].value_counts().index, data[\"status\"].value_counts())\n",
    "    plt.xticks(rotation=20, ha=\"right\", fontsize=8)\n",
    "    plt.xlabel(\"Class\", fontsize=8)\n",
    "    plt.ylabel(\"Frequency\", fontsize=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_data(data_path, data_dir_path):\n",
    "    # Read data file then remove every column other than the specified columns\n",
    "    # Removes empty samples and filters through cough probability\n",
    "    data = pd.read_csv(data_path, sep=\",\")\n",
    "    \n",
    "    data = (\n",
    "        data[[\"uuid\", \"cough_detected\", \"SNR\", \"age\", \"gender\", \"status\"]]\n",
    "        .loc[data[\"cough_detected\"] >= 0.5]\n",
    "        .dropna().reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Check if the following MP3 with uuid exists\n",
    "    mp3_data = []\n",
    "    non_exist = []\n",
    "    for file in data[\"uuid\"]:\n",
    "        if os.path.exists(os.path.join(data_dir_path, f\"{file}.mp3\")):\n",
    "            #print(\"Exists!\")\n",
    "            mp3_data.append(os.path.join(data_dir_path, f\"{file}.mp3\"))\n",
    "        else:\n",
    "            #print(\"Does not exist!\")\n",
    "            non_exist.append(file)\n",
    "        # elif os.path.exists(os.path.join(data_dir_path, f'{file}.ogg')):\n",
    "        #    ogg_data.append(os.path.join(data_dir_path, f'{file}.ogg'))\n",
    "\n",
    "    # Remove entries with missing MP3 files from the original data\n",
    "    data = data[~data[\"uuid\"].isin(non_exist)]\n",
    "\n",
    "    # Replace the uuids with the path to uuid\n",
    "    data[\"uuid\"] = mp3_data\n",
    "\n",
    "    data.to_csv(\"audio_data.csv\", index=False)\n",
    "    print(\"Finished!\")\n",
    "\n",
    "# Define data variables\n",
    "data_meta = \"metadata_compiled.csv\"\n",
    "data_dir_path = r\"../Dataset/MP3/\"\n",
    "\n",
    "# Preprocess data if you havent already\n",
    "#preprocess_data(data_meta, data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_undersampled_dataloader = DataLoader(\\n    train_undersampled_dataset,\\n    batch_size=batch,\\n    shuffle=True,\\n    num_workers=workers,\\n    collate_fn=collate_fn,\\n    pin_memory=True,\\n)\\ntest_undersampled_dataloader = DataLoader(\\n    test_undersampled_dataset,\\n    batch_size=batch,\\n    shuffle=False,\\n    num_workers=workers,\\n    collate_fn=collate_fn,\\n    pin_memory=True,\\n)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define data variables\n",
    "data_meta = \"metadata_compiled.csv\"\n",
    "data_dir_path = r\"../Dataset/MP3/\"\n",
    "#data = pd.read_csv(\"audio_data.csv\")\n",
    "data = pd.read_csv(\"audio_data2.csv\")\n",
    "\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels into encoded form\n",
    "labels = [\"healthy\", \"symptomatic\", \"COVID-19\"]\n",
    "encoded_labels = le.fit_transform(labels)\n",
    "\n",
    "# Visualize standard dataset\n",
    "# visualize_dataset(data, True, \"Standard\")\n",
    "#print(\"DATASET BEFORE PREPROCESS\\n\", data)\n",
    "\n",
    "# Prepare standard dataset\n",
    "train_data, test_data = preprocess_dataset(data, 0.33)\n",
    "#print(\"type\",type(train_data))\n",
    "#print(\"DATA\", train_data)\n",
    "#print(\"DATA ILOC\", train_data.iloc[0][\"uuid\"])\n",
    "#data1 = train_data[\"uuid\"]#.to_numpy()\n",
    "#label1 = train_data[\"status\"]#.to_numpy()\n",
    "#f = train_data[\"uuid\"]\n",
    "#l = train_data[\"status\"]\n",
    "\n",
    "#print(type(f),f)\n",
    "#print(type(l),l)\n",
    "\n",
    "#print(f[7563])\n",
    "#print(l[7563])\n",
    "\n",
    "# Create undersampled version\n",
    "#undersampled_data = undersample(data, 2000, True)\n",
    "\n",
    "# Visualize undersampled dataset\n",
    "# visualize_dataset(undersampled_data, True, \"Undersampled\")\n",
    "\n",
    "# Prepare undersampled dataset\n",
    "#train_undersampled_data, test_undersampled_data = preprocess_dataset(\n",
    "#    undersampled_data, 0.33\n",
    "#)\n",
    "\n",
    "# Preparing weighted dataset\n",
    "sample_weights = weighted_sample(data)\n",
    "weighted_Sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights, num_samples=len(data), replacement=True\n",
    ")\n",
    "\n",
    "\n",
    "# Create AudioDataset instances for training and validation sets\n",
    "\"\"\"\n",
    "We should try training with different datasets such as:\n",
    " * Standard\n",
    " * Undersampled\n",
    " * Weighted\n",
    "\n",
    "\"\"\"\n",
    "# Standard dataset\n",
    "train_dataset = AudioDataset(train_data, le)\n",
    "test_dataset = AudioDataset(test_data, le)\n",
    "\n",
    "# Undersampled dataset\n",
    "#train_undersampled_dataset = AudioDataset(train_undersampled_data, le)\n",
    "#test_undersampled_dataset = AudioDataset(test_undersampled_data, le)\n",
    "\n",
    "# Create training and test dataloader instances\n",
    "batch = 10\n",
    "workers = 0\n",
    "pin_memory = True\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "\n",
    "\"\"\"\n",
    "train_undersampled_dataloader = DataLoader(\n",
    "    train_undersampled_dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_undersampled_dataloader = DataLoader(\n",
    "    test_undersampled_dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=False,\n",
    "    num_workers=workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# train_weighted_dataloader = DataLoader(train_dataset, sampler=weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)\n",
    "# test_weighted_dataloader = DataLoader(test_dataset, sampler=weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\nimport torchaudio.transforms as T\\n\\n\\n\\n\\nprint(t := test.__getitem__(0))\\n\\n\\n\\nprint(t3 := test.__getitem__(1))\\nprint(\"len t\", len(t[0][0]))\\n\\n\\n\\nprint(\"len t3\", len(t3[0][0]))\\n\\nprint(t[0])\\n\\n\\n\\nprint(t[1])\\n\\n\\n\\n\\nprint(t2 := test.__get_sample_rate__(0))\\n\\n\\n\\n\\nprint(\"decoded label\",le.inverse_transform(t[1]))\\n\\n\\n\\n\\n# Define transform\\n\\n\\n\\nspectrogram = T.Spectrogram(n_fft=512)\\n\\n\\n\\n\\n# Perform transform\\nspec = spectrogram(t[0])\\n\\n\\n\\nfig, axs = plt.subplots(2, 1)\\n\\n\\n\\nplot_waveform(t[0], t2, title=\"Original waveform\", ax=axs[0])\\n\\n\\n\\nplot_spectrogram(spec[0], title=\"spectrogram\", ax=axs[1])\\n\\n\\n\\nfig.tight_layout()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing stuff\n",
    "#print(train_data)\n",
    "#print(test_data)\n",
    "#print(train_dataset.__getitem__(7563))\n",
    "#print(train_dataset.__len__())\n",
    "\n",
    "#train_features, train_labels = next(iter(train_dataloader))\n",
    "#print(f\"Feature batch shape: {train_features.size()}\")\n",
    "#print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(t := test.__getitem__(0))\n",
    "\n",
    "\n",
    "\n",
    "print(t3 := test.__getitem__(1))\n",
    "print(\"len t\", len(t[0][0]))\n",
    "\n",
    "\n",
    "\n",
    "print(\"len t3\", len(t3[0][0]))\n",
    "\n",
    "print(t[0])\n",
    "\n",
    "\n",
    "\n",
    "print(t[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(t2 := test.__get_sample_rate__(0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"decoded label\",le.inverse_transform(t[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define transform\n",
    "\n",
    "\n",
    "\n",
    "spectrogram = T.Spectrogram(n_fft=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform transform\n",
    "spec = spectrogram(t[0])\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "\n",
    "\n",
    "plot_waveform(t[0], t2, title=\"Original waveform\", ax=axs[0])\n",
    "\n",
    "\n",
    "\n",
    "plot_spectrogram(spec[0], title=\"spectrogram\", ax=axs[1])\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import IPython.display as ipd\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device running on: {device}\")\n",
    "\n",
    "\n",
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=3, stride=2, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_input, n_channel, kernel_size=10, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(n_channel)\n",
    "        self.pool1 = nn.MaxPool2d(4)\n",
    "        self.conv2 = nn.Conv2d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(n_channel)\n",
    "        self.pool2 = nn.MaxPool2d(4)\n",
    "        self.conv3 = nn.Conv2d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool2d(4)\n",
    "        self.conv4 = nn.Conv2d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm2d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool2d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "class ModifiedM5(nn.Module):\n",
    "    def __init__(self, n_output=3, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, n_channel, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(n_channel)\n",
    "        self.conv2 = nn.Conv2d(n_channel, n_channel, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(n_channel)\n",
    "        self.conv3 = nn.Conv2d(n_channel, 2 * n_channel, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.bn3 = nn.BatchNorm2d(2 * n_channel)\n",
    "        self.conv4 = nn.Conv2d(2 * n_channel, 2 * n_channel, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.bn4 = nn.BatchNorm2d(2 * n_channel)\n",
    "        # Adjusted input size for the fully connected layer\n",
    "        self.fc1 = nn.Linear(2 * n_channel * 2 * 372, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output of conv layers\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "#model = ModifiedM5(n_output=len(labels))\n",
    "#model.to(device)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "#n = count_parameters(model)\n",
    "#print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import MFCC\n",
    "\n",
    "# Settings for MelSpectrogram computation\n",
    "melkwargs = {\n",
    "    \"n_mels\": 80,  # How many mel frequency filters are used\n",
    "    \"n_fft\": 480,  # How many fft components are used for each feature\n",
    "    \"win_length\": 480,  # How many frames are included in each window\n",
    "    \"hop_length\": 160,  # How many frames the window is shifted for each component\n",
    "    \"center\": False,  # Whether frams are padded such that the component of timestep t is centered at t\n",
    "    \"f_max\": 7600,  # Maximum frequency to consider\n",
    "    \"f_min\": 20,\n",
    "}\n",
    "\n",
    "# Instantiate MFCC feature extractor\n",
    "mfcc = MFCC(\n",
    "    n_mfcc=40,  # Number of cepstrum components\n",
    "    sample_rate=16000,  # Sample rate of input audio\n",
    "    melkwargs=melkwargs,\n",
    ")  # Keyword arguments for MelSpectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def MFCC_Features(data):\n",
    "    features = mfcc(data)\n",
    "    features = F.pad(features, (0,6000 - features.shape[3]), \"constant\", 0).permute(0,1,2,3)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision import models\n",
    "\n",
    "#model = fasterrcnn_resnet50_fpn()\n",
    "#in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "#model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=3) \n",
    "\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 3)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "model.train()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a8cfdb96ea47769b79ec6023e5cbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 | Training:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Batch 0/85 | Loss: 0.9948\n",
      "Epoch 1/5 | Batch 20/85 | Loss: 1.8507\n",
      "Epoch 1/5 | Batch 40/85 | Loss: 0.6376\n",
      "Epoch 1/5 | Batch 60/85 | Loss: 0.7218\n",
      "Epoch 1/5 | Batch 80/85 | Loss: 0.5125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465a1ec1752442dcad0da5c04d7ab704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 | Training:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Batch 0/85 | Loss: 0.4837\n",
      "Epoch 2/5 | Batch 20/85 | Loss: 0.2486\n",
      "Epoch 2/5 | Batch 40/85 | Loss: 0.6994\n",
      "Epoch 2/5 | Batch 60/85 | Loss: 0.3873\n",
      "Epoch 2/5 | Batch 80/85 | Loss: 0.6822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa950043a09145328cecde91f0743a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 | Training:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Batch 0/85 | Loss: 0.5692\n",
      "Epoch 3/5 | Batch 20/85 | Loss: 0.5931\n",
      "Epoch 3/5 | Batch 40/85 | Loss: 0.4025\n",
      "Epoch 3/5 | Batch 60/85 | Loss: 0.6464\n",
      "Epoch 3/5 | Batch 80/85 | Loss: 0.5782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09903d9f8ae641319dedf21fef379966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 | Training:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Batch 0/85 | Loss: 0.3896\n",
      "Epoch 4/5 | Batch 20/85 | Loss: 0.7687\n",
      "Epoch 4/5 | Batch 40/85 | Loss: 0.8424\n",
      "Epoch 4/5 | Batch 60/85 | Loss: 0.4439\n",
      "Epoch 4/5 | Batch 80/85 | Loss: 0.8193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5946c5dfd54edbb48ad79fabbda007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 | Training:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Batch 0/85 | Loss: 0.9350\n",
      "Epoch 5/5 | Batch 20/85 | Loss: 0.5764\n",
      "Epoch 5/5 | Batch 40/85 | Loss: 0.6511\n",
      "Epoch 5/5 | Batch 60/85 | Loss: 0.5604\n",
      "Epoch 5/5 | Batch 80/85 | Loss: 0.5416\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Defining training variables\n",
    "epochs = 5\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=20, gamma=0.1\n",
    ")  # reduce the learning after 20 epochs by a factor of 10\n",
    "log_interval = 20\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs, targets) in tqdm(enumerate(\n",
    "       train_dataloader),\n",
    "       total=len(train_dataloader),\n",
    "       leave=True,\n",
    "       desc=f\"Epoch {epoch+1}/{epochs} | Training\",\n",
    "    ):\n",
    "        #print(f\"batch idx: {i+1} \", end='\\r')\n",
    "        features = MFCC_Features(inputs)\n",
    "        \n",
    "        #print(f\"========== BATCH {i} ========== \")\n",
    "        #for idx, f in enumerate(features):\n",
    "        #    print(f\"{idx+1} | {f.shape}\")\n",
    "        #print(f\"========== BATCH {i} ========== \")\n",
    "        \n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Batch {i}/{len(train_dataloader)} | Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        #loss = F.nll_loss(output.squeeze(), targets)\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        \n",
    "        #if i % log_interval == 0:\n",
    "        #    print(f\"Train Epoch: {epoch} [{i * len(data)}/{len(train_dataloader.dataset)} ({100. * i / len(train_dataloader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "        #losses.append(loss.item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 328/419 (78%)\n"
     ]
    }
   ],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_dataloader:\n",
    "        data = MFCC_Features(data)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "    print(f\"Accuracy: {correct}/{len(test_dataloader.dataset)} ({100. * correct / len(test_dataloader.dataset):.0f}%)\")\n",
    "    \n",
    "\n",
    "model = torch.load(\"model.pth\")\n",
    "model.to(device)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
