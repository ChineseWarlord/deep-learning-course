{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project - Covid-19 Cough Audio Classification\n",
    "\n",
    "1. Explore the dataset through code\n",
    "    * How many samples does the dataset contain?\n",
    "    * How many classes? How many samples per class? Show a histogram of the number of instances per class\n",
    "    * Play a random sample from each class\n",
    "    * Describe if/how you think the data distribution will affect training of a classifier.\n",
    "    * Decide what part of the dataset to use; all, some classes, some samples. Motivate your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load in metadata.csv\n",
    "data_path = r\"metadata_compiled.csv\"\n",
    "data = pd.read_csv(data_path, sep=\",\")\n",
    "#data.columns = data.columns.str.replace(\" \", \"\")\n",
    "\n",
    "data = data[[\"uuid\",\"cough_detected\",\"SNR\",\"age\",\"gender\",\"status\"]].loc[data['cough_detected'] >= 0.5].dropna()\n",
    "print(data[\"status\"].value_counts())\n",
    "print(\"Total samples\",len(data))\n",
    "\n",
    "data.to_csv(\"status_check.csv\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.title(\"Histogram of Patient Status\")\n",
    "plt.bar(data['status'].value_counts().index, data['status'].value_counts())\n",
    "plt.xticks(rotation=20, ha='right', fontsize=8)\n",
    "plt.xlabel('Class', fontsize=8)\n",
    "plt.ylabel('Frequency', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data[\"uuid\"][1])\n",
    "\n",
    "data_dir_path = r\"../Dataset/\"\n",
    "# glob(os.path.join(clip_dir, '*.txt')):\n",
    "t = os.listdir(data_dir_path)\n",
    "\n",
    "\"\"\"\n",
    "#data = []\n",
    "#for i in t:\n",
    "#    if i.endswith(\".webm\") or i.endswith(\".ogg\"):\n",
    "#        data.append(os.path.join(data_dir_path, i))\n",
    "\n",
    "#data = pd.DataFrame(data, columns=[\"Links\"]).to_csv(\"files.csv\", index=False)  \n",
    "\n",
    "#print(t := os.path.join(data_dir_path, data[\"uuid\"][1]+\".json\"))\n",
    "\n",
    "#print(t[0])\n",
    "#print(data[\"uuid\"][1])\n",
    "\"\"\"\n",
    "\n",
    "webm_data = []\n",
    "ogg_data = []\n",
    "c = 0\n",
    "for file in data['uuid']:\n",
    "    if os.path.exists(os.path.join(data_dir_path, f'{file}.webm')):\n",
    "        webm_data.append(os.path.join(data_dir_path, f'{file}.webm'))\n",
    "    elif os.path.exists(os.path.join(data_dir_path, f'{file}.ogg')):\n",
    "        ogg_data.append(os.path.join(data_dir_path, f'{file}.ogg'))\n",
    "    #file_path = os.path.join(data_dir_path, f\"{path}.webm\")\n",
    "    #webm_data.append(file_path)\n",
    "    #json_data = pd.read_json(file_path, orient=\"table\")\n",
    "    #print(json_data)\n",
    "  \n",
    "webm_data = pd.DataFrame(webm_data)\n",
    "webm_data.to_csv(\"webm_data.csv\")\n",
    "ogg_data = pd.DataFrame(ogg_data)\n",
    "ogg_data.to_csv(\"ogg_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, data, label_encoder = None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data = pd.read_csv(data, sep=\",\")\n",
    "        self.data = self.data[[\"uuid\",\"cough_detected\",\"SNR\",\"age\",\"gender\",\"status\"]].loc[self.data['cough_detected'] >= 0.5].dropna()\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        self.data_paths = []\n",
    "        self.labels = []\n",
    "        for file in self.data['uuid']:\n",
    "            if os.path.exists(os.path.join(self.data_dir, f'{file}.webm')):\n",
    "                self.data_paths.append(os.path.join(self.data_dir, f'{file}.webm'))\n",
    "                self.labels.append(self.data.loc[self.data[\"uuid\"] == file, \"status\"].values[0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.data_paths[idx]\n",
    "        print(audio_path)\n",
    "        if os.path.exists(audio_path):\n",
    "            print(\"exists\")\n",
    "        audio_sample,_ = torchaudio.load(audio_path, format=\"FFmpeg\")\n",
    "        \n",
    "        audio_label = self.labels[idx]\n",
    "        audio_label = self.label_encoder.transform(audio_label)\n",
    "        \n",
    "        return audio_sample, audio_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_data(data_path, data_dir_path):\n",
    "    # Read data file then remove every column other than the specified columns\n",
    "    # Removes empty samples and filters through cough probability\n",
    "    data = pd.read_csv(data_path, sep=\",\")\n",
    "    data = data[[\"uuid\",\"cough_detected\",\"SNR\",\"age\",\"gender\",\"status\"]].loc[data['cough_detected'] >= 0.5].dropna()\n",
    "    \n",
    "    webm_data = []\n",
    "    ogg_data = []\n",
    "    for file in data['uuid']:\n",
    "        if os.path.exists(os.path.join(data_dir_path, f'{file}.webm')):\n",
    "            #print(data.loc[data[\"uuid\"] == file, \"status\"].values[0])\n",
    "            label = data.loc[data[\"uuid\"] == file, \"status\"].values[0]\n",
    "            webm_data.append((os.path.join(data_dir_path, f'{file}.webm'), label))\n",
    "        elif os.path.exists(os.path.join(data_dir_path, f'{file}.ogg')):\n",
    "            ogg_data.append(os.path.join(data_dir_path, f'{file}.ogg'))\n",
    "    \n",
    "    return webm_data\n",
    "    \n",
    "data = r\"metadata_compiled.csv\"\n",
    "data_dir_path = r\"../Dataset/\"\n",
    "#data = preprocess_data(data_path, data_dir_path)\n",
    "\n",
    "#print(data)\n",
    "#print(pd.DataFrame(data))\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit([\"healthy\", \"symptomatic\", \"COVID-19\"])\n",
    "\n",
    "test = AudioDataset(data_dir_path, data, le)\n",
    "\n",
    "#print(test.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Dataset/0009eb28-d8be-4dc1-92bb-907e53bc5c7a.webm\n",
      "exists\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri ../Dataset/0009eb28-d8be-4dc1-92bb-907e53bc5c7a.webm and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(audio_path):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m audio_sample,_ \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m audio_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     30\u001b[0m audio_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_encoder\u001b[38;5;241m.\u001b[39mtransform(audio_label)\n",
      "File \u001b[1;32md:\\GitHub\\deep-learning-course\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[1;32md:\\GitHub\\deep-learning-course\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[1;34m(uri, format, backend_name)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri ../Dataset/0009eb28-d8be-4dc1-92bb-907e53bc5c7a.webm and format None."
     ]
    }
   ],
   "source": [
    "print(test.__getitem__(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
