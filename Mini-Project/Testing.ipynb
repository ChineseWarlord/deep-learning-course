{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["'\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load\\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the read-only \"../input/\" directory\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n\\nimport os\\n\\ndir = r\"/kaggle/input/covid-19-audio-classification/MP3/\"\\n#for dirname, _, filenames in os.walk(dir):\\n#    for i, filename in enumerate(filenames):\\n#        print(os.path.join(dirname, filename))\\n#        if i == 5: break\\n\\nif not os.path.exists(\"/kaggle/working/models\"):\\n    os.makedirs(\\'models\\')\\n       \\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \\n# You can also write temporary files to /kaggle/temp/, but they won\\'t be saved outside of the current session\\n'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","dir = r\"/kaggle/input/covid-19-audio-classification/MP3/\"\n","#for dirname, _, filenames in os.walk(dir):\n","#    for i, filename in enumerate(filenames):\n","#        print(os.path.join(dirname, filename))\n","#        if i == 5: break\n","\n","if not os.path.exists(\"/kaggle/working/models\"):\n","    os.makedirs('models')\n","       \n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Setup and define custom dataset class\n","\n","The custom dataset class finds each raw audio sample and corresponding label, encodes the label and returns the raw audio sample as mono-channel as well as the label.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["from pydub.silence import split_on_silence\n","\n","def remove_silence(audio_object, min_silence_ms=100, threshold_dBFS=-40, keep_silence=100, seek_step=1):\n","    # Check for loudness (DEBUGGING)\n","    #loudness_dBFS = audio_object.dBFS\n","    #print(\"Loudness (dBFS):\", loudness_dBFS)\n","    \n","    # Attempt to split and remove silence from the audio signal\n","    audio_segments = split_on_silence(audio_object, min_silence_ms, threshold_dBFS, seek_step)\n","    \n","    # Check if audio_segments is empty if yes return the original audio object as numpy array\n","    if not audio_segments:\n","        \n","        # Get the array of samples from the audio segment\n","        org_audio = np.array(audio_object.get_array_of_samples(), dtype=np.float32)\n","\n","        # Normalize the samples if needed \n","        org_audio /= np.max(np.abs(org_audio))\n","        \n","        return org_audio\n","    \n","    # Add the different audio segments together\n","    audio_processed = sum(audio_segments)\n","    \n","    # Return the samples from the processed audio, save as numpy array, and normalize it\n","    audio_processed = np.array(audio_processed.get_array_of_samples(), dtype=np.float32)\n","    audio_processed /= np.max(np.abs(audio_processed))\n","    #print(\"audio_processed\",audio_processed)\n","    #print(\"audio_processed.shape\",audio_processed.shape)\n","\n","    return audio_processed\n","   \n","def encode_age(age):\n","    # Define age mapping\n","    age_mapping = {\"child\": 0, \"teen\": 1, \"adult\": 2, \"senior\": 3}\n","    \n","    # Determine age range\n","    if age <= 12:  # Children from ages 0-12\n","        return age_mapping[\"child\"]\n","    elif age <= 19:  # Teenagers from ages 13-19\n","        return age_mapping[\"teen\"]\n","    elif age <= 50:  # Adults from ages 20-50\n","        return age_mapping[\"adult\"]\n","    else:  # Seniors (age > 50)\n","        return age_mapping[\"senior\"]"]},{"cell_type":"code","execution_count":3,"metadata":{"_kg_hide-input":false,"trusted":true},"outputs":[],"source":["from pydub import AudioSegment\n","import torchaudio\n","import torch\n","import numpy as np\n","\n","class AudioDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, args, label_encoder=None):\n","        # Initialize attributes\n","        self.data = data[\"uuid\"]\n","        self.label = data[\"status\"]\n","        self.age = data[\"age\"]\n","        self.gender = data[\"gender\"]\n","        self.SNR = data[\"SNR\"]\n","        self.label_encoder = label_encoder\n","        self.min_silence = args[0]\n","        self.threshold = args[1]\n","        self.keep_silence = args[2]\n","        self.sample_rate = {}\n","\n","    def __len__(self):\n","        return len(self.label)\n","\n","    def __getitem__(self, idx):\n","        # Extract audio sample from idx\n","        audio_path = self.data[idx]\n","        #print(\"Audio path\", audio_path)\n","        \n","        # Load in audio\n","        #audio_samples, sample_ratess = torchaudio.load(audio_path)\n","        #print(\"original signal\", audio_samples)\n","        #print(\"original signal.shape\", audio_samples.shape)\n","        #self.sample_rate[idx] = sample_rate\n","        audio_object = AudioSegment.from_file(audio_path)\n","        audio_sample = remove_silence(audio_object, self.min_silence, self.threshold, self.keep_silence)\n","        #print(\"processed signal\", audio_sample)\n","        #print(\"processed signal.shape\", audio_sample.shape)\n","        self.sample_rate[idx] = audio_object.frame_rate\n","        \n","        \n","        # Extract audio label from idx and transform\n","        audio_label = [self.label[idx]]\n","        audio_label = self.label_encoder.transform(audio_label)\n","        \n","        # Extract age, gender, and SNR from idx and encode the necessary features\n","        gender_mapping = {\"male\": 0, \"female\": 1}\n","        gender = np.array([gender_mapping[self.gender[idx]]])\n","        age = np.array(encode_age(self.age[idx]))\n","        snr = np.array([self.SNR[idx]])\n","        \n","        # Check if audio sample is stereo -> convert to mono (remove_silence already turns it into 1 channel)\n","        #if len(audio_sample.shape) > 1 and audio_sample.shape[1] > 1:\n","            # Convert stereo audio to mono\n","            #audio_sample = audio_sample.mean(dim=0, keepdim=True)\n","            \n","        return torch.tensor(audio_sample, dtype=torch.float32), torch.tensor(audio_label, dtype=torch.int64), torch.tensor(gender), torch.tensor(age), torch.tensor(snr)\n","\n","    def __get_sample_rate__(self, idx):\n","        # If needed extract sample rate\n","        return self.sample_rate.get(idx)"]},{"cell_type":"markdown","metadata":{},"source":["# Custom collate function\n","\n","The following collate function will take batches of raw audio samples and zero pad them to match the largest sized audio sample.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["def pad_sequence(batch):\n","    # Make all tensor in a batch the same length by padding with zeros\n","    batch = [item.t() for item in batch]\n","    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.0)\n","    #print(\"batch\", batch)\n","    #print(\"batch shape\", batch.shape)\n","    return batch.unsqueeze(1) # Add channel dimension for MFCC input\n","    #return batch.permute(0, 2, 1)\n","\n","\n","def collate_fn(batch):\n","    # A data tuple has the form:\n","    # waveform, label\n","\n","    # Separate audio samples and labels\n","    waveforms, labels, genders, ages, snrs = zip(*batch)\n","    \n","    # Pad the audio samples (if needed)\n","    #padded_waveforms = pad_sequence(waveforms)\n","    \n","    # Convert features and labels to tensor        \n","    labels = torch.tensor(labels)\n","    genders = torch.tensor(genders)\n","    ages = torch.tensor(ages)\n","    snrs = torch.tensor(snrs)\n","    \n","    # Extract tensors from tuples\n","    #waveforms = [item[0] for item in waveforms]\n","    #original = [item[0] for item in original]\n","    #print(\"collate waveform\",waveforms)\n","    #print(\"collate original waveform\",original)\n","\n","    # Convert to tensors\n","    #waveforms = np.array(waveforms, dtype=np.float32)\n","    #waveforms = torch.tensor(waveforms, dtype=torch.float32)\n","    #print(\"collate waveform tensor\",waveforms)\n","    #original = torch.tensor(original, dtype=torch.float32)\n","    \n","    #waveforms = torch.tensor(waveforms, dtype=torch.float32)\n","    #original = torch.tensor(original, dtype=torch.float32)\n","    \n","    #print(\"collate waveform\",waveforms)\n","    #print(\"collate waveform.shape\",waveforms.shape)\n","\n","    #print(\"collate original waveform\",original)\n","    #print(\"collate original waveform.shape\",original[0].shape)\n","    \n","    \n","    #return waveforms, labels, original\n","    return waveforms, labels, genders, ages, snrs\n","    #return padded_waveforms, labels, genders, ages, snrs"]},{"cell_type":"markdown","metadata":{},"source":["# Miscellaneous functions\n","\n","The following code block contains miscellaneous functions such as plotting of waveforms, spectograms, fbank, and preprocessing of the data.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import librosa \n","\n","def waveform_plot(signal, sr, title, threshold=None, plot=None):\n","    # Calculate time axis\n","    time = np.arange(0, len(signal)) / sr\n","    \n","    \n","\n","    plt.figure(figsize=(10, 8))\n","    \n","    # Plot standard waveform\n","    plt.subplot(3,1,1) \n","    plt.plot(time, signal, color='b')\n","    plt.xlabel('Time (s)')\n","    plt.ylabel('Amplitude')\n","    plt.title(title)\n","    plt.grid(True)\n","    plt.show()\n","    \n","    if plot:\n","        # Calculate dBFS values\n","        if np.any(signal != 0):\n","            db_signal = 20 * np.log10(np.abs(signal) / np.max(np.abs(signal)))\n","        else:\n","            db_signal = -60\n","            \n","        plt.subplot(3,1,2)\n","        # Plot waveform in dB scale\n","        plt.plot(time, db_signal, color='b')\n","\n","        # Plot threshold level\n","        if threshold:\n","            plt.axhline(y=threshold, color='r', linestyle='--', label=f'{threshold} dBFS Threshold')\n","            plt.legend()\n","\n","        plt.xlabel('Time (s)')\n","        plt.ylabel('Amplitude (dBFS)')\n","        plt.title(title)\n","        plt.grid(True)\n","\n","        n_fft = 2048  # Length of the FFT window\n","        hop_length = 512  # Hop length for FFT\n","        S = np.abs(librosa.stft(signal.astype(float), n_fft=n_fft, hop_length=hop_length))\n","\n","        # Convert amplitude to dB scale (sound pressure level)\n","        S_db = librosa.amplitude_to_db(S, ref=np.max)\n","\n","        # Get frequency bins corresponding to FFT\n","        freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n","\n","        # Step 3: Plot the SPL values over frequency\n","        plt.subplot(3,1,3)\n","        plt.plot(freqs, np.mean(S_db, axis=1), color='b')\n","        plt.title('Sound Pressure Level (SPL) vs. Frequency')\n","        plt.xlabel('Frequency (Hz)')\n","        plt.ylabel('SPL (dB)')\n","        plt.grid(True)\n","        plt.xlim([20, 25000])  # Set frequency range for better visualization\n","        plt.xscale('log')  # Use log scale for frequency axis\n","\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","# Stolen from pytorch tutorial xd\n","def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", batch=0, idx=0, ax=None):\n","    if ax is None:\n","        fig, ax = plt.subplots(1, 1)\n","    if title is not None:\n","        ax.set_title(title)\n","    ax.set_ylabel(ylabel)\n","    im = ax.imshow(\n","        librosa.power_to_db(specgram),\n","        origin=\"lower\",\n","        aspect=\"auto\",\n","        interpolation=\"nearest\",\n","    )\n","    plt.colorbar(im, ax=ax, label='dB')\n","    #plt.close()\n","    plt.savefig(f\"test_outputs/batch{batch}_idx{idx}_{title}.png\")\n","\n","def plot_fbank(fbank, title=None):\n","    fig, axs = plt.subplots(1, 1)\n","    axs.set_title(title or \"Filter bank\")\n","    axs.imshow(fbank, aspect=\"auto\")\n","    axs.set_ylabel(\"frequency bin\")\n","    axs.set_xlabel(\"mel bin\")\n","    \n","def preprocess_data(data_meta_path, data_dir_path, output_dir):\n","    # Read data file then remove every column other than the specified columns\n","    # Removes empty samples and filters through cough probability\n","    data = pd.read_csv(data_meta_path, sep=\",\")\n","    data = (\n","        data[[\"uuid\", \"cough_detected\", \"SNR\", \"age\", \"gender\", \"status\"]]\n","        .loc[data[\"cough_detected\"] >= 0.8]\n","        .dropna().reset_index(drop=True).sort_values(by='cough_detected')\n","    )\n","    data = data[(data[\"gender\"] != \"other\")]\n","    \n","    #Count the occurrences of each age value\n","    age_counts = data['age'].value_counts()\n","    \n","    # Filter out ages with fewer than 100 samples\n","    ages_to_keep = age_counts.index[age_counts >= 100]\n","\n","    # Filter the DataFrame based on the selected ages\n","    data = data[data['age'].isin(ages_to_keep)]\n","\n","    # Check if the following MP3 with uuid exists\n","    mp3_data = []\n","    non_exist = []\n","    for file in data[\"uuid\"]:\n","        if os.path.exists(os.path.join(data_dir_path, f\"{file}.mp3\")):\n","            mp3_data.append(os.path.join(data_dir_path, f\"{file}.mp3\"))\n","        else:\n","            non_exist.append(file)\n","        # elif os.path.exists(os.path.join(data_dir_path, f'{file}.ogg')):\n","        #    ogg_data.append(os.path.join(data_dir_path, f'{file}.ogg'))\n","\n","    # Remove entries with missing MP3 files from the original data\n","    data = data[~data[\"uuid\"].isin(non_exist)]\n","\n","    # Replace the uuids with the path to uuid\n","    data[\"uuid\"] = mp3_data\n","    \n","    # Save the data as csv\n","    data.to_csv(os.path.join(output_dir, \"filtered_audio_data.csv\"), index=False)\n","\n","    print(\"Finished processing!\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["'\\ndata_path = r\"misc_data/metadata_compiled.csv\"\\ndata_dir_path = r\"../Dataset/MP3/\"\\noutput_dir = r\"misc_data/\"\\npreprocess_data(data_path, data_dir_path, output_dir)\\n'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","data_path = r\"misc_data/metadata_compiled.csv\"\n","data_dir_path = r\"../Dataset/MP3/\"\n","output_dir = r\"misc_data/\"\n","preprocess_data(data_path, data_dir_path, output_dir)\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Dataset specific functions\n","\n","The following codeblock contains functions specially related to the dataset preprocessing.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","def preprocess_dataset(data, test_size):\n","    # Extract audio samples and labels\n","    X = data.drop(columns=[\"status\"])\n","    y = data[\"status\"]\n","\n","\n","    # Perform a stratified split\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=test_size, stratify=y, random_state=42\n","    )\n","\n","    # Combine audio samples and target labels for training and validation sets\n","    train_data = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n","    test_data = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n","\n","    return train_data, test_data\n","\n","def weighted_sample(data):\n","    # Find class distribution\n","    class_counts = data[\"status\"].value_counts()\n","    # print(class_counts)\n","\n","    # Check class weights\n","    class_weights = 1 / class_counts\n","    # print(class_weights)\n","\n","    # Adjust weighting to each sample\n","    sample_weights = [1 / class_counts[i] for i in data[\"status\"].values]\n","    # print(\"len sample weights:\",len(sample_weights))\n","\n","    return sample_weights\n","\n","def undersample(data, n, normalize=False):\n","    # Step 1: Identify majority class\n","    class_counts = data[\"status\"].value_counts()\n","    majority_class = class_counts.idxmax()\n","\n","    # Step 2: Calculate desired class distribution (e.g., balanced distribution)\n","    desired_class_count = n  # Target number of samples for each class\n","\n","    # Step 3: Select subset from majority class\n","    undersampled_data_majority = data[data[\"status\"] == majority_class].sample(\n","        n=desired_class_count\n","    )\n","\n","    # Combine with samples from minority classes\n","    undersampled_data_minority = data[~(data[\"status\"] == majority_class)]\n","\n","    # Combine undersampled majority class with minority classes\n","    undersampled_data = pd.concat(\n","        [undersampled_data_majority, undersampled_data_minority]\n","    )\n","\n","    # Shuffle the undersampled dataset\n","    undersampled_data = undersampled_data.sample(frac=1).reset_index(drop=True)\n","\n","    return undersampled_data\n","\n","def visualize_dataset(data, normalize, title):\n","    print(f\"{title} Distribution\")\n","    print(data[\"status\"].value_counts(normalize=normalize))\n","    print(\"Total samples\", len(data))\n","\n","    plt.figure(figsize=(6, 4))\n","    plt.title(f\"Histogram of Patient Status\\n- {title}\")\n","    plt.bar(data[\"status\"].value_counts().index, data[\"status\"].value_counts())\n","    plt.xticks(rotation=20, ha=\"right\", fontsize=8)\n","    plt.xlabel(\"Class\", fontsize=8)\n","    plt.ylabel(\"Frequency\", fontsize=8)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization of dataset and dataset loader\n","\n","This codeblock includes the initialization of the dataset as well as any processing needed, such as splitting it into training/testing datasets, as well as different sampling techniques, such as undersampling/weighted sampling.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train_dataset <__main__.AudioDataset object at 0x0000016B80134410>\n"]}],"source":["from torch.utils.data import WeightedRandomSampler\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import torch\n","import os\n","\n","# Set seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Load data\n","data = pd.read_csv(\"misc_data/filtered_audio_data.csv\")\n","#data = pd.read_csv(\"/kaggle/input/covid-19-audio-classification/audio_data.csv\")\n","#data['uuid'] = data['uuid'].apply(lambda x: x.replace('../Dataset/MP3/', \"/kaggle/input/covid-19-audio-classification/MP3/\"))\n","\n","# Initialize LabelEncoder\n","le = LabelEncoder()\n","\n","# Fit and transform labels into encoded form\n","labels = [\"healthy\", \"symptomatic\", \"COVID-19\"]\n","encoded_labels = le.fit_transform(labels)\n","\n","# Prepare standard dataset\n","train_data, test_data = preprocess_dataset(data, 0.3) # First split the original dataset into 70% training\n","val_data, test_data = preprocess_dataset(test_data, 0.5) # Second split the \"test_data\" into 50/50 validation and test (or technically 15/15)\n","\n","# Prepare and create undersampled version\n","#undersampled_data = undersample(data, 2000, True)\n","#visualize_dataset(undersampled_data, None, \"Standard\")\n","#visualize_dataset(undersampled_data, \"normalize\", \"Normalized\")\n","#train_undersampled_data, test_undersampled_data = preprocess_dataset(undersampled_data, 0.3)\n","#val_undersampled_data, test_undersampled_data = preprocess_dataset(test_undersampled_data, 0.5)\n","#visualize_dataset(train_undersampled_data, None, \"Train\")\n","#visualize_dataset(train_undersampled_data, \"normalize\", \"Train Normalized\")\n","#visualize_dataset(val_undersampled_data, None, \"Validation\")\n","#visualize_dataset(val_undersampled_data, \"normalize\", \"Validation Normalized\")\n","\n","# Prepare and create weighted sampler\n","train_sample_weights = weighted_sample(train_data)\n","val_sample_weights = weighted_sample(val_data)\n","test_sample_weights = weighted_sample(test_data)\n","\n","train_weighted_Sampler = WeightedRandomSampler(weights=train_sample_weights, num_samples=len(train_data), replacement=True)\n","val_weighted_Sampler = WeightedRandomSampler(weights=val_sample_weights, num_samples=len(val_data), replacement=True)\n","test_weighted_Sampler = WeightedRandomSampler(weights=test_sample_weights, num_samples=len(test_data), replacement=True)\n","\n","# Create AudioDataset instances for training and validation sets\n","# Standard dataset\n","min_silence = 500\n","threshold = -40\n","keep_silence = 100\n","args = [min_silence, threshold, keep_silence]\n","\n","train_dataset = AudioDataset(train_data, args, le)\n","val_dataset = AudioDataset(val_data, args, le)\n","test_dataset = AudioDataset(test_data, args, le)\n","\n","print(\"train_dataset\",train_dataset)\n","\n","# Undersampled dataset\n","#train_undersampled_dataset = AudioDataset(train_undersampled_data, le)\n","#test_undersampled_dataset = AudioDataset(test_undersampled_data, le)\n","\n","# Create training and test dataloader instances\n","batch = 8\n","workers = 0\n","pin_memory = True\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n","#val_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n","#test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n","\n","\"\"\"\n","train_undersampled_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=batch,\n","    shuffle=True,\n","    num_workers=workers,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n",")\n","val_undersampled_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=batch,\n","    shuffle=False,\n","    num_workers=workers,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n",")\n","test_undersampled_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=batch,\n","    shuffle=False,\n","    num_workers=workers,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n",")\n","\"\"\"\n","\n","train_weighted_dataloader = DataLoader(train_dataset, sampler=train_weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)\n","val_weighted_dataloader = DataLoader(val_dataset, sampler=val_weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)\n","test_weighted_dataloader = DataLoader(test_dataset, sampler=test_weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Initialize and define MFCC feature extractor\n","\n","In the following codeblock the MFCC specific parameters are defined and initialized. The codeblock also includes a function that pads the extracted MFCC features in order to pass it to the model.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["from torchaudio.transforms import MFCC\n","from torchvision.transforms import Resize\n","import torch.nn.functional as F\n","\n","def MFCC_Features(data, padding=3000, normalize=False, resize=False, batch=0):\n","    \"\"\"\n","    Args:\n","    data: Input audio waveform\n","    max_length: Maximum length for padding\n","    normalize: Normalize the channel layer\n","    resize: Resize the spectrogram\n","    target_size: Target size for resizing\n","    \"\"\"\n","    # Extract MFCC features\n","    #features = mfcc(data)\n","    #print(\"features.shape\",features.shape)\n","    #features = [mfcc(waveform) for waveform in data]\n","    features = [torch.unsqueeze(mfcc(waveform), 0) for waveform in data] # Adding channels\n","    #features = [torch.unsqueeze(torch.unsqueeze(mfcc(waveform), 0), 0) for waveform in data] # Adding batch size and channels\n","    \n","    \n","    #for feature in features:\n","    #        print(\"Feature shape after MFCC:\", feature.shape)\n","    \n","    #for i, feature in enumerate(features):\n","    #    x = torch.unsqueeze(torch.unsqueeze(feature,0),0)\n","    #    #x = torch.unsqueeze(feature,0)\n","    #    features[i] = x\n","        \n","    #for feature in features:\n","            #print(\"Feature shape after adding channels:\", feature.shape)\n","    \n","    # Hardcoded padding\n","    if padding:\n","        features = F.pad(features, (0, padding - features.shape[3]), \"constant\", 0)\n","    \n","    \n","    # Normalize the features for each sample\n","    if normalize == True:\n","        #for j, i in enumerate(features):\n","        #    plot_spectrogram(i[0], \"Before normalization\", batch = batch, idx = j)\n","        #features = (features - features.mean()) / features.std()\n","        \n","        #print(\"features.shape during normalization step\", features.shape)\n","        for j, feature in enumerate(features):\n","            mean = feature.mean(dim=[1 ,2], keepdim=True)\n","            std = feature.std(dim=[1, 2], keepdim=True)\n","            features[j] = (feature - mean) / std\n","        #features = [(feature - feature.mean()) / feature.std() for feature in features]\n","        #for j, i in enumerate(features):\n","        #    #print(\"shape after normalization i\",i.shape)\n","        #    plot_spectrogram(i[0], \"After normalization\", batch = batch, idx = j)\n","\n","    #print(\"len normal\", len(features[0]))\n","    \n","    # Add two artificial channels filled with zeros\n","    if resize == True:\n","        #artificial_channels = torch.zeros(features.shape[0], 2, features.shape[2], features.shape[3])\n","        #features = torch.cat([features, artificial_channels], dim=1)\n","        #for j, i in enumerate(features):\n","        #    plot_spectrogram(i[0], \"Before resizing\", batch = batch, idx = j)\n","        #features = Resize((224,224), antialias=True)(features)\n","        \n","        # Print out the shape of each feature tensor\n","        #for feature in features:\n","        #    print(\"Feature shape before resizing:\", feature[0].shape)\n","        \n","        features = [Resize((224, 224), antialias=True)(feature) for feature in features]\n","        #for feature in features:\n","        #    print(\"Feature shape after resizing:\", feature.shape)\n","        \n","        #for j, i in enumerate(features):\n","        #    plot_spectrogram(i[0], \"After resizing\", batch = batch, idx = j)\n","    \n","    features = torch.stack(features)\n","    #print(\"features after stacking\", features.shape)\n","    return features\n","\n","# Settings for MelSpectrogram computation\n","melkwargs = {\n","    \"n_mels\": 80,  # How many mel frequency filters are used\n","    \"n_fft\": 480,  # How many fft components are used for each feature\n","    \"win_length\": 480,  # How many frames are included in each window\n","    \"hop_length\": 160,  # How many frames the window is shifted for each component\n","    \"center\": False,  # Whether frams are padded such that the component of timestep t is centered at t\n","    \"f_max\": 11000,  # Maximum frequency to consider\n","    \"f_min\": 0,\n","}\n","\n","# Instantiate MFCC feature extractor\n","mfcc = MFCC(\n","    n_mfcc=22,  # Number of cepstrum components\n","    sample_rate=22000,  # Sample rate of input audio\n","    melkwargs=melkwargs)  # Keyword arguments for MelSpectogram"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["'\\nfrom tqdm import tqdm\\nimport warnings\\n# Settings the warnings to be ignored \\n#warnings.filterwarnings(\\'ignore\\')\\nmax_val = 0\\nfor i, (inputs, targets, gender, age, snr) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), leave=True):\\n    #inputs, labels, original = next(iter(train_dataloader))\\n    #signal = np.array(inputs[0])\\n    #labels = np.array(targets[0])\\n    #original_signal = np.array(original[0][0])\\n    #print(\"SIGNAL\",signal.shape)\\n    #print(\"ORIGINAL\", original_signal.shape)\\n    #waveform_plot(original_signal, 48000, \"Standard waveform\", threshold)\\n    #waveform_plot(signal, 48000, \"Reduced waveform\")\\n    #print(\"batch inputs.shape\", inputs.shape)\\n    #print(\"batch\",inputs)\\n    #gender_age_info = torch.cat((gender.unsqueeze(1), age.unsqueeze(1)), dim=1)\\n    features = MFCC_Features(inputs, padding=3000)\\n    # Concatenate MFCC features and gender_age_info along the channel dimension\\n    #inputs_concatenated = torch.cat((features, gender_age_info.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, features.shape[2], features.shape[3])), dim=1)\\n    gender_age_snr = torch.stack((gender, age, snr), dim=1)\\n    #print(features.shape)\\n\\n    print(\"=================\")\\n    #print(\"inputs:\",inputs)\\n    print(\"targets:\",targets)\\n    print(\"gender:\",gender)\\n    print(\"age:\",age)\\n    print(\"snr:\",snr)\\n    print(\"gender_age_snr:\", gender_age_snr)\\n    print(\"gender_age_snr.shape:\", gender_age_snr.shape)\\n    #print(\"features\", features)\\n    print(\"features.shape\", features.shape)\\n    #print(\"inputs_concatenated\", inputs_concatenated)\\n    #print(\"inputs_concatenated.shape\", inputs_concatenated.shape)\\n    print(\"=================\")\\n\\n'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","from tqdm import tqdm\n","import warnings\n","# Settings the warnings to be ignored \n","#warnings.filterwarnings('ignore')\n","max_val = 0\n","for i, (inputs, targets, gender, age, snr) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), leave=True):\n","    #inputs, labels, original = next(iter(train_dataloader))\n","    #signal = np.array(inputs[0])\n","    #labels = np.array(targets[0])\n","    #original_signal = np.array(original[0][0])\n","    #print(\"SIGNAL\",signal.shape)\n","    #print(\"ORIGINAL\", original_signal.shape)\n","    #waveform_plot(original_signal, 48000, \"Standard waveform\", threshold)\n","    #waveform_plot(signal, 48000, \"Reduced waveform\")\n","    #print(\"batch inputs.shape\", inputs.shape)\n","    #print(\"batch\",inputs)\n","    #gender_age_info = torch.cat((gender.unsqueeze(1), age.unsqueeze(1)), dim=1)\n","    features = MFCC_Features(inputs, padding=3000)\n","    # Concatenate MFCC features and gender_age_info along the channel dimension\n","    #inputs_concatenated = torch.cat((features, gender_age_info.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, features.shape[2], features.shape[3])), dim=1)\n","    gender_age_snr = torch.stack((gender, age, snr), dim=1)\n","    #print(features.shape)\n","\n","    print(\"=================\")\n","    #print(\"inputs:\",inputs)\n","    print(\"targets:\",targets)\n","    print(\"gender:\",gender)\n","    print(\"age:\",age)\n","    print(\"snr:\",snr)\n","    print(\"gender_age_snr:\", gender_age_snr)\n","    print(\"gender_age_snr.shape:\", gender_age_snr.shape)\n","    #print(\"features\", features)\n","    print(\"features.shape\", features.shape)\n","    #print(\"inputs_concatenated\", inputs_concatenated)\n","    #print(\"inputs_concatenated.shape\", inputs_concatenated.shape)\n","    print(\"=================\")\n","\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Initializing and defining model\n","\n","The following codeblock contains the initialization of the ResNet50 model from the PyTorch library.\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","from torchvision import models\n","import torch.nn as nn\n","import torch\n","\n","class MultiInputResNet18(nn.Module):\n","    def __init__(self, weights=None, num_classes=3):\n","        super(MultiInputResNet18, self).__init__()\n","        self.resnet = models.resnet18(weights=weights, num_classes=num_classes)\n","        # Adjust the first convolutional layer to match number of channels\n","        self.resnet.conv1 = nn.Conv2d(2, 64, kernel_size=(3, 3), stride=(1,1), padding=(1, 1))\n","        \n","        # Replace the last layer to adjust for new number of input features\n","        num_features = self.resnet.fc.in_features \n","        self.resnet.fc = nn.Linear(num_features, num_classes)\n","\n","    def forward(self, mfcc, gender_age_snr):\n","        print(\"Shape of mfcc:\", mfcc.shape)\n","        print(\"Shape of age_gender_snr:\", gender_age_snr.shape)\n","        #age_gender_snr = age_gender_snr.unsqueeze(2)\n","        #x = torch.cat((mfcc, gender_age_snr.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, mfcc.shape[2], mfcc.shape[3])), dim=1)\n","        x = torch.cat((mfcc, gender_age_snr.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, mfcc.shape[2], mfcc.shape[3])), dim=1)\n","        print(\"Shape of x:\", x.shape)\n","        \n","        #x = torch.cat((mfcc, age_gender_snr), dim=1)\n","        \n","        # Pass the concatenated features through the final classification layer\n","        output = self.resnet.fc(x)\n","        \n","        return output   "]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device running on: cpu\n","Only 1 GPU available!\n"]}],"source":["from torchvision import models\n","import torch.nn as nn\n","\n","# Load in the pre-trained resnet model\n","#model = models.vgg16_bn(weights=None, num_classes=3)\n","model = models.resnet18(weights=None, num_classes=3)\n","#model = models.resnet50()\n","#model = MultiInputResNet18()\n","\n","# Modifying the first layer to be able to pass 1-channel image (spectrogram) for ResNet model\n","#num_features = model.fc.in_features\n","#model.fc = nn.Linear(num_features, 3)\n","#model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1,1), padding=(1, 1)) # VGG\n","model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1,1), padding=(1, 1)) # RESNET\n","\n","# Set the model to training mode and put it on GPU if available\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","print(f\"Device running on: {device}\")\n","\n","model.to(device); # add \";\" to keep from printing the network architecture\n","\n","# Wrap your model with DataParallel if multiple GPUs are available\n","if torch.cuda.device_count() > 1:\n","    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model)\n","else:\n","    print(\"Only 1 GPU available!\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["'\\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\\nfrom torch.optim.lr_scheduler import StepLR\\nfrom torch.cuda.amp import GradScaler\\nfrom tqdm import tqdm\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nimport torchaudio\\nimport torch\\nimport os\\nepochs = 1\\nfor epoch in range(epochs):\\n    #model.train() # Initiate training mode\\n    # Training loop\\n    for i, (inputs, targets, gender, age, snr) in tqdm(enumerate(\\n       train_weighted_dataloader),\\n       total=len(train_weighted_dataloader),\\n       leave=True,\\n       desc=f\"Epoch {epoch+1}/{epochs} | Training\"\\n        ):\\n        \\n        #print(\"=================\")\\n        #print(\"inputs:\",inputs)\\n        #print(\"targets:\",targets)\\n        #print(\"gender:\",gender)\\n        #print(\"age:\",age)\\n        #print(\"snr:\",snr)\\n        \\n        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True, batch=i)\\n\\n        #print(\"features\", features)\\n        print(\"features.shape\", features.shape)\\n        #print(\"=================\")\\n        break\\n    break\\n'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","from torch.optim.lr_scheduler import StepLR\n","from torch.cuda.amp import GradScaler\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchaudio\n","import torch\n","import os\n","epochs = 1\n","for epoch in range(epochs):\n","    #model.train() # Initiate training mode\n","    # Training loop\n","    for i, (inputs, targets, gender, age, snr) in tqdm(enumerate(\n","       train_weighted_dataloader),\n","       total=len(train_weighted_dataloader),\n","       leave=True,\n","       desc=f\"Epoch {epoch+1}/{epochs} | Training\"\n","        ):\n","        \n","        #print(\"=================\")\n","        #print(\"inputs:\",inputs)\n","        #print(\"targets:\",targets)\n","        #print(\"gender:\",gender)\n","        #print(\"age:\",age)\n","        #print(\"snr:\",snr)\n","        \n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True, batch=i)\n","\n","        #print(\"features\", features)\n","        print(\"features.shape\", features.shape)\n","        #print(\"=================\")\n","        break\n","    break\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Setup weights and bias logging\n"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["'\\n# Defining weights and biases config\\nwandb.init(\\n    # set the wandb project where this run will be logged\\n    project=\"mini-project\",\\n    config={\\n    \"architecture\": \"VGG-16 Batch Normalization\",\\n    \"dataset\": \"COVID-19 Audio Classification\",\\n    \"learning_rate\": lr,\\n    \"step_size\": step,\\n    \"weight_decay\": decay,\\n    \"optimizer\": optim,\\n    \"gamma\": gamma,\\n    \"epochs\": epochs\\n    }\\n)\\n'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","import wandb\n","\n","# Initialize wandb\n","!wandb login --relogin 9be53a0c7076cae09612be80ee5e0e80d9dac79c\n","\n","\"\"\"\n","# Defining training variables\n","lr = 0.0001\n","step = 5\n","decay = 0.1\n","optim = \"adam\"\n","gamma = 0.5\n","epochs = 50\n","\n","\"\"\"\n","# Defining weights and biases config\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"mini-project\",\n","    config={\n","    \"architecture\": \"VGG-16 Batch Normalization\",\n","    \"dataset\": \"COVID-19 Audio Classification\",\n","    \"learning_rate\": lr,\n","    \"step_size\": step,\n","    \"weight_decay\": decay,\n","    \"optimizer\": optim,\n","    \"gamma\": gamma,\n","    \"epochs\": epochs\n","    }\n",")\n","\"\"\""]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["COVID-19 Healthy Symptomatic\n","[5.88954635 0.43635832 1.85696517]\n"]}],"source":["from sklearn.utils.class_weight import compute_class_weight\n","train_labels = train_data[\"status\"]\n","class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n","\n","print(\"COVID-19\",\"Healthy\", \"Symptomatic\")\n","print(class_weights)"]},{"cell_type":"markdown","metadata":{},"source":["# Training loop\n"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.0002, 0.0009, 0.0009,  ..., 0.0030, 0.0002, 0.0002],\n","       dtype=torch.float64)\n","Currently: Training\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:   0%|          | 1/747 [00:06<1:17:53,  6.27s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | Batch 0/747 | Training Loss: 1.1268\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:   3%|▎         | 21/747 [02:56<1:42:50,  8.50s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | Batch 20/747 | Training Loss: 0.8940\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:   5%|▌         | 41/747 [05:52<1:41:10,  8.60s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | Batch 40/747 | Training Loss: 0.6926\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:   8%|▊         | 61/747 [08:56<1:49:52,  9.61s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | Batch 60/747 | Training Loss: 0.6760\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:  11%|█         | 81/747 [12:04<1:46:54,  9.63s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | Batch 80/747 | Training Loss: 0.8284\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:  14%|█▎        | 101/747 [14:51<1:25:05,  7.90s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | Batch 100/747 | Training Loss: 0.8373\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:  16%|█▌        | 121/747 [18:10<2:34:09, 14.77s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | Batch 120/747 | Training Loss: 0.4848\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:  19%|█▉        | 141/747 [21:05<1:42:04, 10.11s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | Batch 140/747 | Training Loss: 0.7037\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:  22%|██▏       | 161/747 [24:20<1:33:00,  9.52s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | Batch 160/747 | Training Loss: 0.7095\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:  24%|██▍       | 181/747 [27:15<1:18:26,  8.32s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | Batch 180/747 | Training Loss: 0.6446\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:  25%|██▍       | 186/747 [28:07<1:24:49,  9.07s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets) \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#loss = F.cross_entropy(outputs, targets, reduction='mean')\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Compute gradients of the loss\u001b[39;00m\n\u001b[0;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     52\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[1;32md:\\GitHub\\deep-learning-course\\.venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\GitHub\\deep-learning-course\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from torch.optim.lr_scheduler import StepLR\n","from torch.cuda.amp import GradScaler\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchaudio\n","import torch\n","import os\n","\n","if optim == \"adam\":\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n","else: \n","    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=decay)\n","scheduler = StepLR(optimizer, step_size=step, gamma=0.5)\n","print(torch.tensor(train_sample_weights))\n","criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32))\n","#scaler = GradScaler()\n","log_interval = 20\n","best_vloss = float(\"inf\")\n","model_no = 0\n","\n","print(\"Currently: Training\")\n","for epoch in range(epochs):\n","    model.train() # Initiate training mode\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    \n","    # Training loop\n","    for i, (inputs, targets, *_) in tqdm(enumerate(\n","       train_weighted_dataloader),\n","       total=len(train_weighted_dataloader),\n","       leave=True,\n","       desc=f\"Epoch {epoch+1}/{epochs} | Training\"\n","        ):\n","        #print(f\"========== BATCH {i} ========== \")\n","        #for idx, f in enumerate(features):\n","        #    print(f\"{idx+1} | {f.shape}\")\n","        #print(f\"========== BATCH {i} ========== \")\n","        \n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True) # Compute the MFCC features\n","        features, targets = features.to(device), targets.to(device) # Load them onto GPU\n","        optimizer.zero_grad() # Zero the parameters\n","        outputs = model(features) # Retrieve the output from the model\n","        loss = criterion(outputs, targets) # Compute the loss\n","        #loss = F.cross_entropy(outputs, targets, reduction='mean')\n","        loss.backward() # Compute gradients of the loss\n","        optimizer.step() # Update weights\n","        \n","        running_loss += loss.item()\n","        \n","        # Calculate accuracy\n","        _, predicted = torch.max(outputs, 1)\n","        correct_predictions += (predicted == targets).sum().item()\n","        total_predictions += targets.size(0)\n","        \n","        if i % log_interval == 0:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i}/{len(train_weighted_dataloader)} | Training Loss: {loss.item():.4f}\")\n","    \n","    # Compute accuracy\n","    accuracy = correct_predictions / total_predictions\n","    \n","    print(f\"Training Accuracy: {accuracy:.4f}\")\n","    \n","    # Compute average training loss for the epoch\n","    avg_loss = running_loss / len(train_weighted_dataloader)\n","        \n","    # Validation loop\n","    running_vloss = 0.0\n","    vcorrect_predictions = 0\n","    vtotal_predictions = 0\n","    model.eval()\n","    with torch.no_grad(): # Disable gradient computation\n","        for j, (vinputs, vtargets, *_) in tqdm(enumerate(\n","            val_weighted_dataloader),\n","            total=len(val_weighted_dataloader),\n","            leave=True,\n","            desc=f\"Epoch {epoch+1}/{epochs} | Validating\"):\n","            vfeatures = MFCC_Features(vinputs) # Compute the MFCC features\n","            vfeatures, vtargets = vfeatures.to(device), vtargets.to(device) # Load them onto GPU\n","            voutputs = model(vfeatures)\n","            vloss = criterion(voutputs.to(device), vtargets)\n","            #vloss = F.cross_entropy(voutputs, vtargets, reduction='mean')\n","            running_vloss += vloss.item()\n","            \n","            # Calculate accuracy\n","            _, vpredicted = torch.max(voutputs, 1)\n","            vcorrect_predictions += (vpredicted == vtargets).sum().item()\n","            vtotal_predictions += vtargets.size(0)\n","    \n","    # Compute average validation loss for the epoch\n","    avg_vloss = running_vloss / len(val_weighted_dataloader)\n","    \n","    # Compute accuracy\n","    vaccuracy = vcorrect_predictions / vtotal_predictions\n","    \n","    # Compute precision, recall, F1 score\n","    precision = precision_score(vtargets.cpu(), vpredicted.cpu(), average='macro',zero_division=0.0)\n","    recall = recall_score(vtargets.cpu(), vpredicted.cpu(), average='macro',zero_division=0.0)\n","    f1 = f1_score(vtargets.cpu(), vpredicted.cpu(), average='macro',zero_division=0.0)\n","    \n","    # Log metrics to wandb\n","    #wandb.log({\"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n","    #wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_loss,\"train_acc\": accuracy, \"val_loss\": avg_vloss, \"val_accuracy\": vaccuracy})\n","    \n","    print(f\"Epoch #{epoch+1} | Training Loss: {avg_loss:.4f} | Validation Loss: {avg_vloss:.4f} | Validation Accuracy: {vaccuracy:.4f}\\n           Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n","                      \n","    # Update learning rate\n","    #print_lr(is_verbose, group, lr, epoch=None)\n","    scheduler.step()\n","    \n","    print(f\"Epoch {epoch+1}, Learning Rate: {scheduler.get_last_lr()}\")\n","    \n","    # Track best performance, and save the model's state\n","    if avg_vloss < best_vloss:\n","        best_vloss = avg_vloss\n","        model_no += 1\n","        if not os.path.exists(\"/kaggle/working/models\"):\n","            os.makedirs('models')\n","        model_path = f\"/kaggle/working/models/VGG16_bn_weighted_model_no_{model_no}_epoch_{epoch+1}.pth\"\n","        torch.save(model.state_dict(), model_path)\n","\n","# Finish the run\n","#wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4560779,"sourceId":7791391,"sourceType":"datasetVersion"},{"datasetId":4561341,"sourceId":7792152,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
