diff --git a/Mini-Project/BASELINE.ipynb b/Mini-Project/BASELINE.ipynb
index 30cae3d..b9e5729 100644
--- a/Mini-Project/BASELINE.ipynb
+++ b/Mini-Project/BASELINE.ipynb
@@ -1 +1 @@
-{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importing necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:36:21.525640Z","iopub.status.busy":"2024-03-21T19:36:21.524945Z","iopub.status.idle":"2024-03-21T19:36:26.944309Z","shell.execute_reply":"2024-03-21T19:36:26.943519Z","shell.execute_reply.started":"2024-03-21T19:36:21.525603Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Torch-related imports\n","from torch.utils.data import WeightedRandomSampler, DataLoader\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim.lr_scheduler import StepLR\n","from torchvision.transforms import Resize\n","from torchaudio.transforms import MFCC\n","from torch.cuda.amp import GradScaler\n","from torchvision import models\n","from torchinfo import summary\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn as nn\n","import torchaudio\n","import torch\n","\n","# Sklearn-related imports\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.utils.class_weight as class_weight\n","import sklearn.model_selection as model_selection\n","import sklearn.preprocessing as preprocessing\n","import sklearn.metrics as metrics\n","\n","# Audio processing imports\n","from pydub.silence import split_on_silence\n","from pydub import AudioSegment\n","import librosa\n","\n","# Miscellaneous imports\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import wandb\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset, dataloader, and model parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:36:26.946355Z","iopub.status.busy":"2024-03-21T19:36:26.946043Z","iopub.status.idle":"2024-03-21T19:36:26.987132Z","shell.execute_reply":"2024-03-21T19:36:26.986282Z","shell.execute_reply.started":"2024-03-21T19:36:26.946327Z"},"trusted":true},"outputs":[],"source":["# Set seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Path to data\n","data = pd.read_csv(\"/kaggle/input/filtered-csv/filtered_audio_data.csv\")\n","data[\"uuid\"] = data[\"uuid\"].str.replace(\"../Dataset/MP3/\", \"/kaggle/input/covid-19-audio-classification/MP3/\")\n","#data = pd.read_csv(\"misc_data/filtered_audio_data.csv\")\n","\n","# Class labels\n","labels = [\"healthy\", \"symptomatic\", \"COVID-19\"]\n","\n","# Silence arguments\n","min_silence = 500\n","threshold_dBFS = -40\n","keep_silence = 300\n","\n","# Dataloader and dataset arguments\n","batch = 16\n","workers = 4\n","pin_memory = True\n","dataset_type = \"undersampled\" # weighted or unsampled\n","undersampling = 500\n","\n","# Settings for MelSpectrogram computation\n","melkwargs = {\n","    \"n_mels\": 60,  # How many mel frequency filters are used\n","    \"n_fft\": 350,  # How many fft components are used for each feature\n","    \"win_length\": 350,  # How many frames are included in each window\n","    \"hop_length\": 100,  # How many frames the window is shifted for each component\n","    \"center\": False,  # Whether frams are padded such that the component of timestep t is centered at t\n","    \"f_max\": 11000,  # Maximum frequency to consider\n","    \"f_min\": 0,\n","}\n","n_mfcc = 22\n","sample_rate = 22000\n","\n","# Model type\n","\"\"\"\n","Models:\n","    \"resnet18\":\n","    \"resnet34\":\n","    \"resnet50\":\n","    \"vgg_bn\":\n","    \"multi_resnet\":\n","    \"modified_multi_resnet\"\n","    \"modified_multi_resnet_spectral\"\n","\"\"\"\n","model_type = \"modified_multi_resnet_spectral\"\n","model_arch = \"resnet18\"\n","model_output = \"modified_multi_resnet18_spectral_undersampled\"\n","\n","# Model training and Weights and Biases variables\n","lr = 0.0001\n","step = 3 # steplr\n","decay = 0.001 # L2 regularization\n","gamma = 0.5 # steplr\n","optimizer_type = \"adam\" # adam or sgd\n","scheduler_type = \"steplr\" # stepplateau or steplr\n","factor = 0.1 # stepplateau\n","patience = 2 # stepplateau\n","thresh_plat = 0.001 # stepplateau\n","cooldown = 0 # stepplateau\n","epochs = 25\n","ID = \"Simon\"\n","runName = \"Test run 1\"\n","arch = \"Multi Input ResNet18 Spectral\"\n","desc = \"This model is trained on MFCC features, numeric features, and spectral features.\"\n","dataset = \"COVID-19 Audio Classification\"\n","weighted = False"]},{"cell_type":"markdown","metadata":{},"source":["# Setup weights and biases logging"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:36:26.988552Z","iopub.status.busy":"2024-03-21T19:36:26.988249Z","iopub.status.idle":"2024-03-21T19:37:05.266978Z","shell.execute_reply":"2024-03-21T19:37:05.265933Z","shell.execute_reply.started":"2024-03-21T19:36:26.988527Z"},"trusted":true},"outputs":[],"source":["#Initialize wandb\n","!wandb login --relogin 9be53a0c7076cae09612be80ee5e0e80d9dac79c\n","\n","#Defining weights and biases config\n","wandb.init(\n","   # set the wandb project where this run will be logged\n","   project=\"Final-Mini-Project\",\n","   name=runName,\n","   notes=desc,\n","   config={\n","   \"ID\": ID,\n","   \"architecture\": arch,\n","   \"dataset\": dataset,\n","   \"learning_rate\": lr,\n","   \"step_size\": step,\n","   \"weight_decay\": decay,\n","   \"optimizer\": optimizer_type,\n","   \"scheduler\": scheduler_type,\n","   \"gamma\": gamma,\n","   \"epochs\": epochs,\n","   \"factor\": factor,\n","   \"threshold\": thresh_plat,\n","   \"patience\": patience,\n","   \"cooldown\": cooldown\n","   }\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Setup and define custom dataset class\n","\n","The custom dataset class finds each raw audio sample and corresponding label, encodes the label and returns the raw audio sample as mono-channel as well as the label."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:05.269727Z","iopub.status.busy":"2024-03-21T19:37:05.268990Z","iopub.status.idle":"2024-03-21T19:37:05.871653Z","shell.execute_reply":"2024-03-21T19:37:05.870666Z","shell.execute_reply.started":"2024-03-21T19:37:05.269692Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def remove_silence(audio_object, min_silence_ms=100, threshold_dBFS=-40, keep_silence=100, seek_step=1):\n","    # Check for loudness (DEBUGGING)\n","    # loudness_dBFS = audio_object.dBFS\n","    # print(\"Loudness (dBFS):\", loudness_dBFS)\n","\n","    # Attempt to split and remove silence from the audio signal\n","    audio_segments = split_on_silence(audio_object, min_silence_ms, threshold_dBFS, keep_silence, seek_step)\n","\n","    # Check if audio_segments is empty if yes return the original audio object as numpy array\n","    if not audio_segments:\n","\n","        # Get the array of samples from the audio segment\n","        org_audio = np.array(audio_object.get_array_of_samples(), dtype=np.float32)\n","\n","        # Normalize the samples if needed\n","        org_audio /= np.max(np.abs(org_audio))\n","\n","        return org_audio\n","\n","    # Add the different audio segments together\n","    audio_processed = sum(audio_segments)\n","\n","    # Return the samples from the processed audio, save as numpy array, and normalize it\n","    audio_processed = np.array(audio_processed.get_array_of_samples(), dtype=np.float32)\n","    audio_processed /= np.max(np.abs(audio_processed))\n","\n","    return audio_processed\n","\n","\n","def encode_age(age):\n","    # Define age mapping\n","    age_mapping = {\"child\": 0, \"teen\": 1, \"adult\": 2, \"senior\": 3}\n","\n","    # Determine age range\n","    if age <= 12:  # Children from ages 0-12\n","        return age_mapping[\"child\"]\n","    elif age <= 19:  # Teenagers from ages 13-19\n","        return age_mapping[\"teen\"]\n","    elif age <= 50:  # Adults from ages 20-50\n","        return age_mapping[\"adult\"]\n","    else:  # Seniors (age > 50)\n","        return age_mapping[\"senior\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-03-21T19:37:05.874599Z","iopub.status.busy":"2024-03-21T19:37:05.874274Z","iopub.status.idle":"2024-03-21T19:37:06.561002Z","shell.execute_reply":"2024-03-21T19:37:06.559863Z","shell.execute_reply.started":"2024-03-21T19:37:05.874572Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class AudioDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, args, label_encoder=None):\n","        # Initialize attributes\n","        self.data = data[\"uuid\"]\n","        self.label = data[\"status\"]\n","        self.age = data[\"age\"]\n","        self.gender = data[\"gender\"]\n","        self.SNR = data[\"SNR\"]\n","        self.label_encoder = label_encoder\n","        self.min_silence = args[0]\n","        self.threshold = args[1]\n","        self.keep_silence = args[2]\n","        self.sample_rate = {}\n","\n","    def __len__(self):\n","        return len(self.label)\n","\n","    def __getitem__(self, idx):\n","        # Extract audio sample from idx\n","        audio_path = self.data[idx]\n","\n","        # Load in audio\n","        audio_object = AudioSegment.from_file(audio_path)\n","        audio_sample = remove_silence(audio_object, self.min_silence, self.threshold, self.keep_silence)\n","        self.sample_rate[idx] = audio_object.frame_rate\n","\n","        # Extract audio label from idx and transform\n","        audio_label = [self.label[idx]]\n","        audio_label = self.label_encoder.transform(audio_label)\n","\n","        # Extract age, gender, and SNR from idx and encode the necessary features\n","        gender_mapping = {\"male\": 0, \"female\": 1}\n","        gender = np.array([gender_mapping[self.gender[idx]]], dtype=np.int8)\n","        age = np.array(encode_age(self.age[idx]), dtype=np.int8)\n","        snr = np.array([self.SNR[idx]])\n","\n","        # Check if audio sample is stereo -> convert to mono (remove_silence already turns it into 1 channel)\n","        # if len(audio_sample.shape) > 1 and audio_sample.shape[1] > 1:\n","        # Convert stereo audio to mono\n","        # audio_sample = audio_sample.mean(dim=0, keepdim=True)\n","\n","        return (\n","            torch.tensor(audio_sample, dtype=torch.float32),\n","            torch.tensor(audio_label, dtype=torch.int32),\n","            torch.tensor(gender, dtype=torch.int32),\n","            torch.tensor(age, dtype=torch.int32),\n","            torch.tensor(snr, dtype=torch.float32),\n","        )\n","\n","    def __get_sample_rate__(self, idx):\n","        # If needed extract sample rate\n","        return self.sample_rate.get(idx)"]},{"cell_type":"markdown","metadata":{},"source":["# Custom collate function\n","\n","The following collate function will take batches of raw audio samples and zero pad them to match the largest sized audio sample."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:06.562571Z","iopub.status.busy":"2024-03-21T19:37:06.562243Z","iopub.status.idle":"2024-03-21T19:37:07.414253Z","shell.execute_reply":"2024-03-21T19:37:07.413057Z","shell.execute_reply.started":"2024-03-21T19:37:06.562538Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def pad_sequence(batch):\n","    # Make all tensor in a batch the same length by padding with zeros\n","    batch = [item.t() for item in batch]\n","    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.0)\n","\n","    return batch.unsqueeze(1)  # Add channel dimension for MFCC input\n","\n","\n","def collate_fn(batch):\n","    # A data tuple has the form:\n","    # waveform, label\n","\n","    # Separate audio samples and labels\n","    waveforms, labels, genders, ages, snrs = zip(*batch)\n","\n","    # Pad the audio samples (if needed)\n","    # padded_waveforms = pad_sequence(waveforms)\n","\n","    # Convert labels to tensor\n","    labels = torch.tensor(labels, dtype=torch.int32)\n","\n","    # Stack numeric features into a tensor and normalize them (if needed)\n","    # scaler = StandardScaler()\n","    genders = torch.tensor(genders)\n","    ages = torch.tensor(ages)\n","    snrs = torch.tensor(snrs)\n","    numeric_features = torch.stack((genders, ages, snrs), dim=1)\n","    # numeric_features = torch.tensor(scaler.fit_transform(numeric_features))\n","\n","    return waveforms, labels, numeric_features"]},{"cell_type":"markdown","metadata":{},"source":["# Miscellaneous functions\n","\n","The following code block contains miscellaneous functions such as plotting of waveforms, spectograms, fbank, and preprocessing of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-03-21T19:37:07.416712Z","iopub.status.busy":"2024-03-21T19:37:07.416106Z","iopub.status.idle":"2024-03-21T19:37:08.086465Z","shell.execute_reply":"2024-03-21T19:37:08.085520Z","shell.execute_reply.started":"2024-03-21T19:37:07.416674Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def waveform_plot(signal, sr, title, threshold=None, plot=None):\n","    # Calculate time axis\n","    time = np.arange(0, len(signal)) / sr\n","\n","    # Plot standard waveform\n","    plt.figure(figsize=(10, 8))\n","    plt.subplot(3, 1, 1)\n","    plt.plot(time, signal, color=\"b\")\n","    plt.xlabel(\"Time (s)\")\n","    plt.ylabel(\"Amplitude\")\n","    plt.title(title)\n","    plt.grid(True)\n","    plt.show()\n","\n","    if plot:\n","        # Calculate dBFS values\n","        if np.any(signal != 0):\n","            db_signal = 20 * np.log10(np.abs(signal) / np.max(np.abs(signal)))\n","        else:\n","            db_signal = -60\n","\n","        plt.subplot(3, 1, 2)\n","        # Plot waveform in dB scale\n","        plt.plot(time, db_signal, color=\"b\")\n","\n","        # Plot threshold level\n","        if threshold:\n","            plt.axhline(y=threshold, color=\"r\", linestyle=\"--\", label=f\"{threshold} dBFS Threshold\")\n","            plt.legend()\n","\n","        plt.xlabel(\"Time (s)\")\n","        plt.ylabel(\"Amplitude (dBFS)\")\n","        plt.title(title)\n","        plt.grid(True)\n","\n","        n_fft = 2048  # Length of the FFT window\n","        hop_length = 512  # Hop length for FFT\n","        S = np.abs(librosa.stft(signal.astype(float), n_fft=n_fft, hop_length=hop_length))\n","\n","        # Convert amplitude to dB scale (sound pressure level)\n","        S_db = librosa.amplitude_to_db(S, ref=np.max)\n","\n","        # Get frequency bins corresponding to FFT\n","        freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n","\n","        # Step 3: Plot the SPL values over frequency\n","        plt.subplot(3, 1, 3)\n","        plt.plot(freqs, np.mean(S_db, axis=1), color=\"b\")\n","        plt.title(\"Sound Pressure Level (SPL) vs. Frequency\")\n","        plt.xlabel(\"Frequency (Hz)\")\n","        plt.ylabel(\"SPL (dB)\")\n","        plt.grid(True)\n","        plt.xlim([20, 25000])  # Set frequency range for better visualization\n","        plt.xscale(\"log\")  # Use log scale for frequency axis\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","\n","# Stolen from pytorch tutorial xd\n","def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", batch=0, idx=0, ax=None):\n","    if ax is None:\n","        fig, ax = plt.subplots(1, 1)\n","    if title is not None:\n","        ax.set_title(title)\n","    ax.set_ylabel(ylabel)\n","    im = ax.imshow(\n","        librosa.power_to_db(specgram),\n","        origin=\"lower\",\n","        aspect=\"auto\",\n","        interpolation=\"nearest\",\n","    )\n","    plt.colorbar(im, ax=ax, label=\"dB\")\n","    # plt.close()\n","    plt.savefig(f\"test_outputs/batch{batch}_idx{idx}_{title}.png\")\n","\n","\n","def plot_fbank(fbank, title=None):\n","    fig, axs = plt.subplots(1, 1)\n","    axs.set_title(title or \"Filter bank\")\n","    axs.imshow(fbank, aspect=\"auto\")\n","    axs.set_ylabel(\"frequency bin\")\n","    axs.set_xlabel(\"mel bin\")\n","\n","\n","def preprocess_data(data_meta_path, data_dir_path, output_dir):\n","    # Read data file then remove every column other than the specified columns\n","    # Removes empty samples and filters through cough probability\n","    data = pd.read_csv(data_meta_path, sep=\",\")\n","    data = data[[\"uuid\", \"cough_detected\", \"SNR\", \"age\", \"gender\", \"status\"]].loc[data[\"cough_detected\"] >= 0.8].dropna().reset_index(drop=True).sort_values(by=\"cough_detected\")\n","    data = data[(data[\"gender\"] != \"other\")]\n","\n","    # Count the occurrences of each age value\n","    age_counts = data[\"age\"].value_counts()\n","\n","    # Filter out ages with fewer than 100 samples\n","    ages_to_keep = age_counts.index[age_counts >= 100]\n","\n","    # Filter the DataFrame based on the selected ages\n","    data = data[data[\"age\"].isin(ages_to_keep)]\n","\n","    # Check if the following MP3 with uuid exists\n","    mp3_data = []\n","    non_exist = []\n","    for file in data[\"uuid\"]:\n","        if os.path.exists(os.path.join(data_dir_path, f\"{file}.mp3\")):\n","            mp3_data.append(os.path.join(data_dir_path, f\"{file}.mp3\"))\n","        else:\n","            non_exist.append(file)\n","\n","    # Remove entries with missing MP3 files from the original data\n","    data = data[~data[\"uuid\"].isin(non_exist)]\n","\n","    # Replace the uuids with the path to uuid\n","    data[\"uuid\"] = mp3_data\n","\n","    # Save the data as csv\n","    data.to_csv(os.path.join(output_dir, \"filtered_audio_data.csv\"), index=False)\n","\n","    print(\"Finished processing!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:08.088269Z","iopub.status.busy":"2024-03-21T19:37:08.087907Z","iopub.status.idle":"2024-03-21T19:37:08.871044Z","shell.execute_reply":"2024-03-21T19:37:08.869898Z","shell.execute_reply.started":"2024-03-21T19:37:08.088232Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["\"\"\"\n","data_path = r\"misc_data/metadata_compiled.csv\"\n","data_dir_path = r\"../Dataset/MP3/\"\n","output_dir = r\"misc_data/\"\n","preprocess_data(data_path, data_dir_path, output_dir)\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Dataset specific functions\n","\n","The following codeblock contains functions specially related to the dataset preprocessing."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:08.874453Z","iopub.status.busy":"2024-03-21T19:37:08.873328Z","iopub.status.idle":"2024-03-21T19:37:09.651045Z","shell.execute_reply":"2024-03-21T19:37:09.650151Z","shell.execute_reply.started":"2024-03-21T19:37:08.874422Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def preprocess_dataset(data, test_size):\n","    # Extract audio samples and labels\n","    X = data.drop(columns=[\"status\"])\n","    y = data[\"status\"]\n","\n","    # Perform a stratified split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n","\n","    # Combine audio samples and target labels for training and validation sets\n","    train_data = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n","    test_data = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n","\n","    return train_data, test_data\n","\n","\n","def weighted_sample(data):\n","    # Find class distribution\n","    class_counts = data[\"status\"].value_counts()\n","\n","    # Adjust weighting to each sample\n","    sample_weights = [1 / class_counts[i] for i in data[\"status\"].values]\n","\n","    return sample_weights\n","\n","\n","def undersample(data, minority_class_label, n):\n","    # Identify minority class\n","    minority_class = minority_class_label\n","\n","    # Calculate desired class distribution (e.g., balanced distribution)\n","    desired_class_count = n  # Target number of samples for each class\n","\n","    # Select subset from minority class\n","    undersampled_data_minority = data[data[\"status\"] == minority_class].sample(n=desired_class_count)\n","\n","    # Combine with samples from majority classes\n","    undersampled_data_majority = data[~(data[\"status\"] == minority_class)]\n","\n","    # Combine undersampled minority class with majority classes\n","    undersampled_data = pd.concat([undersampled_data_majority, undersampled_data_minority])\n","\n","    # Shuffle the undersampled dataset\n","    undersampled_data = undersampled_data.sample(frac=1).reset_index(drop=True)\n","\n","    return undersampled_data\n","\n","\n","def visualize_dataset(data, normalize, title, feature=\"status\"):\n","    print(f\"{title} Distribution\")\n","    print(data[feature].value_counts(normalize=normalize))\n","    print(\"Total samples\", len(data))\n","\n","    plt.figure(figsize=(6, 4))\n","    plt.title(f\"Histogram of Patient Status\\n- {title}\")\n","    plt.bar(data[feature].value_counts().index, data[feature].value_counts())\n","    plt.xticks(rotation=20, ha=\"right\", fontsize=8)\n","    plt.xlabel(\"Class\", fontsize=8)\n","    plt.ylabel(\"Frequency\", fontsize=8)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization of dataset and dataset loader\n","\n","This codeblock includes the initialization of the dataset as well as any processing needed, such as splitting it into training/testing datasets, as well as different sampling techniques, such as undersampling/weighted sampling."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:09.653258Z","iopub.status.busy":"2024-03-21T19:37:09.652883Z","iopub.status.idle":"2024-03-21T19:37:10.426293Z","shell.execute_reply":"2024-03-21T19:37:10.425310Z","shell.execute_reply.started":"2024-03-21T19:37:09.653223Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def select_dataset(dataset_type=\"undersampled\", train_data=None, val_data=None, test_data=None, args=None, batch=None, workers=None, undersampling=500):\n","    if dataset_type == \"undersampled\":\n","        # Prepare and create undersampled version\n","        undersampled_data = undersample(data, \"healthy\", undersampling)\n","        undersampled_data = undersample(undersampled_data, \"symptomatic\", undersampling)\n","\n","        # train_undersampled_data, test_undersampled_data = preprocess_dataset(undersampled_data, 0.3) # ORIGINAL\n","        train_undersampled_data, val_undersampled_data = preprocess_dataset(undersampled_data, 0.3)\n","        val_undersampled_data, test_undersampled_data = preprocess_dataset(val_undersampled_data, 0.5)\n","\n","        # Undersampled dataset\n","        train_dataset = AudioDataset(train_undersampled_data, args, le)\n","        val_dataset = AudioDataset(val_undersampled_data, args, le)\n","        test_dataset = AudioDataset(test_undersampled_data, args, le)\n","        \n","        #visualize_dataset(train_undersampled_data, False, \"Train\", \"gender\")\n","        #visualize_dataset(val_undersampled_data, False, \"Val\", \"gender\")\n","        \n","        train_dataloader = DataLoader(\n","            train_dataset,\n","            batch_size=batch,\n","            shuffle=True,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        val_dataloader = DataLoader(\n","            val_dataset,\n","            batch_size=batch,\n","            shuffle=False,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=batch,\n","            shuffle=False,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","\n","        return train_dataloader, val_dataloader, test_dataloader\n","\n","    elif dataset_type == \"weighted\":\n","        # Prepare and create weighted sampler\n","        train_sample_weights = weighted_sample(train_data)\n","        val_sample_weights = weighted_sample(val_data)\n","        test_sample_weights = weighted_sample(test_data)\n","\n","        train_weighted_Sampler = WeightedRandomSampler(weights=train_sample_weights, num_samples=len(train_data), replacement=True)\n","        val_weighted_Sampler = WeightedRandomSampler(weights=val_sample_weights, num_samples=len(val_data), replacement=True)\n","        test_weighted_Sampler = WeightedRandomSampler(weights=test_sample_weights, num_samples=len(test_data), replacement=True)\n","\n","        # Create dataset and dataloader instances\n","        train_dataset = AudioDataset(train_data, args, le)\n","        val_dataset = AudioDataset(val_data, args, le)\n","        test_dataset = AudioDataset(test_data, args, le)\n","\n","        train_dataloader = DataLoader(\n","            train_dataset,\n","            sampler=train_weighted_Sampler,\n","            batch_size=batch,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        val_dataloader = DataLoader(\n","            val_dataset,\n","            sampler=val_weighted_Sampler,\n","            batch_size=batch,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            sampler=test_weighted_Sampler,\n","            batch_size=batch,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","\n","        return train_dataloader, val_dataloader, test_dataloader\n","\n","\n","# Initialize LabelEncoder\n","le = LabelEncoder()\n","\n","# Fit and transform labels into encoded form\n","encoded_labels = le.fit_transform(labels)\n","\n","# Silence removal arguments\n","args = [min_silence, threshold_dBFS, keep_silence]\n","\n","# Prepare standard dataset\n","train_data, test_data = preprocess_dataset(data, 0.3)  # First split the original dataset into 70% training\n","val_data, test_data = preprocess_dataset(test_data, 0.5)  # Second split the \"test_data\" into 50/50 validation and test (technically 15/15)\n","\n","# Initialize dataloaders\n","train_dataloader, val_dataloader, test_dataloader = select_dataset(dataset_type, train_data, val_data, test_data, args, batch, workers, undersampling)"]},{"cell_type":"markdown","metadata":{},"source":["# MFCC feature extractor\n","\n","In the following codeblock the MFCC specific parameters are defined and initialized. The codeblock also includes a function that pads the extracted MFCC features in order to pass it to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:10.427876Z","iopub.status.busy":"2024-03-21T19:37:10.427580Z","iopub.status.idle":"2024-03-21T19:37:11.126078Z","shell.execute_reply":"2024-03-21T19:37:11.125045Z","shell.execute_reply.started":"2024-03-21T19:37:10.427845Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def MFCC_Features(data, padding=False, normalize=False, resize=False):\n","    \"\"\"\n","    Args:\n","    data: Input audio waveform\n","    max_length: Maximum length for padding\n","    normalize: Normalize the channel layer\n","    resize: Resize the spectrogram\n","    target_size: Target size for resizing\n","    \"\"\"\n","    # Extract MFCC features\n","    # features = mfcc(data)\n","    features = [torch.unsqueeze(mfcc(waveform), 0) for waveform in data]  # Adding channels\n","    # features = [torch.unsqueeze(torch.unsqueeze(mfcc(waveform), 0), 0) for waveform in data] # Adding batch size and channels\n","\n","    # Hardcoded padding (WIP)\n","    if padding:\n","        features = F.pad(features, (0, padding - features.shape[3]), \"constant\", 0)\n","\n","    # Normalize the features for each sample\n","    if normalize == True:\n","        for j, feature in enumerate(features):\n","            mean = feature.mean(dim=[1, 2], keepdim=True)\n","            std = feature.std(dim=[1, 2], keepdim=True)\n","            features[j] = (feature - mean) / std\n","\n","    # Resize mel spectrograms\n","    if resize == True:\n","        features = [Resize((224, 1500), antialias=True)(feature) for feature in features]\n","\n","    # Stack each feature as [batch_size, channels, features, length]\n","    features = torch.stack(features)\n","\n","    return features\n","\n","\n","# Instantiate MFCC feature extractor\n","mfcc = MFCC(n_mfcc=n_mfcc, sample_rate=sample_rate, melkwargs=melkwargs)"]},{"cell_type":"markdown","metadata":{},"source":["# Spectral features extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:11.128291Z","iopub.status.busy":"2024-03-21T19:37:11.127705Z","iopub.status.idle":"2024-03-21T19:37:11.736376Z","shell.execute_reply":"2024-03-21T19:37:11.735545Z","shell.execute_reply.started":"2024-03-21T19:37:11.128263Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def spectral_centroid(S=None, sr=22050, nfft=2048, h_length=512):\n","    return librosa.feature.spectral_centroid(S=S, sr=sr, n_fft=nfft, hop_length=h_length)[0]\n","\n","\n","def root_mean_square(S=None, f_length=2048, h_length=512):\n","    return librosa.feature.rms(S=S, frame_length=f_length, hop_length=h_length)[0]\n","\n","\n","def zero_crossing_rate(signal, f_length=2048, h_length=512):\n","    return librosa.feature.zero_crossing_rate(y=signal, frame_length=f_length, hop_length=h_length)[0]\n","\n","\n","def dynamic_parameters(audio_sample, sr=48000):\n","    duration_seconds = len(audio_sample) / sr\n","\n","    if duration_seconds <= 0.5:  # Very short audio (less than 0.5 seconds)\n","        n_fft = 256\n","        hop_length = 64\n","        frame_length = 256\n","    elif 0.5 < duration_seconds <= 1:  # Short audio (0.5 - 1 second)\n","        n_fft = 1024\n","        hop_length = 256\n","        frame_length = 1024\n","    elif 1 < duration_seconds <= 5:  # Medium-length audio (1-5 seconds)\n","        n_fft = 2048\n","        hop_length = 512\n","        frame_length = 2048\n","    else:  # Long audio (longer than 5 seconds)\n","        n_fft = 4096\n","        hop_length = 1024\n","        frame_length = 4096\n","    return n_fft, hop_length, frame_length\n","\n","\n","def plot_spectral_features(ZCR, RMS, SC, STFT=None, sample_rate=48000, hop_length=None, title=None):\n","    # Plot STFT spectrogram\n","    plt.figure(figsize=(12, 8))\n","\n","    plt.subplot(4, 1, 1)\n","    librosa.display.specshow(librosa.amplitude_to_db(np.abs(STFT), ref=np.max), sr=sample_rate, x_axis='time', y_axis='log')\n","    plt.colorbar(format='%+2.0f dB')\n","    plt.title(f'{title} STFT Spectrogram')\n","\n","    # Plot ZCR\n","    plt.figure(figsize=(10, 6))\n","    plt.subplot(4, 1, 2)\n","    plt.plot(ZCR, color='blue')\n","    plt.title('Zero Crossing Rate (ZCR)')\n","    plt.xlabel('Time')\n","    plt.ylabel('ZCR Value')\n","\n","    # Plot RMS\n","    plt.subplot(4, 1, 3)\n","    plt.plot(RMS, color='red')\n","    plt.title('Root Mean Square (RMS)')\n","    plt.xlabel('Time')\n","    plt.ylabel('RMS Value')\n","\n","    # Plot SC\n","    plt.subplot(4, 1, 4)\n","    plt.plot(SC, color='green')\n","    plt.title('Spectral Centroid (SC)')\n","    plt.xlabel('Time')\n","    plt.ylabel('SC Value')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def spectral_features(y, spectral=False):\n","    ZCR_features = []\n","    RMS_features = []\n","    SC_features = []\n","    STFT_features = []\n","    for sample in y:\n","        sample = np.asarray(sample)\n","\n","        # Return n_fft, hop_length, frame_length based on length of sample\n","        n_fft, hop_length, frame_length = dynamic_parameters(sample)\n","\n","        # Compute magnitude spectrum of sample\n","        STFT = librosa.stft(y=sample, n_fft=n_fft, hop_length=hop_length, win_length=n_fft, window=\"hann\", center=True, pad_mode=\"constant\")\n","        S, _ = librosa.magphase(STFT)\n","\n","        # Compute ZCR, RMS, and SC for additional feature extraction\n","        ZCR = zero_crossing_rate(signal=sample, f_length=frame_length, h_length=hop_length)\n","        RMS = root_mean_square(S=S, f_length=frame_length, h_length=hop_length)\n","        SC = spectral_centroid(S=S, sr=48000, nfft=n_fft, h_length=hop_length)\n","\n","        # Convert into tensors and normalize\n","        ZCR = torch.tensor(ZCR, dtype=torch.float32)\n","        RMS = torch.tensor(RMS, dtype=torch.float32)\n","        SC = torch.tensor(SC, dtype=torch.float32)\n","        ZCR = (ZCR - ZCR.mean()) / ZCR.std()\n","        RMS = (RMS - RMS.mean()) / RMS.std()\n","        SC = (SC - SC.mean()) / SC.std()\n","        STFT = (STFT - STFT.mean()) / STFT.std()\n","        \n","        # Convert STFT into spectrogram\n","        STFT = torch.tensor(librosa.amplitude_to_db(np.abs(STFT), ref=np.max), dtype=torch.float32).unsqueeze(0)\n","        \n","        # Append the features for the current signal to the respective lists\n","        ZCR_features.append(ZCR)\n","        RMS_features.append(RMS)\n","        SC_features.append(SC)\n","        if spectral:\n","            STFT_features.append(STFT)\n","\n","\n","    # Compute the maximum length of features the combined features    \n","    max_len = max(max((len(zcr), len(rms), len(sc))) for zcr, rms, sc in zip(ZCR_features, RMS_features, SC_features))\n","    if spectral:\n","        STFT_len_dim1 = max(stft.shape[1] for stft in STFT_features)\n","        STFT_len_dim2 = max(stft.shape[2] for stft in STFT_features)\n","    \n","    # Pad each feature vector to the max length and resize the STFT spectrograms\n","    ZCR_features = [F.pad(zcr, (0, max_len - zcr.shape[0]), value=0.0) for zcr in ZCR_features]\n","    RMS_features = [F.pad(rms, (0, max_len - rms.shape[0]), value=0.0) for rms in RMS_features]\n","    SC_features = [F.pad(sc, (0, max_len - sc.shape[0]), value=0.0) for sc in SC_features]\n","    if spectral:\n","        #STFT_features = [Resize((STFT_len_dim1, STFT_len_dim2), antialias=True)(STFT) for STFT in STFT_features] # Original\n","        STFT_features = [Resize((500, 300), antialias=True)(STFT) for STFT in STFT_features]\n","    \n","    # Stack each list of tensors to create new shape:\n","    # [batch_size, channels, feature_length]\n","    ZCR_features = torch.stack(ZCR_features, dim=0).unsqueeze(1)\n","    RMS_features = torch.stack(RMS_features, dim=0).unsqueeze(1)\n","    SC_features = torch.stack(SC_features, dim=0).unsqueeze(1)\n","    if spectral:\n","        STFT_features = torch.stack(STFT_features, dim=0)\n","    \n","    return ZCR_features, RMS_features, SC_features, STFT_features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:11.738038Z","iopub.status.busy":"2024-03-21T19:37:11.737668Z","iopub.status.idle":"2024-03-21T19:37:12.465965Z","shell.execute_reply":"2024-03-21T19:37:12.465013Z","shell.execute_reply.started":"2024-03-21T19:37:11.738001Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["\"\"\"\n","for i, (inputs, targets, numeric) in tqdm(\n","    enumerate(train_dataloader),\n","    total=len(train_dataloader),\n","    leave=True):\n","    # Convert features and target labels into long\n","    ZCR, RMS, SC, STFT = spectral_features(inputs)\n","    print(\"zcr batch\", ZCR.shape)\n","    print(\"rms batch\", RMS.shape)\n","    print(\"sc batch\", SC.shape)\n","    print(\"stft batch\", STFT.shape)\n","    for ZCR, RMS, SC, STFT in zip(ZCR, RMS, SC, STFT):\n","        # Plot STFT spectrogram\n","        plt.figure(figsize=(12, 8))\n","\n","        plt.subplot(4, 1, 1)\n","        plt.imshow(np.array(STFT.squeeze(0)), aspect='auto', origin='lower')\n","        plt.colorbar(format='%+2.0f dB')\n","        plt.xlabel('Time (s)')\n","        plt.ylabel('Frequency (Hz)')\n","        plt.title('STFT Spectrogram')\n","\n","        # Plot ZCR\n","        plt.figure(figsize=(10, 6))\n","        plt.subplot(4, 1, 2)\n","        plt.plot(np.array(ZCR.squeeze()), color='blue')\n","        plt.title('Zero Crossing Rate (ZCR)')\n","        plt.xlabel('Time')\n","        plt.ylabel('ZCR Value')\n","\n","        # Plot RMS\n","        plt.subplot(4, 1, 3)\n","        plt.plot(np.array(RMS.squeeze()), color='red')\n","        plt.title('Root Mean Square (RMS)')\n","        plt.xlabel('Time')\n","        plt.ylabel('RMS Value')\n","\n","        # Plot SC\n","        plt.subplot(4, 1, 4)\n","        plt.plot(np.array(SC.squeeze()), color='green')\n","        plt.title('Spectral Centroid (SC)')\n","        plt.xlabel('Time')\n","        plt.ylabel('SC Value')\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    \n","    if i == 0: break\n","    \n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Network architectures"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MultiInputResNet(nn.Module):\n","    def __init__(self, weights=None, num_classes=3, model_arch=\"resnet18\"):\n","        super(MultiInputResNet, self).__init__()\n","        if model_arch == \"resnet18\":\n","            self.resnet = models.resnet18(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet34\":\n","            self.resnet = models.resnet34(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet50\":\n","            self.resnet = models.resnet50(weights=weights, num_classes=num_classes)\n","\n","        # Adjust the first convolutional layer to match number of channels\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","        # Add additional branch to handle numeric features\n","        self.numeric_features = nn.Sequential(\n","            nn.Linear(3, 64),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","            nn.Linear(64, 512),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","        )\n","\n","        # Adding a fully connected layer\n","        self.fc = nn.Linear(512 + 3, num_classes)  # 512 from ResNet + 3 from numeric features\n","\n","    def forward(self, mfcc, numeric):\n","        # Process MFCC input through ResNet\n","        mfcc_resnet_output = self.resnet(mfcc)\n","\n","        # Process numeric features through additional branch\n","        numeric_output = self.numeric_features(numeric)\n","\n","        # Concatenate the outputs from both branches\n","        combined_features = torch.cat((mfcc_resnet_output, numeric_output), dim=1)\n","\n","        # return raw scores/logits\n","        output = self.fc(combined_features)\n","\n","        # Apply softmax activation to get probabilities\n","        # output_probs = F.softmax(output, dim=1)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Modified_MultiInputResNet(nn.Module):\n","    def __init__(self, weights=None, num_classes=3, model_arch=\"resnet18\"):\n","        super(Modified_MultiInputResNet, self).__init__()\n","        if model_arch == \"resnet18\":\n","            self.resnet = models.resnet18(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet34\":\n","            self.resnet = models.resnet34(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet50\":\n","            self.resnet = models.resnet50(weights=weights, num_classes=num_classes)\n","\n","        # Adjust the first convolutional layer to match number of channels\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","        # Add additional branch to handle numeric features\n","        self.numeric_features = nn.Sequential(\n","            nn.Linear(3, 64),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","            nn.Linear(64, 512),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","        )\n","\n","        # Adding a fully connected layer\n","        self.fc = nn.Linear(512 + 3, num_classes)  # 512 from ResNet + 3 from numeric features\n","\n","        # Adding fully connected layers for age and gender prediction\n","        self.fc_age = nn.Linear(3, 4)\n","        self.fc_gender = nn.Linear(3, 2)\n","\n","    def forward(self, mfcc, numeric):\n","        # Process MFCC input through ResNet\n","        mfcc_resnet_output = self.resnet(mfcc)\n","\n","        # Process numeric features through additional branch\n","        numeric_output = self.numeric_features(numeric)\n","        numeric_output2 = F.relu(numeric)\n","\n","        # Concatenate the outputs from both branches\n","        combined_features = torch.cat((mfcc_resnet_output, numeric_output), dim=1)\n","\n","        output_class = self.fc(combined_features)\n","        output_age = self.fc_age(numeric_output2)\n","        output_gender = self.fc_gender(numeric_output2)\n","\n","        ## Apply softmax activation to get probabilities\n","        # output_probs_class = F.softmax(output_class, dim=1)\n","        # output_probs_age = F.softmax(output_age, dim=1)\n","        # output_probs_gender = F.softmax(output_gender, dim=1)\n","\n","        return output_class, output_age, output_gender"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:13.061890Z","iopub.status.busy":"2024-03-21T19:37:13.061566Z","iopub.status.idle":"2024-03-21T19:37:13.772872Z","shell.execute_reply":"2024-03-21T19:37:13.771912Z","shell.execute_reply.started":"2024-03-21T19:37:13.061853Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class MultiInputResNet_spectral(nn.Module):\n","    def __init__(self, weights=None, num_classes=3, model_arch=\"resnet18\"):\n","        super(MultiInputResNet_spectral, self).__init__()\n","        if model_arch == \"resnet18\":\n","            self.resnet = models.resnet18(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet34\":\n","            self.resnet = models.resnet34(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet50\":\n","            self.resnet = models.resnet50(weights=weights, num_classes=num_classes)\n","\n","        # Adjust the first convolutional layer to match number of channels\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","        # Modify final fully connected layer to output relevant features\n","        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 512)\n","        \n","        # Add additional branch to handle numeric features\n","        self.numeric_features = nn.Sequential(\n","            nn.Linear(1, 64),\n","            nn.ReLU(inplace=False),\n","            nn.Dropout(0.5),\n","            nn.Linear(64, 512),\n","            nn.ReLU(inplace=False),\n","            #nn.Dropout(0.5),\n","        )\n","\n","        # Add additional branch to handle spectral features\n","        self.spectral_features = nn.Sequential(\n","            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=False), \n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=False), \n","            nn.MaxPool1d(kernel_size=2), \n","            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=False), \n","            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=False), \n","            nn.AdaptiveAvgPool1d(1)\n","        )\n","\n","        # Adding a fully connected layer\n","        self.fc = nn.Sequential(\n","            nn.Linear(3584, 3000),   # 512 from Resnet, 3x512 from numeric features, and 3*512 from spectral features\n","            nn.ReLU(), \n","            nn.Dropout(0.1),\n","            nn.Linear(3000, 2500),\n","            #nn.ReLU(),\n","            #nn.Dropout(0.2),\n","            #nn.Linear(2500, 1500),\n","            #nn.ReLU(),\n","            #nn.Dropout(0.3),\n","            #nn.Linear(1500, 1000),\n","            #nn.ReLU(),\n","            #nn.Linear(1000, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(2500, num_classes)\n","        )\n","        \n","        self.fc_stft = nn.Sequential(\n","            nn.Linear(4096, 3000), # STFT\n","            nn.ReLU(), \n","            nn.Dropout(0.1),\n","            nn.Linear(3000, 2500),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(2500, num_classes)\n","        )\n","        \n","        # Adding fully connected layers for age and gender prediction\n","        self.fc_age = nn.Sequential(\n","            nn.Linear(3584, 1792), # original\n","            #nn.Linear(4096, 1792),\n","            nn.Dropout(0.1),\n","            nn.Linear(1792, 896),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(896, 448),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(448, 224),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(224, 112),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(112, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(64, 4)\n","        )\n","\n","        self.fc_gender = nn.Sequential(\n","            nn.Linear(3584, 1792), # original\n","            #nn.Linear(4096, 1792),\n","            nn.ReLU(), \n","            nn.Dropout(0.3),\n","            nn.Linear(1792, 896),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(896, 448),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(448, 224),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(224, 112),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(112, 64),\n","            nn.Sigmoid(),\n","            nn.Linear(64, 2)\n","        )\n","\n","    def forward(self, mfcc, numeric, ZCR, RMS, SC, STFT=None):\n","        # Process MFCC input through ResNet\n","        mfcc_resnet_output = self.resnet(mfcc)\n","        \n","        # Process STFT input through ResNet\n","        if STFT:\n","            stft_resnet_output = self.resnet(STFT)\n","\n","        # Process numeric features through the additional branch\n","        gender_output = self.numeric_features(numeric[:, 0].unsqueeze(1))\n","        age_output = self.numeric_features(numeric[:, 1].unsqueeze(1))\n","        SNR_output = self.numeric_features(numeric[:, 2].unsqueeze(1))\n","\n","        # Process each spectral features through the additional branch\n","        ZCR_output = self.spectral_features(ZCR).squeeze(dim=2)\n","        RMS_output = self.spectral_features(RMS).squeeze(dim=2)\n","        SC_output = self.spectral_features(SC).squeeze(dim=2)\n","        \n","        # Combine the features\n","        if STFT:\n","            combined_features = torch.cat((mfcc_resnet_output,stft_resnet_output, gender_output, age_output, SNR_output, ZCR_output, RMS_output, SC_output), dim=1)\n","        else:\n","            combined_features = torch.cat((mfcc_resnet_output, gender_output, age_output, SNR_output, ZCR_output, RMS_output, SC_output), dim=1)\n","        \n","        # Classification output\n","        if STFT:\n","            output_class = self.fc_stft(combined_features)\n","        else:\n","            output_class = self.fc(combined_features)\n","            \n","        \n","        # Age and gender predictions\n","        output_age = self.fc_age(combined_features)\n","        output_gender = self.fc_gender(combined_features)\n","        \n","        return output_class, output_age, output_gender\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training and validation functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:21.799474Z","iopub.status.busy":"2024-03-21T19:37:21.799105Z","iopub.status.idle":"2024-03-21T19:37:22.545834Z","shell.execute_reply":"2024-03-21T19:37:22.544795Z","shell.execute_reply.started":"2024-03-21T19:37:21.799438Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def initialize_training_setup(model, optimizer_type, scheduler_type, weighted=False, device=None):\n","    scheduler = None\n","    criterion2 = None\n","    if optimizer_type == \"adam\":\n","        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n","\n","    elif optimizer_type == \"sgd\":\n","        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=decay)\n","\n","    # Compute class weights for loss function (if using weighted)\n","    if weighted == True:\n","        train_labels = train_data[\"status\"]\n","        class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n","        class_weights = torch.tensor(class_weights, dtype=torch.float32)\n","        print(\"COVID-19\",\"Healthy\", \"Symptomatic\")\n","        print(class_weights)\n","        class_weights = class_weights.to(device)\n","        criterion = nn.CrossEntropyLoss(weight=class_weights)\n","        criterion2 = nn.CrossEntropyLoss()\n","    else:\n","        criterion = nn.CrossEntropyLoss()\n","        \n","    if scheduler_type == \"steplr\":\n","        scheduler = StepLR(optimizer, step_size=step, gamma=gamma)\n","\n","    elif scheduler_type == \"stepplateau\":\n","        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, threshold=thresh_plat, cooldown=cooldown, min_lr=0)\n","\n","    return optimizer, criterion, scheduler, criterion2\n","\n","\n","def train_epoch_weighted(model, device, epoch, optimizer, criterion, criterion2):\n","    model.train()\n","    running_loss = 0.0\n","    correct_predictions_class = 0\n","    correct_predictions_age = 0\n","    correct_predictions_gender = 0\n","    total_predictions_class = 0\n","    total_predictions_age = 0\n","    total_predictions_gender = 0\n","\n","    print(\"Currently: Training\")\n","    for i, (inputs, targets, numeric) in tqdm(\n","        enumerate(train_dataloader),\n","        total=len(train_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Training\",\n","    ):\n","        # Convert features and target labels into long\n","        class_targets = torch.round(targets).to(torch.long)\n","        gender_targets = torch.round(numeric[:, 0]).to(torch.long)\n","        age_targets = torch.round(numeric[:, 1]).to(torch.long)\n","\n","        # Load them onto GPU\n","        class_targets = class_targets.to(device)\n","        gender_targets = gender_targets.to(device)\n","        age_targets = age_targets.to(device)\n","        \n","        # Extract features\n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        #ZCR, RMS, SC, STFT = spectral_features(inputs) # Compute spectral features \n","        ZCR, RMS, SC, _ = spectral_features(inputs) # Compute spectral features \n","        features, numeric= features.to(device), numeric.to(device) # Load them onto GPU\n","        #ZCR, RMS, SC, STFT = ZCR.to(device), RMS.to(device), SC.to(device), STFT.to(device)\n","        ZCR, RMS, SC = ZCR.to(device), RMS.to(device), SC.to(device)\n","\n","        # Training loop\n","        optimizer.zero_grad()  # Zero the parameters\n","        #output_class, output_age, output_gender = model(features, numeric, ZCR, RMS, SC, STFT)  # Retrieve the output from the model\n","        output_class, output_age, output_gender = model(features, numeric, ZCR, RMS, SC)  # Retrieve the output from the model\n","\n","        # Compute the loss for each output separately\n","        loss_class = criterion2(output_class, class_targets)\n","        loss_gender = criterion(output_gender, gender_targets)\n","        loss_age = criterion(output_age, age_targets)\n","\n","        # Compute total loss - add weights to mitigate the avg high training loss\n","        total_loss = (1.0 * loss_class) + (0.1 * loss_age) + (0.1 * loss_gender)\n","        total_loss.backward()  # Compute gradients of the loss\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping to prevent exploding gradients\n","        optimizer.step()  # Update weights\n","\n","        running_loss += total_loss.item()\n","\n","        # Compute correct predictions and update statistics for class prediction\n","        _, predicted_class = torch.max(output_class, 1)\n","        correct_predictions_class += (predicted_class == class_targets).sum().item()\n","        total_predictions_class += class_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for age prediction\n","        _, predicted_age = torch.max(output_age, 1)\n","        correct_predictions_age += (predicted_age == age_targets).sum().item()\n","        total_predictions_age += age_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for gender prediction\n","        _, predicted_gender = torch.max(output_gender, 1)\n","        correct_predictions_gender += (predicted_gender == gender_targets).sum().item()\n","        total_predictions_gender += gender_targets.size(0)\n","\n","        # Print statistics for every 5th mini-batch\n","        if i % 5 == 4:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i+1}/{len(train_dataloader)} | Total Training Loss: {total_loss.item():.4f}\")\n","            print(f\"Class Training Loss: {loss_class:.4f} | Gender Training Loss: {loss_gender:.4f} | Age Training Loss: {loss_age:.4f}\")\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    avg_loss = running_loss / len(train_dataloader)\n","    avg_accuracy_class = correct_predictions_class / total_predictions_class\n","    avg_accuracy_age = correct_predictions_age / total_predictions_age\n","    avg_accuracy_gender = correct_predictions_gender / total_predictions_gender\n","    print()\n","    print(\"=========================================================\")\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","    print(f\"Avg Training loss: {avg_loss:.4f}\")\n","    print(f\"Avg Training Accuracy Class: {avg_accuracy_class:.4f}\")\n","    print(f\"Avg Training Accuracy Age: {avg_accuracy_age:.4f}\")\n","    print(f\"Avg Training Accuracy Gender: {avg_accuracy_gender:.4f}\")\n","    print(\"=========================================================\")\n","    print()\n","    \n","    return avg_loss, avg_accuracy_class, avg_accuracy_age, avg_accuracy_gender\n","\n","\n","def train_epoch_undersampled(model, device, epoch, optimizer, criterion):\n","    model.train()\n","    running_loss = 0.0\n","    correct_predictions_class = 0\n","    correct_predictions_age = 0\n","    correct_predictions_gender = 0\n","    total_predictions_class = 0\n","    total_predictions_age = 0\n","    total_predictions_gender = 0\n","\n","    print(\"Currently: Training\")\n","    for i, (inputs, targets, numeric) in tqdm(\n","        enumerate(train_dataloader),\n","        total=len(train_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Training\",\n","    ):\n","        # Convert features and target labels into long\n","        class_targets = torch.round(targets).to(torch.long)\n","        gender_targets = torch.round(numeric[:, 0]).to(torch.long)\n","        age_targets = torch.round(numeric[:, 1]).to(torch.long)\n","\n","        # Load them onto GPU\n","        class_targets = class_targets.to(device)\n","        gender_targets = gender_targets.to(device)\n","        age_targets = age_targets.to(device)\n","        \n","        # Extract features\n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        ZCR, RMS, SC, _ = spectral_features(inputs) # Compute spectral features \n","        features, numeric= features.to(device), numeric.to(device) # Load them onto GPU\n","        #ZCR, RMS, SC, STFT = ZCR.to(device), RMS.to(device), SC.to(device), STFT.to(device) # STFT\n","        ZCR, RMS, SC = ZCR.to(device), RMS.to(device), SC.to(device)\n","\n","        # Training loop\n","        optimizer.zero_grad()  # Zero the parameters\n","        #output_class, output_age, output_gender = model(features, numeric, ZCR, RMS, SC, STFT)  # Retrieve the output from the model\n","        output_class, output_age, output_gender = model(features, numeric, ZCR, RMS, SC)  # Retrieve the output from the model\n","\n","        # Compute the loss for each output separately\n","        loss_class = criterion(output_class, class_targets)\n","        loss_gender = criterion(output_gender, gender_targets)\n","        loss_age = criterion(output_age, age_targets)\n","\n","        # Compute total loss - add weights to mitigate the avg high training loss\n","        total_loss = (1.0 * loss_class) + (0.1 * loss_age) + (0.1 * loss_gender)\n","        total_loss.backward()  # Compute gradients of the loss\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping to prevent exploding gradients\n","        optimizer.step()  # Update weights\n","\n","        running_loss += total_loss.item()\n","\n","        # Compute correct predictions and update statistics for class prediction\n","        _, predicted_class = torch.max(output_class, 1)\n","        correct_predictions_class += (predicted_class == class_targets).sum().item()\n","        total_predictions_class += class_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for age prediction\n","        _, predicted_age = torch.max(output_age, 1)\n","        correct_predictions_age += (predicted_age == age_targets).sum().item()\n","        total_predictions_age += age_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for gender prediction\n","        _, predicted_gender = torch.max(output_gender, 1)\n","        correct_predictions_gender += (predicted_gender == gender_targets).sum().item()\n","        total_predictions_gender += gender_targets.size(0)\n","\n","        # Print statistics for every 5th mini-batch\n","        if i % 5 == 4:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i+1}/{len(train_dataloader)} | Total Training Loss: {total_loss.item():.4f}\")\n","            print(f\"Class Training Loss: {loss_class:.4f} | Gender Training Loss: {loss_gender:.4f} | Age Training Loss: {loss_age:.4f}\")\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    avg_loss = running_loss / len(train_dataloader)\n","    avg_accuracy_class = correct_predictions_class / total_predictions_class\n","    avg_accuracy_age = correct_predictions_age / total_predictions_age\n","    avg_accuracy_gender = correct_predictions_gender / total_predictions_gender\n","    print()\n","    print(\"=========================================================\")\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","    print(f\"Avg Training loss: {avg_loss:.4f}\")\n","    print(f\"Avg Training Accuracy Class: {avg_accuracy_class:.4f}\")\n","    print(f\"Avg Training Accuracy Age: {avg_accuracy_age:.4f}\")\n","    print(f\"Avg Training Accuracy Gender: {avg_accuracy_gender:.4f}\")\n","    print(\"=========================================================\")\n","    print()\n","    \n","    return avg_loss, avg_accuracy_class, avg_accuracy_age, avg_accuracy_gender\n","\n","\n","def validate_epoch(model, device, epoch, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    correct_predictions_class = 0\n","    correct_predictions_age = 0\n","    correct_predictions_gender = 0\n","    total_predictions_class = 0\n","    total_predictions_age = 0\n","    total_predictions_gender = 0\n","    \n","    precision_class = 0.0\n","    precision_gender = 0.0\n","    precision_age = 0.0\n","    recall_class = 0.0\n","    recall_gender = 0.0\n","    recall_age = 0.0\n","    f1_class = 0.0\n","    f1_gender = 0.0\n","    f1_age = 0.0\n","\n","    print(\"Currently: Validating\")\n","    for j, (vinputs, vtargets, vnumeric) in tqdm(\n","        enumerate(val_dataloader),\n","        total=len(val_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Validating\",\n","    ):\n","        # Convert features and target labels into long\n","        class_targets = torch.round(vtargets).to(torch.long)\n","        gender_targets = torch.round(vnumeric[:, 0]).to(torch.long)\n","        age_targets = torch.round(vnumeric[:, 1]).to(torch.long)\n","\n","        # Load them onto GPU\n","        class_targets = class_targets.to(device)\n","        gender_targets = gender_targets.to(device)\n","        age_targets = age_targets.to(device)\n","        \n","        # Extract features\n","        vfeatures = MFCC_Features(vinputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        #ZCR, RMS, SC, STFT = spectral_features(vinputs)\n","        ZCR, RMS, SC, _ = spectral_features(vinputs)\n","        vfeatures, vnumeric = vfeatures.to(device), vnumeric.to(device)  # Load them onto GPU\n","        #ZCR, RMS, SC, STFT = ZCR.to(device), RMS.to(device), SC.to(device), STFT.to(device)\n","        ZCR, RMS, SC = ZCR.to(device), RMS.to(device), SC.to(device)\n","\n","        # Validation loop\n","        #voutput_class, voutput_age, voutput_gender = model(vfeatures, vnumeric, ZCR, RMS, SC, STFT) # STFT\n","        voutput_class, voutput_age, voutput_gender = model(vfeatures, vnumeric, ZCR, RMS, SC) # STFT\n","\n","        # Compute the loss for each output separately\n","        vloss_class = criterion(voutput_class, class_targets)\n","        vloss_gender = criterion(voutput_gender, gender_targets)\n","        vloss_age = criterion(voutput_age, age_targets)\n","\n","        # Compute total loss - add weights to mitigate the avg high training loss\n","        total_loss = (1.0 * vloss_class) + (0.1 * vloss_age) + (0.1 * vloss_gender)\n","        running_loss += total_loss.item()\n","\n","        # Compute correct predictions and update statistics for class prediction\n","        _, predicted_class = torch.max(voutput_class, 1)\n","        correct_predictions_class += (predicted_class == class_targets).sum().item()\n","        total_predictions_class += class_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for age prediction\n","        _, predicted_age = torch.max(voutput_age, 1)\n","        correct_predictions_age += (predicted_age == age_targets).sum().item()\n","        total_predictions_age += age_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for gender prediction\n","        _, predicted_gender = torch.max(voutput_gender, 1)\n","        correct_predictions_gender += (predicted_gender == gender_targets).sum().item()\n","        total_predictions_gender += gender_targets.size(0)\n","        \n","        # Compute precision, recall, F1 score\n","        precision_class += precision_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","        precision_gender += precision_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","        precision_age += precision_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","        recall_class += recall_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","        recall_gender += recall_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","        recall_age += recall_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","        f1_class += f1_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","        f1_gender += f1_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","        f1_age += f1_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","        \n","        # Print statistics for every 5th mini-batch\n","        if j % 5 == 4:\n","            avg_precision_class_batch = precision_class / (5 * (j + 1))\n","            avg_precision_gender_batch = precision_gender / (5 * (j + 1))\n","            avg_precision_age_batch = precision_age / (5 * (j + 1))\n","            avg_recall_class_batch = recall_class / (5 * (j + 1))\n","            avg_recall_gender_batch = recall_gender / (5 * (j + 1))\n","            avg_recall_age_batch = recall_age / (5 * (j + 1))\n","            avg_f1_class_batch = f1_class / (5 * (j + 1))\n","            avg_f1_gender_batch = f1_gender / (5 * (j + 1))\n","            avg_f1_age_batch = f1_age / (5 * (j + 1))\n","            print()\n","            print(\"=========================================================\")\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {j+1}/{len(val_dataloader)} | Total Validation Loss: {total_loss.item():.4f}\")\n","            print(f\"Class Validation Loss: {vloss_class:.4f} | Gender Validation Loss: {vloss_gender:.4f} | Age Validation Loss: {vloss_age:.4f}\")\n","            print(f\"Precision Class: {avg_precision_class_batch:.4f}  | Precision Age: {avg_precision_age_batch:.4f} | Precision Gender: {avg_precision_gender_batch:.4f}\")\n","            print(f\"Recall Class: {avg_recall_class_batch:.4f}      | Recall Age: {avg_recall_age_batch:.4f} | Recall Gender: {avg_recall_gender_batch:.4f}\")\n","            print(f\"F1 Score Class: {avg_f1_class_batch:.4f}    | F1 Score Age: {avg_f1_age_batch:.4f} | F1 Score Gender: {avg_f1_gender_batch:.4f}\")\n","            print(\"=========================================================\")\n","            print()\n","\n","    # Compute and return average validation loss, accuracy, precision, recall, and F1 score\n","    avg_vloss = running_loss / len(val_dataloader)\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    vaccuracy_class = correct_predictions_class / total_predictions_class\n","    vaccuracy_age = correct_predictions_age / total_predictions_age\n","    vaccuracy_gender = correct_predictions_gender / total_predictions_gender\n","\n","    avg_precision_class = precision_class / len(val_dataloader)\n","    avg_precision_gender = precision_gender / len(val_dataloader)\n","    avg_precision_age = precision_age / len(val_dataloader)\n","    avg_recall_class = recall_class / len(val_dataloader)\n","    avg_recall_gender = recall_gender / len(val_dataloader)\n","    avg_recall_age = recall_age / len(val_dataloader)\n","    avg_f1_class = f1_class / len(val_dataloader)\n","    avg_f1_gender = f1_gender / len(val_dataloader)\n","    avg_f1_age = f1_age / len(val_dataloader)\n","\n","    # return avg_vloss, vaccuracy, precision, recall, f1\n","    metrics = ((vaccuracy_class, vaccuracy_age, vaccuracy_gender), \n","           (avg_precision_class, avg_precision_age, avg_precision_gender), \n","           (avg_recall_class, avg_recall_age, avg_recall_gender), \n","           (avg_f1_class, avg_f1_age, avg_f1_gender))\n","\n","    return avg_vloss, metrics\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training and validating model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:22.548189Z","iopub.status.busy":"2024-03-21T19:37:22.547782Z","iopub.status.idle":"2024-03-21T19:37:24.134454Z","shell.execute_reply":"2024-03-21T19:37:24.133630Z","shell.execute_reply.started":"2024-03-21T19:37:22.548154Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def select_model(model_type, weighted=None):\n","    if model_type == \"resnet18\":\n","        model = models.resnet18(weights=weighted, num_classes=3)\n","        model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"resnet34\":\n","        model = models.resnet50(weights=weighted, num_classes=3)\n","        model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"resnet50\":\n","        model = models.resnet50(weights=weighted, num_classes=3)\n","        model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"vgg_bn\":\n","        model = models.vgg16_bn(weights=weighted, num_classes=3)\n","        model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"multi_resnet\":\n","        model = MultiInputResNet(weights=weighted, num_classes=3, model_arch=model_arch)\n","        return model\n","    elif model_type == \"modified_multi_resnet\":\n","        model = Modified_MultiInputResNet(weights=weighted, num_classes=3, model_arch=model_arch)\n","        return model\n","    elif model_type == \"modified_multi_resnet_spectral\":\n","        model = MultiInputResNet_spectral(weights=weighted, num_classes=3, model_arch=model_arch)\n","        return model\n","\n","\n","# Initialize model\n","model = select_model(model_type)\n","\n","# Set the model to training mode and put it on GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","print(f\"Device running on: {device}\")\n","\n","# Wrap model with DataParallel if multiple GPUs are available\n","if torch.cuda.device_count() > 1:\n","    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model)\n","else:\n","    print(\"Only 1 GPU available!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:24.138172Z","iopub.status.busy":"2024-03-21T19:37:24.137888Z"},"jupyter":{"source_hidden":true},"scrolled":true,"trusted":true},"outputs":[],"source":["# Initialize training setup\n","optimizer, criterion, scheduler, criterion2 = initialize_training_setup(model, optimizer_type, scheduler_type, weighted, device)\n","best_vloss = float(\"inf\")\n","model_no = 0\n","\n","# Create a GradScaler instance\n","scaler = GradScaler()\n","\n","# Training and validation loop\n","for epoch in tqdm(range(epochs), total=epochs, leave=True, desc=f\"Epoch | \"):\n","    # avg_loss, accuracy = train_epoch(model, model_type, device, epoch, optimizer, criterion)\n","    # avg_loss, accuracy_class, accuracy_age, accuracy_gender = train_epoch_modified(model, device, epoch, optimizer, criterion)\n","    avg_loss, accuracy_class, accuracy_age, accuracy_gender = train_epoch_undersampled(model, device, epoch, optimizer, criterion, criterion2)\n","    # avg_vloss, vaccuracy, precision, recall, f1 = validate_epoch(model, device, epoch, criterion)\n","    # avg_vloss, metrics = validate_epoch_modified(model, device, epoch, criterion)\n","    avg_vloss, metrics = validate_epoch(model, device, epoch, criterion)\n","\n","    # Print and log metrics\n","    # MultiResnet model\n","    # print(f\"Epoch #{epoch+1} | Training Loss: {avg_loss:.4f} | Validation Loss: {avg_vloss:.4f} | Validation Accuracy: {vaccuracy:.4f}\")\n","    # print(f\"Epoch #{epoch+1} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n","    # print(f\"Epoch #{epoch+1} | Learning Rate: {scheduler.get_last_lr()}\")\n","\n","    # Extract metrics\n","    vaccuracy = metrics[0]\n","    vprecision = metrics[1]\n","    vrecall = metrics[2]\n","    vf1 = metrics[3]\n","\n","    # ModifiedMultiResnet model\n","    print(f\"Epoch #{epoch+1} | Training Loss: {avg_loss:.4f} | Validation Loss: {avg_vloss:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class Accuracy: {vaccuracy[0]:.4f} | Age Accuracy: {vaccuracy[1]:.4f} | Gender Accuracy: {vaccuracy[2]:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class Precision: {vprecision[0]:.4f} | Precision: {vprecision[1]:.4f} | Precision: {vprecision[2]:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class Recall: {vrecall[0]:.4f} | Age Recall: {vrecall[1]:.4f} | Gender Recall: {vrecall[2]:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class F1 score: {vf1[0]:.4f} | Age F1 score: {vf1[1]:.4f} | Gender F1 Score: {vf1[2]:.4f}\")\n","\n","    \n","    # Update learning rate\n","    if scheduler_type == \"steplr\":\n","        scheduler.step()\n","    elif scheduler_type == \"stepplateau\":\n","        scheduler.step(avg_vloss)\n","\n","    # Log metrics to wandb\n","    # MultiResnet model\n","    # wandb.log({\n","    #        \"epoch\": epoch + 1,\n","    #        \"train_loss\": avg_loss,\n","    #        \"train_acc\": accuracy,\n","    #        \"val_loss\": avg_vloss,\n","    #        \"val_accuracy\": vaccuracy,\n","    #    })\n","    # wandb.log({\"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n","\n","    # ModifiedMultiResnet model accuracy_class, accuracy_age, accuracy_gender\n","    wandb.log({\n","        \"epoch\": epoch + 1,\n","        \"avg_train_loss\": avg_loss,\n","        \"avg_val_loss\": avg_vloss,\n","        \"train_class_accuracy\": accuracy_class,\n","        \"train_age_accuracy\": accuracy_age,\n","        \"train_gender_accuracy\": accuracy_gender,\n","        \"val_class_accuracy\": vaccuracy[0],\n","        \"val_age_accuracy\": vaccuracy[1],\n","        \"val_gender_accuracy\": vaccuracy[2],\n","        \"val_class_precision\": vprecision[0],\n","        \"val_age_precision\": vprecision[1],\n","        \"val_gender_precision\": vprecision[2],\n","        \"val_class_recall\": vrecall[0],\n","        \"val_age_recall\": vrecall[1],\n","        \"val_gender_recall\": vrecall[2],\n","        \"val_class_f1_score\": vf1[0],\n","        \"val_age_f1_score\": vf1[1],\n","        \"val_gender_f1_score\": vf1[2]\n","    })\n","\n","    # Track best performance, and save the model's state\n","    if avg_vloss < best_vloss:\n","        best_vloss = avg_vloss\n","        model_no += 1\n","        if not os.path.exists(\"models\"):\n","            os.makedirs(\"models\")\n","        model_path = f\"models/{model_output}_no_{model_no}_epoch_{epoch+1}.pth\"\n","        torch.save(model.state_dict(), model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4560779,"sourceId":7791391,"sourceType":"datasetVersion"},{"datasetId":4561341,"sourceId":7792152,"sourceType":"datasetVersion"},{"datasetId":4609644,"sourceId":7858628,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
+{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importing necessary libraries"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:36:21.525640Z","iopub.status.busy":"2024-03-21T19:36:21.524945Z","iopub.status.idle":"2024-03-21T19:36:26.944309Z","shell.execute_reply":"2024-03-21T19:36:26.943519Z","shell.execute_reply.started":"2024-03-21T19:36:21.525603Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Torch-related imports\n","from torch.utils.data import WeightedRandomSampler, DataLoader\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim.lr_scheduler import StepLR\n","from torchvision.transforms import Resize\n","from torchaudio.transforms import MFCC\n","from torch.cuda.amp import GradScaler\n","from torchvision import models\n","from torchinfo import summary\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torchaudio.transforms as T\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn as nn\n","import torchaudio\n","import torch\n","\n","# Sklearn-related imports\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.utils.class_weight as class_weight\n","import sklearn.model_selection as model_selection\n","import sklearn.preprocessing as preprocessing\n","import sklearn.metrics as metrics\n","\n","# Audio processing imports\n","from pydub.silence import split_on_silence\n","from pydub import AudioSegment\n","import librosa\n","\n","# Miscellaneous imports\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import wandb\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset, dataloader, and model parameters"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:36:26.946355Z","iopub.status.busy":"2024-03-21T19:36:26.946043Z","iopub.status.idle":"2024-03-21T19:36:26.987132Z","shell.execute_reply":"2024-03-21T19:36:26.986282Z","shell.execute_reply.started":"2024-03-21T19:36:26.946327Z"},"trusted":true},"outputs":[],"source":["# Set seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Path to data\n","#data = pd.read_csv(\"/kaggle/input/filtered-csv/filtered_audio_data.csv\")\n","#data[\"uuid\"] = data[\"uuid\"].str.replace(\"../Dataset/MP3/\", \"/kaggle/input/covid-19-audio-classification/MP3/\")\n","data = pd.read_csv(\"misc_data/filtered_audio_data.csv\")\n","\n","# Class labels\n","labels = [\"healthy\", \"symptomatic\", \"COVID-19\"]\n","\n","# Silence arguments\n","min_silence = 500\n","threshold_dBFS = -40\n","keep_silence = 300\n","\n","# Dataloader and dataset arguments\n","batch = 16\n","workers = 4\n","pin_memory = True\n","dataset_type = \"undersampled\" # weighted or unsampled\n","undersampling = 500\n","\n","# Settings for MelSpectrogram computation\n","melkwargs = {\n","    \"n_mels\": 60,  # How many mel frequency filters are used\n","    \"n_fft\": 350,  # How many fft components are used for each feature\n","    \"win_length\": 350,  # How many frames are included in each window\n","    \"hop_length\": 100,  # How many frames the window is shifted for each component\n","    \"center\": False,  # Whether frams are padded such that the component of timestep t is centered at t\n","    \"f_max\": 11000,  # Maximum frequency to consider\n","    \"f_min\": 0,\n","}\n","n_mfcc = 22\n","sample_rate = 22000\n","\n","# Model type\n","\"\"\"\n","Models:\n","    \"resnet18\":\n","    \"resnet34\":\n","    \"resnet50\":\n","    \"vgg_bn\":\n","    \"multi_resnet\":\n","    \"modified_multi_resnet\"\n","    \"modified_multi_resnet_spectral\"\n","\"\"\"\n","model_type = \"modified_multi_resnet_spectral\"\n","model_arch = \"resnet18\"\n","model_output = \"modified_multi_resnet18_spectral_undersampled\"\n","\n","# Model training and Weights and Biases variables\n","lr = 0.0001\n","step = 3 # steplr\n","decay = 0.001 # L2 regularization\n","gamma = 0.5 # steplr\n","optimizer_type = \"adam\" # adam or sgd\n","scheduler_type = \"steplr\" # stepplateau or steplr\n","factor = 0.1 # stepplateau\n","patience = 2 # stepplateau\n","thresh_plat = 0.001 # stepplateau\n","cooldown = 0 # stepplateau\n","epochs = 25\n","ID = \"Simon\"\n","runName = \"Test run 1\"\n","arch = \"Multi Input ResNet18 Spectral\"\n","desc = \"This model is trained on MFCC features, numeric features, and spectral features.\"\n","dataset = \"COVID-19 Audio Classification\"\n","weighted = False"]},{"cell_type":"markdown","metadata":{},"source":["# Setup weights and biases logging"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:36:26.988552Z","iopub.status.busy":"2024-03-21T19:36:26.988249Z","iopub.status.idle":"2024-03-21T19:37:05.266978Z","shell.execute_reply":"2024-03-21T19:37:05.265933Z","shell.execute_reply.started":"2024-03-21T19:36:26.988527Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\simon\\.netrc\n"]},{"data":{"text/html":["Finishing last run (ID:bju6jn8b) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f90593ca7f684780ba30c47adfbb6748","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.150 MB of 0.150 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">Test run 1</strong> at: <a href='https://wandb.ai/avs8-dl-24/Final-Mini-Project/runs/bju6jn8b' target=\"_blank\">https://wandb.ai/avs8-dl-24/Final-Mini-Project/runs/bju6jn8b</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>.\\wandb\\run-20240402_170007-bju6jn8b\\logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:bju6jn8b). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1dbf179802e4881950d2cc3ec2fc9ff","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011988888889188982, max=1.0…"]},"metadata":{},"output_type":"display_data"}],"source":["#Initialize wandb\n","!wandb login --relogin 9be53a0c7076cae09612be80ee5e0e80d9dac79c\n","\n","#Defining weights and biases config\n","wandb.init(\n","   # set the wandb project where this run will be logged\n","   project=\"Final-Mini-Project\",\n","   name=runName,\n","   notes=desc,\n","   config={\n","   \"ID\": ID,\n","   \"architecture\": arch,\n","   \"dataset\": dataset,\n","   \"learning_rate\": lr,\n","   \"step_size\": step,\n","   \"weight_decay\": decay,\n","   \"optimizer\": optimizer_type,\n","   \"scheduler\": scheduler_type,\n","   \"gamma\": gamma,\n","   \"epochs\": epochs,\n","   \"factor\": factor,\n","   \"threshold\": thresh_plat,\n","   \"patience\": patience,\n","   \"cooldown\": cooldown\n","   }\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Setup and define custom dataset class\n","\n","The custom dataset class finds each raw audio sample and corresponding label, encodes the label and returns the raw audio sample as mono-channel as well as the label."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:05.269727Z","iopub.status.busy":"2024-03-21T19:37:05.268990Z","iopub.status.idle":"2024-03-21T19:37:05.871653Z","shell.execute_reply":"2024-03-21T19:37:05.870666Z","shell.execute_reply.started":"2024-03-21T19:37:05.269692Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def remove_silence(audio_object, min_silence_ms=100, threshold_dBFS=-40, keep_silence=100, seek_step=1):\n","    # Check for loudness (DEBUGGING)\n","    # loudness_dBFS = audio_object.dBFS\n","    # print(\"Loudness (dBFS):\", loudness_dBFS)\n","\n","    # Attempt to split and remove silence from the audio signal\n","    audio_segments = split_on_silence(audio_object, min_silence_ms, threshold_dBFS, keep_silence, seek_step)\n","\n","    # Check if audio_segments is empty if yes return the original audio object as numpy array\n","    if not audio_segments:\n","\n","        # Get the array of samples from the audio segment\n","        org_audio = np.array(audio_object.get_array_of_samples(), dtype=np.float32)\n","\n","        # Normalize the samples if needed\n","        org_audio /= np.max(np.abs(org_audio))\n","\n","        return org_audio\n","\n","    # Add the different audio segments together\n","    audio_processed = sum(audio_segments)\n","\n","    # Return the samples from the processed audio, save as numpy array, and normalize it\n","    audio_processed = np.array(audio_processed.get_array_of_samples(), dtype=np.float32)\n","    audio_processed /= np.max(np.abs(audio_processed))\n","\n","    return audio_processed\n","\n","\n","def encode_age(age):\n","    # Define age mapping\n","    age_mapping = {\"child\": 0, \"teen\": 1, \"adult\": 2, \"senior\": 3}\n","\n","    # Determine age range\n","    if age <= 12:  # Children from ages 0-12\n","        return age_mapping[\"child\"]\n","    elif age <= 19:  # Teenagers from ages 13-19\n","        return age_mapping[\"teen\"]\n","    elif age <= 50:  # Adults from ages 20-50\n","        return age_mapping[\"adult\"]\n","    else:  # Seniors (age > 50)\n","        return age_mapping[\"senior\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-03-21T19:37:05.874599Z","iopub.status.busy":"2024-03-21T19:37:05.874274Z","iopub.status.idle":"2024-03-21T19:37:06.561002Z","shell.execute_reply":"2024-03-21T19:37:06.559863Z","shell.execute_reply.started":"2024-03-21T19:37:05.874572Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class AudioDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, args, label_encoder=None):\n","        # Initialize attributes\n","        self.data = data[\"uuid\"]\n","        self.label = data[\"status\"]\n","        self.age = data[\"age\"]\n","        self.gender = data[\"gender\"]\n","        self.SNR = data[\"SNR\"]\n","        self.label_encoder = label_encoder\n","        self.min_silence = args[0]\n","        self.threshold = args[1]\n","        self.keep_silence = args[2]\n","        self.sample_rate = {}\n","\n","    def __len__(self):\n","        return len(self.label)\n","\n","    def __getitem__(self, idx):\n","        # Extract audio sample from idx\n","        audio_path = self.data[idx]\n","\n","        # Load in audio\n","        audio_object = AudioSegment.from_file(audio_path)\n","        audio_sample = remove_silence(audio_object, self.min_silence, self.threshold, self.keep_silence)\n","        self.sample_rate[idx] = audio_object.frame_rate\n","\n","        # Extract audio label from idx and transform\n","        audio_label = [self.label[idx]]\n","        audio_label = self.label_encoder.transform(audio_label)\n","\n","        # Extract age, gender, and SNR from idx and encode the necessary features\n","        gender_mapping = {\"male\": 0, \"female\": 1}\n","        gender = np.array([gender_mapping[self.gender[idx]]], dtype=np.int8)\n","        age = np.array(encode_age(self.age[idx]), dtype=np.int8)\n","        snr = np.array([self.SNR[idx]])\n","\n","        # Check if audio sample is stereo -> convert to mono (remove_silence already turns it into 1 channel)\n","        # if len(audio_sample.shape) > 1 and audio_sample.shape[1] > 1:\n","        # Convert stereo audio to mono\n","        # audio_sample = audio_sample.mean(dim=0, keepdim=True)\n","\n","        return (\n","            torch.tensor(audio_sample, dtype=torch.float32),\n","            torch.tensor(audio_label, dtype=torch.int32),\n","            torch.tensor(gender, dtype=torch.int32),\n","            torch.tensor(age, dtype=torch.int32),\n","            torch.tensor(snr, dtype=torch.float32),\n","        )\n","\n","    def __get_sample_rate__(self, idx):\n","        # If needed extract sample rate\n","        return self.sample_rate.get(idx)"]},{"cell_type":"markdown","metadata":{},"source":["# Custom collate function\n","\n","The following collate function will take batches of raw audio samples and zero pad them to match the largest sized audio sample."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:06.562571Z","iopub.status.busy":"2024-03-21T19:37:06.562243Z","iopub.status.idle":"2024-03-21T19:37:07.414253Z","shell.execute_reply":"2024-03-21T19:37:07.413057Z","shell.execute_reply.started":"2024-03-21T19:37:06.562538Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def pad_sequence(batch):\n","    # Make all tensor in a batch the same length by padding with zeros\n","    batch = [item.t() for item in batch]\n","    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.0)\n","\n","    return batch.unsqueeze(1)  # Add channel dimension for MFCC input\n","\n","\n","def collate_fn(batch):\n","    # A data tuple has the form:\n","    # waveform, label\n","\n","    # Separate audio samples and labels\n","    waveforms, labels, genders, ages, snrs = zip(*batch)\n","\n","    # Pad the audio samples (if needed)\n","    # padded_waveforms = pad_sequence(waveforms)\n","\n","    # Convert labels to tensor\n","    labels = torch.tensor(labels, dtype=torch.int32)\n","\n","    # Stack numeric features into a tensor and normalize them (if needed)\n","    # scaler = StandardScaler()\n","    genders = torch.tensor(genders)\n","    ages = torch.tensor(ages)\n","    snrs = torch.tensor(snrs)\n","    numeric_features = torch.stack((genders, ages, snrs), dim=1)\n","    # numeric_features = torch.tensor(scaler.fit_transform(numeric_features))\n","\n","    return waveforms, labels, numeric_features"]},{"cell_type":"markdown","metadata":{},"source":["# Miscellaneous functions\n","\n","The following code block contains miscellaneous functions such as plotting of waveforms, spectograms, fbank, and preprocessing of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-03-21T19:37:07.416712Z","iopub.status.busy":"2024-03-21T19:37:07.416106Z","iopub.status.idle":"2024-03-21T19:37:08.086465Z","shell.execute_reply":"2024-03-21T19:37:08.085520Z","shell.execute_reply.started":"2024-03-21T19:37:07.416674Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def waveform_plot(signal, sr, title, threshold=None, plot=None):\n","    # Calculate time axis\n","    time = np.arange(0, len(signal)) / sr\n","\n","    # Plot standard waveform\n","    plt.figure(figsize=(10, 8))\n","    plt.subplot(3, 1, 1)\n","    plt.plot(time, signal, color=\"b\")\n","    plt.xlabel(\"Time (s)\")\n","    plt.ylabel(\"Amplitude\")\n","    plt.title(title)\n","    plt.grid(True)\n","    plt.show()\n","\n","    if plot:\n","        # Calculate dBFS values\n","        if np.any(signal != 0):\n","            db_signal = 20 * np.log10(np.abs(signal) / np.max(np.abs(signal)))\n","        else:\n","            db_signal = -60\n","\n","        plt.subplot(3, 1, 2)\n","        # Plot waveform in dB scale\n","        plt.plot(time, db_signal, color=\"b\")\n","\n","        # Plot threshold level\n","        if threshold:\n","            plt.axhline(y=threshold, color=\"r\", linestyle=\"--\", label=f\"{threshold} dBFS Threshold\")\n","            plt.legend()\n","\n","        plt.xlabel(\"Time (s)\")\n","        plt.ylabel(\"Amplitude (dBFS)\")\n","        plt.title(title)\n","        plt.grid(True)\n","\n","        n_fft = 2048  # Length of the FFT window\n","        hop_length = 512  # Hop length for FFT\n","        S = np.abs(librosa.stft(signal.astype(float), n_fft=n_fft, hop_length=hop_length))\n","\n","        # Convert amplitude to dB scale (sound pressure level)\n","        S_db = librosa.amplitude_to_db(S, ref=np.max)\n","\n","        # Get frequency bins corresponding to FFT\n","        freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n","\n","        # Step 3: Plot the SPL values over frequency\n","        plt.subplot(3, 1, 3)\n","        plt.plot(freqs, np.mean(S_db, axis=1), color=\"b\")\n","        plt.title(\"Sound Pressure Level (SPL) vs. Frequency\")\n","        plt.xlabel(\"Frequency (Hz)\")\n","        plt.ylabel(\"SPL (dB)\")\n","        plt.grid(True)\n","        plt.xlim([20, 25000])  # Set frequency range for better visualization\n","        plt.xscale(\"log\")  # Use log scale for frequency axis\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","\n","# Stolen from pytorch tutorial xd\n","def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", batch=0, idx=0, ax=None):\n","    if ax is None:\n","        fig, ax = plt.subplots(1, 1)\n","    if title is not None:\n","        ax.set_title(title)\n","    ax.set_ylabel(ylabel)\n","    im = ax.imshow(\n","        librosa.power_to_db(specgram),\n","        origin=\"lower\",\n","        aspect=\"auto\",\n","        interpolation=\"nearest\",\n","    )\n","    plt.colorbar(im, ax=ax, label=\"dB\")\n","    # plt.close()\n","    plt.savefig(f\"test_outputs/batch{batch}_idx{idx}_{title}.png\")\n","\n","\n","def plot_fbank(fbank, title=None):\n","    fig, axs = plt.subplots(1, 1)\n","    axs.set_title(title or \"Filter bank\")\n","    axs.imshow(fbank, aspect=\"auto\")\n","    axs.set_ylabel(\"frequency bin\")\n","    axs.set_xlabel(\"mel bin\")\n","\n","\n","def preprocess_data(data_meta_path, data_dir_path, output_dir):\n","    # Read data file then remove every column other than the specified columns\n","    # Removes empty samples and filters through cough probability\n","    data = pd.read_csv(data_meta_path, sep=\",\")\n","    data = data[[\"uuid\", \"cough_detected\", \"SNR\", \"age\", \"gender\", \"status\"]].loc[data[\"cough_detected\"] >= 0.8].dropna().reset_index(drop=True).sort_values(by=\"cough_detected\")\n","    data = data[(data[\"gender\"] != \"other\")]\n","\n","    # Count the occurrences of each age value\n","    age_counts = data[\"age\"].value_counts()\n","\n","    # Filter out ages with fewer than 100 samples\n","    ages_to_keep = age_counts.index[age_counts >= 100]\n","\n","    # Filter the DataFrame based on the selected ages\n","    data = data[data[\"age\"].isin(ages_to_keep)]\n","\n","    # Check if the following MP3 with uuid exists\n","    mp3_data = []\n","    non_exist = []\n","    for file in data[\"uuid\"]:\n","        if os.path.exists(os.path.join(data_dir_path, f\"{file}.mp3\")):\n","            mp3_data.append(os.path.join(data_dir_path, f\"{file}.mp3\"))\n","        else:\n","            non_exist.append(file)\n","\n","    # Remove entries with missing MP3 files from the original data\n","    data = data[~data[\"uuid\"].isin(non_exist)]\n","\n","    # Replace the uuids with the path to uuid\n","    data[\"uuid\"] = mp3_data\n","\n","    # Save the data as csv\n","    data.to_csv(os.path.join(output_dir, \"filtered_audio_data.csv\"), index=False)\n","\n","    print(\"Finished processing!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:08.088269Z","iopub.status.busy":"2024-03-21T19:37:08.087907Z","iopub.status.idle":"2024-03-21T19:37:08.871044Z","shell.execute_reply":"2024-03-21T19:37:08.869898Z","shell.execute_reply.started":"2024-03-21T19:37:08.088232Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["'\\ndata_path = r\"misc_data/metadata_compiled.csv\"\\ndata_dir_path = r\"../Dataset/MP3/\"\\noutput_dir = r\"misc_data/\"\\npreprocess_data(data_path, data_dir_path, output_dir)\\n'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","data_path = r\"misc_data/metadata_compiled.csv\"\n","data_dir_path = r\"../Dataset/MP3/\"\n","output_dir = r\"misc_data/\"\n","preprocess_data(data_path, data_dir_path, output_dir)\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Dataset specific functions\n","\n","The following codeblock contains functions specially related to the dataset preprocessing."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:08.874453Z","iopub.status.busy":"2024-03-21T19:37:08.873328Z","iopub.status.idle":"2024-03-21T19:37:09.651045Z","shell.execute_reply":"2024-03-21T19:37:09.650151Z","shell.execute_reply.started":"2024-03-21T19:37:08.874422Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def preprocess_dataset(data, test_size):\n","    # Extract audio samples and labels\n","    X = data.drop(columns=[\"status\"])\n","    y = data[\"status\"]\n","\n","    # Perform a stratified split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n","\n","    # Combine audio samples and target labels for training and validation sets\n","    train_data = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n","    test_data = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n","\n","    return train_data, test_data\n","\n","\n","def weighted_sample(data):\n","    # Find class distribution\n","    class_counts = data[\"status\"].value_counts()\n","\n","    # Adjust weighting to each sample\n","    sample_weights = [1 / class_counts[i] for i in data[\"status\"].values]\n","\n","    return sample_weights\n","\n","\n","def undersample(data, minority_class_label, n):\n","    # Identify minority class\n","    minority_class = minority_class_label\n","\n","    # Calculate desired class distribution (e.g., balanced distribution)\n","    desired_class_count = n  # Target number of samples for each class\n","\n","    # Select subset from minority class\n","    undersampled_data_minority = data[data[\"status\"] == minority_class].sample(n=desired_class_count)\n","\n","    # Combine with samples from majority classes\n","    undersampled_data_majority = data[~(data[\"status\"] == minority_class)]\n","\n","    # Combine undersampled minority class with majority classes\n","    undersampled_data = pd.concat([undersampled_data_majority, undersampled_data_minority])\n","\n","    # Shuffle the undersampled dataset\n","    undersampled_data = undersampled_data.sample(frac=1).reset_index(drop=True)\n","\n","    return undersampled_data\n","\n","\n","def visualize_dataset(data, normalize, title, feature=\"status\"):\n","    print(f\"{title} Distribution\")\n","    print(data[feature].value_counts(normalize=normalize))\n","    print(\"Total samples\", len(data))\n","\n","    plt.figure(figsize=(6, 4))\n","    plt.title(f\"Histogram of Patient Status\\n- {title}\")\n","    plt.bar(data[feature].value_counts().index, data[feature].value_counts())\n","    plt.xticks(rotation=20, ha=\"right\", fontsize=8)\n","    plt.xlabel(\"Class\", fontsize=8)\n","    plt.ylabel(\"Frequency\", fontsize=8)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization of dataset and dataset loader\n","\n","This codeblock includes the initialization of the dataset as well as any processing needed, such as splitting it into training/testing datasets, as well as different sampling techniques, such as undersampling/weighted sampling."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:09.653258Z","iopub.status.busy":"2024-03-21T19:37:09.652883Z","iopub.status.idle":"2024-03-21T19:37:10.426293Z","shell.execute_reply":"2024-03-21T19:37:10.425310Z","shell.execute_reply.started":"2024-03-21T19:37:09.653223Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def select_dataset(dataset_type=\"undersampled\", train_data=None, val_data=None, test_data=None, args=None, batch=None, workers=None, undersampling=500):\n","    if dataset_type == \"undersampled\":\n","        # Prepare and create undersampled version\n","        undersampled_data = undersample(data, \"healthy\", undersampling)\n","        undersampled_data = undersample(undersampled_data, \"symptomatic\", undersampling)\n","\n","        # train_undersampled_data, test_undersampled_data = preprocess_dataset(undersampled_data, 0.3) # ORIGINAL\n","        train_undersampled_data, val_undersampled_data = preprocess_dataset(undersampled_data, 0.3)\n","        val_undersampled_data, test_undersampled_data = preprocess_dataset(val_undersampled_data, 0.5)\n","\n","        # Undersampled dataset\n","        train_dataset = AudioDataset(train_undersampled_data, args, le)\n","        val_dataset = AudioDataset(val_undersampled_data, args, le)\n","        test_dataset = AudioDataset(test_undersampled_data, args, le)\n","        \n","        #visualize_dataset(train_undersampled_data, False, \"Train\", \"gender\")\n","        #visualize_dataset(val_undersampled_data, False, \"Val\", \"gender\")\n","        \n","        train_dataloader = DataLoader(\n","            train_dataset,\n","            batch_size=batch,\n","            shuffle=True,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        val_dataloader = DataLoader(\n","            val_dataset,\n","            batch_size=batch,\n","            shuffle=False,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=batch,\n","            shuffle=False,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","\n","        return train_dataloader, val_dataloader, test_dataloader\n","\n","    elif dataset_type == \"weighted\":\n","        # Prepare and create weighted sampler\n","        train_sample_weights = weighted_sample(train_data)\n","        val_sample_weights = weighted_sample(val_data)\n","        test_sample_weights = weighted_sample(test_data)\n","\n","        train_weighted_Sampler = WeightedRandomSampler(weights=train_sample_weights, num_samples=len(train_data), replacement=True)\n","        val_weighted_Sampler = WeightedRandomSampler(weights=val_sample_weights, num_samples=len(val_data), replacement=True)\n","        test_weighted_Sampler = WeightedRandomSampler(weights=test_sample_weights, num_samples=len(test_data), replacement=True)\n","\n","        # Create dataset and dataloader instances\n","        train_dataset = AudioDataset(train_data, args, le)\n","        val_dataset = AudioDataset(val_data, args, le)\n","        test_dataset = AudioDataset(test_data, args, le)\n","\n","        train_dataloader = DataLoader(\n","            train_dataset,\n","            sampler=train_weighted_Sampler,\n","            batch_size=batch,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        val_dataloader = DataLoader(\n","            val_dataset,\n","            sampler=val_weighted_Sampler,\n","            batch_size=batch,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            sampler=test_weighted_Sampler,\n","            batch_size=batch,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","\n","        return train_dataloader, val_dataloader, test_dataloader\n","\n","\n","# Initialize LabelEncoder\n","le = LabelEncoder()\n","\n","# Fit and transform labels into encoded form\n","encoded_labels = le.fit_transform(labels)\n","\n","# Silence removal arguments\n","args = [min_silence, threshold_dBFS, keep_silence]\n","\n","# Prepare standard dataset\n","train_data, test_data = preprocess_dataset(data, 0.3)  # First split the original dataset into 70% training\n","val_data, test_data = preprocess_dataset(test_data, 0.5)  # Second split the \"test_data\" into 50/50 validation and test (technically 15/15)\n","\n","# Initialize dataloaders\n","train_dataloader, val_dataloader, test_dataloader = select_dataset(dataset_type, train_data, val_data, test_data, args, batch, workers, undersampling)"]},{"cell_type":"markdown","metadata":{},"source":["# MFCC feature extractor\n","\n","In the following codeblock the MFCC specific parameters are defined and initialized. The codeblock also includes a function that pads the extracted MFCC features in order to pass it to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:10.427876Z","iopub.status.busy":"2024-03-21T19:37:10.427580Z","iopub.status.idle":"2024-03-21T19:37:11.126078Z","shell.execute_reply":"2024-03-21T19:37:11.125045Z","shell.execute_reply.started":"2024-03-21T19:37:10.427845Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def visualize_spectrogram(spectrogram, title):\n","    \"\"\"\n","    Visualize the spectrogram.\n","    \n","    Args:\n","    - spectrogram (Tensor): Input spectrogram tensor, assumed to be in dB scale.\n","    \"\"\"\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(spectrogram, aspect='auto', origin='lower', cmap='viridis')\n","    plt.colorbar(label='Magnitude (dB)')\n","    plt.xlabel('Time')\n","    plt.ylabel('Frequency Bin')\n","    plt.title(f'Spectrogram Visualization {title}')\n","    plt.show()\n","\n","def MFCC_Features(data, specaugment=False, padding=False, normalize=False, resize=False):\n","    \"\"\"\n","    Args:\n","    data: Input audio waveform\n","    max_length: Maximum length for padding\n","    normalize: Normalize the channel layer\n","    resize: Resize the spectrogram\n","    target_size: Target size for resizing\n","    \"\"\"\n","    # Extract MFCC features\n","    # features = mfcc(data)\n","    features = [torch.unsqueeze(mfcc(waveform), 0) for waveform in data]  # Adding channels\n","    # features = [torch.unsqueeze(torch.unsqueeze(mfcc(waveform), 0), 0) for waveform in data] # Adding batch size and channels\n","\n","    if specaugment:\n","        # Applying time stretching, time and frequency masking\n","        stretch = T.TimeStretch()\n","        timeMask = T.TimeMasking(time_mask_param=80)\n","        freqMask = T.FrequencyMasking(freq_mask_param=80)\n","        features = [stretch(feature, 0.8) for feature in features]\n","        for feature in features:\n","            print(feature.shape)\n","        features = pad_sequence(features, batch_first=True, padding_value=0.0)\n","        features = timeMask(features)\n","        features = freqMask(features)\n","        visualize_spectrogram(features[0], \"'Before' normalization\")\n","        \n","        \n","    \n","    # Hardcoded padding (WIP)\n","    if padding:\n","        features = pad_sequence(features, batch_first=True, padding_value=0.0)\n","        #features = F.pad(features, (0, padding - features.shape[3]), \"constant\", 0)\n","\n","    # Normalize the features for each sample\n","    if normalize == True:\n","        #for j, feature in enumerate(features):\n","        #    mean = feature.mean(dim=[1, 2], keepdim=True)\n","        #    std = feature.std(dim=[1, 2], keepdim=True)\n","        #    features[j] = (feature - mean) / std\n","        \n","        mean = torch.mean(features, dim=0)\n","        std = torch.std(features, dim=0)\n","        features = F.normalize(features, mean=mean, std=std)\n","        visualize_spectrogram(features[0], \"After normalization\")\n","\n","    # Resize mel spectrograms\n","    if resize == True:\n","        features = [Resize((224, 1500), antialias=True)(feature) for feature in features]\n","\n","        # Stack each feature as [batch_size, channels, features, length]\n","        features = torch.stack(features)\n","\n","    return features\n","\n","\n","# Instantiate MFCC feature extractor\n","mfcc = MFCC(n_mfcc=n_mfcc, sample_rate=sample_rate, melkwargs=melkwargs)"]},{"cell_type":"markdown","metadata":{},"source":["# Spectral features extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:11.128291Z","iopub.status.busy":"2024-03-21T19:37:11.127705Z","iopub.status.idle":"2024-03-21T19:37:11.736376Z","shell.execute_reply":"2024-03-21T19:37:11.735545Z","shell.execute_reply.started":"2024-03-21T19:37:11.128263Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def spectral_centroid(S=None, sr=22050, nfft=2048, h_length=512):\n","    return librosa.feature.spectral_centroid(S=S, sr=sr, n_fft=nfft, hop_length=h_length)[0]\n","\n","\n","def root_mean_square(S=None, f_length=2048, h_length=512):\n","    return librosa.feature.rms(S=S, frame_length=f_length, hop_length=h_length)[0]\n","\n","\n","def zero_crossing_rate(signal, f_length=2048, h_length=512):\n","    return librosa.feature.zero_crossing_rate(y=signal, frame_length=f_length, hop_length=h_length)[0]\n","\n","\n","def dynamic_parameters(audio_sample, sr=48000):\n","    duration_seconds = len(audio_sample) / sr\n","\n","    if duration_seconds <= 0.5:  # Very short audio (less than 0.5 seconds)\n","        n_fft = 256\n","        hop_length = 64\n","        frame_length = 256\n","    elif 0.5 < duration_seconds <= 1:  # Short audio (0.5 - 1 second)\n","        n_fft = 1024\n","        hop_length = 256\n","        frame_length = 1024\n","    elif 1 < duration_seconds <= 5:  # Medium-length audio (1-5 seconds)\n","        n_fft = 2048\n","        hop_length = 512\n","        frame_length = 2048\n","    else:  # Long audio (longer than 5 seconds)\n","        n_fft = 4096\n","        hop_length = 1024\n","        frame_length = 4096\n","    return n_fft, hop_length, frame_length\n","\n","\n","def plot_spectral_features(ZCR, RMS, SC, STFT=None, sample_rate=48000, hop_length=None, title=None):\n","    # Plot STFT spectrogram\n","    plt.figure(figsize=(12, 8))\n","\n","    plt.subplot(4, 1, 1)\n","    librosa.display.specshow(librosa.amplitude_to_db(np.abs(STFT), ref=np.max), sr=sample_rate, x_axis='time', y_axis='log')\n","    plt.colorbar(format='%+2.0f dB')\n","    plt.title(f'{title} STFT Spectrogram')\n","\n","    # Plot ZCR\n","    plt.figure(figsize=(10, 6))\n","    plt.subplot(4, 1, 2)\n","    plt.plot(ZCR, color='blue')\n","    plt.title('Zero Crossing Rate (ZCR)')\n","    plt.xlabel('Time')\n","    plt.ylabel('ZCR Value')\n","\n","    # Plot RMS\n","    plt.subplot(4, 1, 3)\n","    plt.plot(RMS, color='red')\n","    plt.title('Root Mean Square (RMS)')\n","    plt.xlabel('Time')\n","    plt.ylabel('RMS Value')\n","\n","    # Plot SC\n","    plt.subplot(4, 1, 4)\n","    plt.plot(SC, color='green')\n","    plt.title('Spectral Centroid (SC)')\n","    plt.xlabel('Time')\n","    plt.ylabel('SC Value')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def spectral_features(y, spectral=False):\n","    ZCR_features = []\n","    RMS_features = []\n","    SC_features = []\n","    STFT_features = []\n","    for sample in y:\n","        sample = np.asarray(sample)\n","\n","        # Return n_fft, hop_length, frame_length based on length of sample\n","        n_fft, hop_length, frame_length = dynamic_parameters(sample)\n","\n","        # Compute magnitude spectrum of sample\n","        STFT = librosa.stft(y=sample, n_fft=n_fft, hop_length=hop_length, win_length=n_fft, window=\"hann\", center=True, pad_mode=\"constant\")\n","        S, _ = librosa.magphase(STFT)\n","\n","        # Compute ZCR, RMS, and SC for additional feature extraction\n","        ZCR = zero_crossing_rate(signal=sample, f_length=frame_length, h_length=hop_length)\n","        RMS = root_mean_square(S=S, f_length=frame_length, h_length=hop_length)\n","        SC = spectral_centroid(S=S, sr=48000, nfft=n_fft, h_length=hop_length)\n","\n","        # Convert into tensors and normalize\n","        ZCR = torch.tensor(ZCR, dtype=torch.float32)\n","        RMS = torch.tensor(RMS, dtype=torch.float32)\n","        SC = torch.tensor(SC, dtype=torch.float32)\n","        ZCR = (ZCR - ZCR.mean()) / ZCR.std()\n","        RMS = (RMS - RMS.mean()) / RMS.std()\n","        SC = (SC - SC.mean()) / SC.std()\n","        STFT = (STFT - STFT.mean()) / STFT.std()\n","        \n","        # Convert STFT into spectrogram\n","        STFT = torch.tensor(librosa.amplitude_to_db(np.abs(STFT), ref=np.max), dtype=torch.float32).unsqueeze(0)\n","        \n","        # Append the features for the current signal to the respective lists\n","        ZCR_features.append(ZCR)\n","        RMS_features.append(RMS)\n","        SC_features.append(SC)\n","        if spectral:\n","            STFT_features.append(STFT)\n","\n","\n","    # Compute the maximum length of features the combined features    \n","    max_len = max(max((len(zcr), len(rms), len(sc))) for zcr, rms, sc in zip(ZCR_features, RMS_features, SC_features))\n","    if spectral:\n","        STFT_len_dim1 = max(stft.shape[1] for stft in STFT_features)\n","        STFT_len_dim2 = max(stft.shape[2] for stft in STFT_features)\n","    \n","    # Pad each feature vector to the max length and resize the STFT spectrograms\n","    ZCR_features = [F.pad(zcr, (0, max_len - zcr.shape[0]), value=0.0) for zcr in ZCR_features]\n","    RMS_features = [F.pad(rms, (0, max_len - rms.shape[0]), value=0.0) for rms in RMS_features]\n","    SC_features = [F.pad(sc, (0, max_len - sc.shape[0]), value=0.0) for sc in SC_features]\n","    if spectral:\n","        #STFT_features = [Resize((STFT_len_dim1, STFT_len_dim2), antialias=True)(STFT) for STFT in STFT_features] # Original\n","        STFT_features = [Resize((500, 300), antialias=True)(STFT) for STFT in STFT_features]\n","    \n","    # Stack each list of tensors to create new shape:\n","    # [batch_size, channels, feature_length]\n","    ZCR_features = torch.stack(ZCR_features, dim=0).unsqueeze(1)\n","    RMS_features = torch.stack(RMS_features, dim=0).unsqueeze(1)\n","    SC_features = torch.stack(SC_features, dim=0).unsqueeze(1)\n","    if spectral:\n","        STFT_features = torch.stack(STFT_features, dim=0)\n","    \n","    return ZCR_features, RMS_features, SC_features, STFT_features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:11.738038Z","iopub.status.busy":"2024-03-21T19:37:11.737668Z","iopub.status.idle":"2024-03-21T19:37:12.465965Z","shell.execute_reply":"2024-03-21T19:37:12.465013Z","shell.execute_reply.started":"2024-03-21T19:37:11.738001Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["'\\nfor i, (inputs, targets, numeric) in tqdm(\\n    enumerate(train_dataloader),\\n    total=len(train_dataloader),\\n    leave=True):\\n    # Convert features and target labels into long\\n    ZCR, RMS, SC, STFT = spectral_features(inputs)\\n    print(\"zcr batch\", ZCR.shape)\\n    print(\"rms batch\", RMS.shape)\\n    print(\"sc batch\", SC.shape)\\n    print(\"stft batch\", STFT.shape)\\n    for ZCR, RMS, SC, STFT in zip(ZCR, RMS, SC, STFT):\\n        # Plot STFT spectrogram\\n        plt.figure(figsize=(12, 8))\\n\\n        plt.subplot(4, 1, 1)\\n        plt.imshow(np.array(STFT.squeeze(0)), aspect=\\'auto\\', origin=\\'lower\\')\\n        plt.colorbar(format=\\'%+2.0f dB\\')\\n        plt.xlabel(\\'Time (s)\\')\\n        plt.ylabel(\\'Frequency (Hz)\\')\\n        plt.title(\\'STFT Spectrogram\\')\\n\\n        # Plot ZCR\\n        plt.figure(figsize=(10, 6))\\n        plt.subplot(4, 1, 2)\\n        plt.plot(np.array(ZCR.squeeze()), color=\\'blue\\')\\n        plt.title(\\'Zero Crossing Rate (ZCR)\\')\\n        plt.xlabel(\\'Time\\')\\n        plt.ylabel(\\'ZCR Value\\')\\n\\n        # Plot RMS\\n        plt.subplot(4, 1, 3)\\n        plt.plot(np.array(RMS.squeeze()), color=\\'red\\')\\n        plt.title(\\'Root Mean Square (RMS)\\')\\n        plt.xlabel(\\'Time\\')\\n        plt.ylabel(\\'RMS Value\\')\\n\\n        # Plot SC\\n        plt.subplot(4, 1, 4)\\n        plt.plot(np.array(SC.squeeze()), color=\\'green\\')\\n        plt.title(\\'Spectral Centroid (SC)\\')\\n        plt.xlabel(\\'Time\\')\\n        plt.ylabel(\\'SC Value\\')\\n\\n        plt.tight_layout()\\n        plt.show()\\n\\n    \\n    if i == 0: break\\n    \\n'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","for i, (inputs, targets, numeric) in tqdm(\n","    enumerate(train_dataloader),\n","    total=len(train_dataloader),\n","    leave=True):\n","    # Convert features and target labels into long\n","    ZCR, RMS, SC, STFT = spectral_features(inputs)\n","    print(\"zcr batch\", ZCR.shape)\n","    print(\"rms batch\", RMS.shape)\n","    print(\"sc batch\", SC.shape)\n","    print(\"stft batch\", STFT.shape)\n","    for ZCR, RMS, SC, STFT in zip(ZCR, RMS, SC, STFT):\n","        # Plot STFT spectrogram\n","        plt.figure(figsize=(12, 8))\n","\n","        plt.subplot(4, 1, 1)\n","        plt.imshow(np.array(STFT.squeeze(0)), aspect='auto', origin='lower')\n","        plt.colorbar(format='%+2.0f dB')\n","        plt.xlabel('Time (s)')\n","        plt.ylabel('Frequency (Hz)')\n","        plt.title('STFT Spectrogram')\n","\n","        # Plot ZCR\n","        plt.figure(figsize=(10, 6))\n","        plt.subplot(4, 1, 2)\n","        plt.plot(np.array(ZCR.squeeze()), color='blue')\n","        plt.title('Zero Crossing Rate (ZCR)')\n","        plt.xlabel('Time')\n","        plt.ylabel('ZCR Value')\n","\n","        # Plot RMS\n","        plt.subplot(4, 1, 3)\n","        plt.plot(np.array(RMS.squeeze()), color='red')\n","        plt.title('Root Mean Square (RMS)')\n","        plt.xlabel('Time')\n","        plt.ylabel('RMS Value')\n","\n","        # Plot SC\n","        plt.subplot(4, 1, 4)\n","        plt.plot(np.array(SC.squeeze()), color='green')\n","        plt.title('Spectral Centroid (SC)')\n","        plt.xlabel('Time')\n","        plt.ylabel('SC Value')\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    \n","    if i == 0: break\n","    \n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Network architectures"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MultiInputResNet(nn.Module):\n","    def __init__(self, weights=None, num_classes=3, model_arch=\"resnet18\"):\n","        super(MultiInputResNet, self).__init__()\n","        if model_arch == \"resnet18\":\n","            self.resnet = models.resnet18(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet34\":\n","            self.resnet = models.resnet34(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet50\":\n","            self.resnet = models.resnet50(weights=weights, num_classes=num_classes)\n","\n","        # Adjust the first convolutional layer to match number of channels\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","        # Add additional branch to handle numeric features\n","        self.numeric_features = nn.Sequential(\n","            nn.Linear(3, 64),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","            nn.Linear(64, 512),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","        )\n","\n","        # Adding a fully connected layer\n","        self.fc = nn.Linear(512 + 3, num_classes)  # 512 from ResNet + 3 from numeric features\n","\n","    def forward(self, mfcc, numeric):\n","        # Process MFCC input through ResNet\n","        mfcc_resnet_output = self.resnet(mfcc)\n","\n","        # Process numeric features through additional branch\n","        numeric_output = self.numeric_features(numeric)\n","\n","        # Concatenate the outputs from both branches\n","        combined_features = torch.cat((mfcc_resnet_output, numeric_output), dim=1)\n","\n","        # return raw scores/logits\n","        output = self.fc(combined_features)\n","\n","        # Apply softmax activation to get probabilities\n","        # output_probs = F.softmax(output, dim=1)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Modified_MultiInputResNet(nn.Module):\n","    def __init__(self, weights=None, num_classes=3, model_arch=\"resnet18\"):\n","        super(Modified_MultiInputResNet, self).__init__()\n","        if model_arch == \"resnet18\":\n","            self.resnet = models.resnet18(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet34\":\n","            self.resnet = models.resnet34(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet50\":\n","            self.resnet = models.resnet50(weights=weights, num_classes=num_classes)\n","\n","        # Adjust the first convolutional layer to match number of channels\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","        # Add additional branch to handle numeric features\n","        self.numeric_features = nn.Sequential(\n","            nn.Linear(3, 64),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","            nn.Linear(64, 512),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","        )\n","\n","        # Adding a fully connected layer\n","        self.fc = nn.Linear(512 + 3, num_classes)  # 512 from ResNet + 3 from numeric features\n","\n","        # Adding fully connected layers for age and gender prediction\n","        self.fc_age = nn.Linear(3, 4)\n","        self.fc_gender = nn.Linear(3, 2)\n","\n","    def forward(self, mfcc, numeric):\n","        # Process MFCC input through ResNet\n","        mfcc_resnet_output = self.resnet(mfcc)\n","\n","        # Process numeric features through additional branch\n","        numeric_output = self.numeric_features(numeric)\n","        numeric_output2 = F.relu(numeric)\n","\n","        # Concatenate the outputs from both branches\n","        combined_features = torch.cat((mfcc_resnet_output, numeric_output), dim=1)\n","\n","        output_class = self.fc(combined_features)\n","        output_age = self.fc_age(numeric_output2)\n","        output_gender = self.fc_gender(numeric_output2)\n","\n","        ## Apply softmax activation to get probabilities\n","        # output_probs_class = F.softmax(output_class, dim=1)\n","        # output_probs_age = F.softmax(output_age, dim=1)\n","        # output_probs_gender = F.softmax(output_gender, dim=1)\n","\n","        return output_class, output_age, output_gender"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:13.061890Z","iopub.status.busy":"2024-03-21T19:37:13.061566Z","iopub.status.idle":"2024-03-21T19:37:13.772872Z","shell.execute_reply":"2024-03-21T19:37:13.771912Z","shell.execute_reply.started":"2024-03-21T19:37:13.061853Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class MultiInputResNet_spectral(nn.Module):\n","    def __init__(self, weights=None, num_classes=3, model_arch=\"resnet18\"):\n","        super(MultiInputResNet_spectral, self).__init__()\n","        if model_arch == \"resnet18\":\n","            self.resnet = models.resnet18(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet34\":\n","            self.resnet = models.resnet34(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet50\":\n","            self.resnet = models.resnet50(weights=weights, num_classes=num_classes)\n","\n","        # Adjust the first convolutional layer to match number of channels\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","        # Modify final fully connected layer to output relevant features\n","        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 512)\n","        \n","        # Add additional branch to handle numeric features\n","        self.numeric_features = nn.Sequential(\n","            nn.Linear(1, 64),\n","            nn.ReLU(inplace=False),\n","            nn.Dropout(0.5),\n","            nn.Linear(64, 512),\n","            nn.ReLU(inplace=False),\n","            #nn.Dropout(0.5),\n","        )\n","\n","        # Add additional branch to handle spectral features\n","        self.spectral_features = nn.Sequential(\n","            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=False), \n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=False), \n","            nn.MaxPool1d(kernel_size=2), \n","            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=False), \n","            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=False), \n","            nn.AdaptiveAvgPool1d(1)\n","        )\n","\n","        # Adding a fully connected layer\n","        self.fc = nn.Sequential(\n","            nn.Linear(3584, 3000),   # 512 from Resnet, 3x512 from numeric features, and 3*512 from spectral features\n","            nn.ReLU(), \n","            nn.Dropout(0.1),\n","            nn.Linear(3000, 2500),\n","            #nn.ReLU(),\n","            #nn.Dropout(0.2),\n","            #nn.Linear(2500, 1500),\n","            #nn.ReLU(),\n","            #nn.Dropout(0.3),\n","            #nn.Linear(1500, 1000),\n","            #nn.ReLU(),\n","            #nn.Linear(1000, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(2500, num_classes)\n","        )\n","        \n","        self.fc_stft = nn.Sequential(\n","            nn.Linear(4096, 3000), # STFT\n","            nn.ReLU(), \n","            nn.Dropout(0.1),\n","            nn.Linear(3000, 2500),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(2500, num_classes)\n","        )\n","        \n","        # Adding fully connected layers for age and gender prediction\n","        self.fc_age = nn.Sequential(\n","            nn.Linear(3584, 1792), # original\n","            #nn.Linear(4096, 1792),\n","            nn.Dropout(0.1),\n","            nn.Linear(1792, 896),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(896, 448),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(448, 224),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(224, 112),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(112, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(64, 4)\n","        )\n","\n","        self.fc_gender = nn.Sequential(\n","            nn.Linear(3584, 1792), # original\n","            #nn.Linear(4096, 1792),\n","            nn.ReLU(), \n","            nn.Dropout(0.3),\n","            nn.Linear(1792, 896),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(896, 448),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(448, 224),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(224, 112),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(112, 64),\n","            nn.Sigmoid(),\n","            nn.Linear(64, 2)\n","        )\n","\n","    def forward(self, mfcc, numeric, ZCR, RMS, SC, STFT=None):\n","        # Process MFCC input through ResNet\n","        mfcc_resnet_output = self.resnet(mfcc)\n","        \n","        # Process STFT input through ResNet\n","        if STFT:\n","            stft_resnet_output = self.resnet(STFT)\n","\n","        # Process numeric features through the additional branch\n","        gender_output = self.numeric_features(numeric[:, 0].unsqueeze(1))\n","        age_output = self.numeric_features(numeric[:, 1].unsqueeze(1))\n","        SNR_output = self.numeric_features(numeric[:, 2].unsqueeze(1))\n","\n","        # Process each spectral features through the additional branch\n","        ZCR_output = self.spectral_features(ZCR).squeeze(dim=2)\n","        RMS_output = self.spectral_features(RMS).squeeze(dim=2)\n","        SC_output = self.spectral_features(SC).squeeze(dim=2)\n","        \n","        # Combine the features\n","        if STFT:\n","            combined_features = torch.cat((mfcc_resnet_output,stft_resnet_output, gender_output, age_output, SNR_output, ZCR_output, RMS_output, SC_output), dim=1)\n","        else:\n","            combined_features = torch.cat((mfcc_resnet_output, gender_output, age_output, SNR_output, ZCR_output, RMS_output, SC_output), dim=1)\n","        \n","        # Classification output\n","        if STFT:\n","            output_class = self.fc_stft(combined_features)\n","        else:\n","            output_class = self.fc(combined_features)\n","            \n","        \n","        # Age and gender predictions\n","        output_age = self.fc_age(combined_features)\n","        output_gender = self.fc_gender(combined_features)\n","        \n","        return output_class, output_age, output_gender\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training and validation functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:21.799474Z","iopub.status.busy":"2024-03-21T19:37:21.799105Z","iopub.status.idle":"2024-03-21T19:37:22.545834Z","shell.execute_reply":"2024-03-21T19:37:22.544795Z","shell.execute_reply.started":"2024-03-21T19:37:21.799438Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def initialize_training_setup(model, optimizer_type, scheduler_type, weighted=False, device=None):\n","    scheduler = None\n","    criterion2 = None\n","    if optimizer_type == \"adam\":\n","        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n","\n","    elif optimizer_type == \"sgd\":\n","        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=decay)\n","\n","    # Compute class weights for loss function (if using weighted)\n","    if weighted == True:\n","        train_labels = train_data[\"status\"]\n","        class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n","        class_weights = torch.tensor(class_weights, dtype=torch.float32)\n","        print(\"COVID-19\",\"Healthy\", \"Symptomatic\")\n","        print(class_weights)\n","        class_weights = class_weights.to(device)\n","        criterion = nn.CrossEntropyLoss(weight=class_weights)\n","        criterion2 = nn.CrossEntropyLoss()\n","    else:\n","        criterion = nn.CrossEntropyLoss()\n","        \n","    if scheduler_type == \"steplr\":\n","        scheduler = StepLR(optimizer, step_size=step, gamma=gamma)\n","\n","    elif scheduler_type == \"stepplateau\":\n","        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, threshold=thresh_plat, cooldown=cooldown, min_lr=0)\n","\n","    return optimizer, criterion, scheduler, criterion2\n","\n","\n","def train_epoch_weighted(model, device, epoch, optimizer, criterion, criterion2):\n","    model.train()\n","    running_loss = 0.0\n","    correct_predictions_class = 0\n","    correct_predictions_age = 0\n","    correct_predictions_gender = 0\n","    total_predictions_class = 0\n","    total_predictions_age = 0\n","    total_predictions_gender = 0\n","\n","    print(\"Currently: Training\")\n","    for i, (inputs, targets, numeric) in tqdm(\n","        enumerate(train_dataloader),\n","        total=len(train_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Training\",\n","    ):\n","        # Convert features and target labels into long\n","        class_targets = torch.round(targets).to(torch.long)\n","        gender_targets = torch.round(numeric[:, 0]).to(torch.long)\n","        age_targets = torch.round(numeric[:, 1]).to(torch.long)\n","\n","        # Load them onto GPU\n","        class_targets = class_targets.to(device)\n","        gender_targets = gender_targets.to(device)\n","        age_targets = age_targets.to(device)\n","        \n","        # Extract features\n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        #ZCR, RMS, SC, STFT = spectral_features(inputs) # Compute spectral features \n","        ZCR, RMS, SC, _ = spectral_features(inputs) # Compute spectral features \n","        features, numeric= features.to(device), numeric.to(device) # Load them onto GPU\n","        #ZCR, RMS, SC, STFT = ZCR.to(device), RMS.to(device), SC.to(device), STFT.to(device)\n","        ZCR, RMS, SC = ZCR.to(device), RMS.to(device), SC.to(device)\n","\n","        # Training loop\n","        optimizer.zero_grad()  # Zero the parameters\n","        #output_class, output_age, output_gender = model(features, numeric, ZCR, RMS, SC, STFT)  # Retrieve the output from the model\n","        output_class, output_age, output_gender = model(features, numeric, ZCR, RMS, SC)  # Retrieve the output from the model\n","\n","        # Compute the loss for each output separately\n","        loss_class = criterion2(output_class, class_targets)\n","        loss_gender = criterion(output_gender, gender_targets)\n","        loss_age = criterion(output_age, age_targets)\n","\n","        # Compute total loss - add weights to mitigate the avg high training loss\n","        total_loss = (1.0 * loss_class) + (0.1 * loss_age) + (0.1 * loss_gender)\n","        total_loss.backward()  # Compute gradients of the loss\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping to prevent exploding gradients\n","        optimizer.step()  # Update weights\n","\n","        running_loss += total_loss.item()\n","\n","        # Compute correct predictions and update statistics for class prediction\n","        _, predicted_class = torch.max(output_class, 1)\n","        correct_predictions_class += (predicted_class == class_targets).sum().item()\n","        total_predictions_class += class_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for age prediction\n","        _, predicted_age = torch.max(output_age, 1)\n","        correct_predictions_age += (predicted_age == age_targets).sum().item()\n","        total_predictions_age += age_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for gender prediction\n","        _, predicted_gender = torch.max(output_gender, 1)\n","        correct_predictions_gender += (predicted_gender == gender_targets).sum().item()\n","        total_predictions_gender += gender_targets.size(0)\n","\n","        # Print statistics for every 5th mini-batch\n","        if i % 5 == 4:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i+1}/{len(train_dataloader)} | Total Training Loss: {total_loss.item():.4f}\")\n","            print(f\"Class Training Loss: {loss_class:.4f} | Gender Training Loss: {loss_gender:.4f} | Age Training Loss: {loss_age:.4f}\")\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    avg_loss = running_loss / len(train_dataloader)\n","    avg_accuracy_class = correct_predictions_class / total_predictions_class\n","    avg_accuracy_age = correct_predictions_age / total_predictions_age\n","    avg_accuracy_gender = correct_predictions_gender / total_predictions_gender\n","    print()\n","    print(\"=========================================================\")\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","    print(f\"Avg Training loss: {avg_loss:.4f}\")\n","    print(f\"Avg Training Accuracy Class: {avg_accuracy_class:.4f}\")\n","    print(f\"Avg Training Accuracy Age: {avg_accuracy_age:.4f}\")\n","    print(f\"Avg Training Accuracy Gender: {avg_accuracy_gender:.4f}\")\n","    print(\"=========================================================\")\n","    print()\n","    \n","    return avg_loss, avg_accuracy_class, avg_accuracy_age, avg_accuracy_gender\n","\n","\n","def train_epoch_undersampled(model, device, epoch, optimizer, criterion):\n","    model.train()\n","    running_loss = 0.0\n","    correct_predictions_class = 0\n","    correct_predictions_age = 0\n","    correct_predictions_gender = 0\n","    total_predictions_class = 0\n","    total_predictions_age = 0\n","    total_predictions_gender = 0\n","\n","    print(\"Currently: Training\")\n","    for i, (inputs, targets, numeric) in tqdm(\n","        enumerate(train_dataloader),\n","        total=len(train_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Training\",\n","    ):\n","        # Convert features and target labels into long\n","        class_targets = torch.round(targets).to(torch.long)\n","        gender_targets = torch.round(numeric[:, 0]).to(torch.long)\n","        age_targets = torch.round(numeric[:, 1]).to(torch.long)\n","\n","        # Load them onto GPU\n","        class_targets = class_targets.to(device)\n","        gender_targets = gender_targets.to(device)\n","        age_targets = age_targets.to(device)\n","        \n","        # Extract features\n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        ZCR, RMS, SC, _ = spectral_features(inputs) # Compute spectral features \n","        features, numeric= features.to(device), numeric.to(device) # Load them onto GPU\n","        #ZCR, RMS, SC, STFT = ZCR.to(device), RMS.to(device), SC.to(device), STFT.to(device) # STFT\n","        ZCR, RMS, SC = ZCR.to(device), RMS.to(device), SC.to(device)\n","\n","        # Training loop\n","        optimizer.zero_grad()  # Zero the parameters\n","        #output_class, output_age, output_gender = model(features, numeric, ZCR, RMS, SC, STFT)  # Retrieve the output from the model\n","        output_class, output_age, output_gender = model(features, numeric, ZCR, RMS, SC)  # Retrieve the output from the model\n","\n","        # Compute the loss for each output separately\n","        loss_class = criterion(output_class, class_targets)\n","        loss_gender = criterion(output_gender, gender_targets)\n","        loss_age = criterion(output_age, age_targets)\n","\n","        # Compute total loss - add weights to mitigate the avg high training loss\n","        total_loss = (1.0 * loss_class) + (0.1 * loss_age) + (0.1 * loss_gender)\n","        total_loss.backward()  # Compute gradients of the loss\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping to prevent exploding gradients\n","        optimizer.step()  # Update weights\n","\n","        running_loss += total_loss.item()\n","\n","        # Compute correct predictions and update statistics for class prediction\n","        _, predicted_class = torch.max(output_class, 1)\n","        correct_predictions_class += (predicted_class == class_targets).sum().item()\n","        total_predictions_class += class_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for age prediction\n","        _, predicted_age = torch.max(output_age, 1)\n","        correct_predictions_age += (predicted_age == age_targets).sum().item()\n","        total_predictions_age += age_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for gender prediction\n","        _, predicted_gender = torch.max(output_gender, 1)\n","        correct_predictions_gender += (predicted_gender == gender_targets).sum().item()\n","        total_predictions_gender += gender_targets.size(0)\n","\n","        # Print statistics for every 5th mini-batch\n","        if i % 5 == 4:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i+1}/{len(train_dataloader)} | Total Training Loss: {total_loss.item():.4f}\")\n","            print(f\"Class Training Loss: {loss_class:.4f} | Gender Training Loss: {loss_gender:.4f} | Age Training Loss: {loss_age:.4f}\")\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    avg_loss = running_loss / len(train_dataloader)\n","    avg_accuracy_class = correct_predictions_class / total_predictions_class\n","    avg_accuracy_age = correct_predictions_age / total_predictions_age\n","    avg_accuracy_gender = correct_predictions_gender / total_predictions_gender\n","    print()\n","    print(\"=========================================================\")\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","    print(f\"Avg Training loss: {avg_loss:.4f}\")\n","    print(f\"Avg Training Accuracy Class: {avg_accuracy_class:.4f}\")\n","    print(f\"Avg Training Accuracy Age: {avg_accuracy_age:.4f}\")\n","    print(f\"Avg Training Accuracy Gender: {avg_accuracy_gender:.4f}\")\n","    print(\"=========================================================\")\n","    print()\n","    \n","    return avg_loss, avg_accuracy_class, avg_accuracy_age, avg_accuracy_gender\n","\n","\n","def validate_epoch(model, device, epoch, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    correct_predictions_class = 0\n","    correct_predictions_age = 0\n","    correct_predictions_gender = 0\n","    total_predictions_class = 0\n","    total_predictions_age = 0\n","    total_predictions_gender = 0\n","    \n","    precision_class = 0.0\n","    precision_gender = 0.0\n","    precision_age = 0.0\n","    recall_class = 0.0\n","    recall_gender = 0.0\n","    recall_age = 0.0\n","    f1_class = 0.0\n","    f1_gender = 0.0\n","    f1_age = 0.0\n","\n","    print(\"Currently: Validating\")\n","    for j, (vinputs, vtargets, vnumeric) in tqdm(\n","        enumerate(val_dataloader),\n","        total=len(val_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Validating\",\n","    ):\n","        # Convert features and target labels into long\n","        class_targets = torch.round(vtargets).to(torch.long)\n","        gender_targets = torch.round(vnumeric[:, 0]).to(torch.long)\n","        age_targets = torch.round(vnumeric[:, 1]).to(torch.long)\n","\n","        # Load them onto GPU\n","        class_targets = class_targets.to(device)\n","        gender_targets = gender_targets.to(device)\n","        age_targets = age_targets.to(device)\n","        \n","        # Extract features\n","        vfeatures = MFCC_Features(vinputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        #ZCR, RMS, SC, STFT = spectral_features(vinputs)\n","        ZCR, RMS, SC, _ = spectral_features(vinputs)\n","        vfeatures, vnumeric = vfeatures.to(device), vnumeric.to(device)  # Load them onto GPU\n","        #ZCR, RMS, SC, STFT = ZCR.to(device), RMS.to(device), SC.to(device), STFT.to(device)\n","        ZCR, RMS, SC = ZCR.to(device), RMS.to(device), SC.to(device)\n","\n","        # Validation loop\n","        #voutput_class, voutput_age, voutput_gender = model(vfeatures, vnumeric, ZCR, RMS, SC, STFT) # STFT\n","        voutput_class, voutput_age, voutput_gender = model(vfeatures, vnumeric, ZCR, RMS, SC) # STFT\n","\n","        # Compute the loss for each output separately\n","        vloss_class = criterion(voutput_class, class_targets)\n","        vloss_gender = criterion(voutput_gender, gender_targets)\n","        vloss_age = criterion(voutput_age, age_targets)\n","\n","        # Compute total loss - add weights to mitigate the avg high training loss\n","        total_loss = (1.0 * vloss_class) + (0.1 * vloss_age) + (0.1 * vloss_gender)\n","        running_loss += total_loss.item()\n","\n","        # Compute correct predictions and update statistics for class prediction\n","        _, predicted_class = torch.max(voutput_class, 1)\n","        correct_predictions_class += (predicted_class == class_targets).sum().item()\n","        total_predictions_class += class_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for age prediction\n","        _, predicted_age = torch.max(voutput_age, 1)\n","        correct_predictions_age += (predicted_age == age_targets).sum().item()\n","        total_predictions_age += age_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for gender prediction\n","        _, predicted_gender = torch.max(voutput_gender, 1)\n","        correct_predictions_gender += (predicted_gender == gender_targets).sum().item()\n","        total_predictions_gender += gender_targets.size(0)\n","        \n","        # Compute precision, recall, F1 score\n","        precision_class += precision_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","        precision_gender += precision_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","        precision_age += precision_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","        recall_class += recall_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","        recall_gender += recall_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","        recall_age += recall_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","        f1_class += f1_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","        f1_gender += f1_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","        f1_age += f1_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","        \n","        # Print statistics for every 5th mini-batch\n","        if j % 5 == 4:\n","            avg_precision_class_batch = precision_class / (5 * (j + 1))\n","            avg_precision_gender_batch = precision_gender / (5 * (j + 1))\n","            avg_precision_age_batch = precision_age / (5 * (j + 1))\n","            avg_recall_class_batch = recall_class / (5 * (j + 1))\n","            avg_recall_gender_batch = recall_gender / (5 * (j + 1))\n","            avg_recall_age_batch = recall_age / (5 * (j + 1))\n","            avg_f1_class_batch = f1_class / (5 * (j + 1))\n","            avg_f1_gender_batch = f1_gender / (5 * (j + 1))\n","            avg_f1_age_batch = f1_age / (5 * (j + 1))\n","            print()\n","            print(\"=========================================================\")\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {j+1}/{len(val_dataloader)} | Total Validation Loss: {total_loss.item():.4f}\")\n","            print(f\"Class Validation Loss: {vloss_class:.4f} | Gender Validation Loss: {vloss_gender:.4f} | Age Validation Loss: {vloss_age:.4f}\")\n","            print(f\"Precision Class: {avg_precision_class_batch:.4f}  | Precision Age: {avg_precision_age_batch:.4f} | Precision Gender: {avg_precision_gender_batch:.4f}\")\n","            print(f\"Recall Class: {avg_recall_class_batch:.4f}      | Recall Age: {avg_recall_age_batch:.4f} | Recall Gender: {avg_recall_gender_batch:.4f}\")\n","            print(f\"F1 Score Class: {avg_f1_class_batch:.4f}    | F1 Score Age: {avg_f1_age_batch:.4f} | F1 Score Gender: {avg_f1_gender_batch:.4f}\")\n","            print(\"=========================================================\")\n","            print()\n","\n","    # Compute and return average validation loss, accuracy, precision, recall, and F1 score\n","    avg_vloss = running_loss / len(val_dataloader)\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    vaccuracy_class = correct_predictions_class / total_predictions_class\n","    vaccuracy_age = correct_predictions_age / total_predictions_age\n","    vaccuracy_gender = correct_predictions_gender / total_predictions_gender\n","\n","    avg_precision_class = precision_class / len(val_dataloader)\n","    avg_precision_gender = precision_gender / len(val_dataloader)\n","    avg_precision_age = precision_age / len(val_dataloader)\n","    avg_recall_class = recall_class / len(val_dataloader)\n","    avg_recall_gender = recall_gender / len(val_dataloader)\n","    avg_recall_age = recall_age / len(val_dataloader)\n","    avg_f1_class = f1_class / len(val_dataloader)\n","    avg_f1_gender = f1_gender / len(val_dataloader)\n","    avg_f1_age = f1_age / len(val_dataloader)\n","\n","    # return avg_vloss, vaccuracy, precision, recall, f1\n","    metrics = ((vaccuracy_class, vaccuracy_age, vaccuracy_gender), \n","           (avg_precision_class, avg_precision_age, avg_precision_gender), \n","           (avg_recall_class, avg_recall_age, avg_recall_gender), \n","           (avg_f1_class, avg_f1_age, avg_f1_gender))\n","\n","    return avg_vloss, metrics\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training and validating model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:22.548189Z","iopub.status.busy":"2024-03-21T19:37:22.547782Z","iopub.status.idle":"2024-03-21T19:37:24.134454Z","shell.execute_reply":"2024-03-21T19:37:24.133630Z","shell.execute_reply.started":"2024-03-21T19:37:22.548154Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device running on: cuda\n","Only 1 GPU available!\n"]}],"source":["def select_model(model_type, weighted=None):\n","    if model_type == \"resnet18\":\n","        model = models.resnet18(weights=weighted, num_classes=3)\n","        model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"resnet34\":\n","        model = models.resnet50(weights=weighted, num_classes=3)\n","        model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"resnet50\":\n","        model = models.resnet50(weights=weighted, num_classes=3)\n","        model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"vgg_bn\":\n","        model = models.vgg16_bn(weights=weighted, num_classes=3)\n","        model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"multi_resnet\":\n","        model = MultiInputResNet(weights=weighted, num_classes=3, model_arch=model_arch)\n","        return model\n","    elif model_type == \"modified_multi_resnet\":\n","        model = Modified_MultiInputResNet(weights=weighted, num_classes=3, model_arch=model_arch)\n","        return model\n","    elif model_type == \"modified_multi_resnet_spectral\":\n","        model = MultiInputResNet_spectral(weights=weighted, num_classes=3, model_arch=model_arch)\n","        return model\n","\n","\n","# Initialize model\n","model = select_model(model_type)\n","\n","# Set the model to training mode and put it on GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","print(f\"Device running on: {device}\")\n","\n","# Wrap model with DataParallel if multiple GPUs are available\n","if torch.cuda.device_count() > 1:\n","    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model)\n","else:\n","    print(\"Only 1 GPU available!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T19:37:24.138172Z","iopub.status.busy":"2024-03-21T19:37:24.137888Z"},"jupyter":{"source_hidden":true},"scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch | :   0%|          | 0/25 [00:00<?, ?it/s]\n"]},{"ename":"TypeError","evalue":"train_epoch_undersampled() takes 5 positional arguments but 6 were given","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Training and validation loop\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), total\u001b[38;5;241m=\u001b[39mepochs, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch | \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# avg_loss, accuracy = train_epoch(model, model_type, device, epoch, optimizer, criterion)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# avg_loss, accuracy_class, accuracy_age, accuracy_gender = train_epoch_modified(model, device, epoch, optimizer, criterion)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     avg_loss, accuracy_class, accuracy_age, accuracy_gender \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_undersampled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# avg_vloss, vaccuracy, precision, recall, f1 = validate_epoch(model, device, epoch, criterion)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# avg_vloss, metrics = validate_epoch_modified(model, device, epoch, criterion)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     avg_vloss, metrics \u001b[38;5;241m=\u001b[39m validate_epoch(model, device, epoch, criterion)\n","\u001b[1;31mTypeError\u001b[0m: train_epoch_undersampled() takes 5 positional arguments but 6 were given"]}],"source":["# Initialize training setup\n","optimizer, criterion, scheduler, criterion2 = initialize_training_setup(model, optimizer_type, scheduler_type, weighted, device)\n","best_vloss = float(\"inf\")\n","model_no = 0\n","\n","# Create a GradScaler instance\n","scaler = GradScaler()\n","\n","# Training and validation loop\n","for epoch in tqdm(range(epochs), total=epochs, leave=True, desc=f\"Epoch | \"):\n","    # avg_loss, accuracy = train_epoch(model, model_type, device, epoch, optimizer, criterion)\n","    # avg_loss, accuracy_class, accuracy_age, accuracy_gender = train_epoch_modified(model, device, epoch, optimizer, criterion)\n","    avg_loss, accuracy_class, accuracy_age, accuracy_gender = train_epoch_undersampled(model, device, epoch, optimizer, criterion)\n","    # avg_vloss, vaccuracy, precision, recall, f1 = validate_epoch(model, device, epoch, criterion)\n","    # avg_vloss, metrics = validate_epoch_modified(model, device, epoch, criterion)\n","    avg_vloss, metrics = validate_epoch(model, device, epoch, criterion)\n","\n","    # Print and log metrics\n","    # MultiResnet model\n","    # print(f\"Epoch #{epoch+1} | Training Loss: {avg_loss:.4f} | Validation Loss: {avg_vloss:.4f} | Validation Accuracy: {vaccuracy:.4f}\")\n","    # print(f\"Epoch #{epoch+1} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n","    # print(f\"Epoch #{epoch+1} | Learning Rate: {scheduler.get_last_lr()}\")\n","\n","    # Extract metrics\n","    vaccuracy = metrics[0]\n","    vprecision = metrics[1]\n","    vrecall = metrics[2]\n","    vf1 = metrics[3]\n","\n","    # ModifiedMultiResnet model\n","    print(f\"Epoch #{epoch+1} | Training Loss: {avg_loss:.4f} | Validation Loss: {avg_vloss:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class Accuracy: {vaccuracy[0]:.4f} | Age Accuracy: {vaccuracy[1]:.4f} | Gender Accuracy: {vaccuracy[2]:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class Precision: {vprecision[0]:.4f} | Precision: {vprecision[1]:.4f} | Precision: {vprecision[2]:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class Recall: {vrecall[0]:.4f} | Age Recall: {vrecall[1]:.4f} | Gender Recall: {vrecall[2]:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class F1 score: {vf1[0]:.4f} | Age F1 score: {vf1[1]:.4f} | Gender F1 Score: {vf1[2]:.4f}\")\n","\n","    \n","    # Update learning rate\n","    if scheduler_type == \"steplr\":\n","        scheduler.step()\n","    elif scheduler_type == \"stepplateau\":\n","        scheduler.step(avg_vloss)\n","\n","    # Log metrics to wandb\n","    # MultiResnet model\n","    # wandb.log({\n","    #        \"epoch\": epoch + 1,\n","    #        \"train_loss\": avg_loss,\n","    #        \"train_acc\": accuracy,\n","    #        \"val_loss\": avg_vloss,\n","    #        \"val_accuracy\": vaccuracy,\n","    #    })\n","    # wandb.log({\"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n","\n","    # ModifiedMultiResnet model accuracy_class, accuracy_age, accuracy_gender\n","    wandb.log({\n","        \"epoch\": epoch + 1,\n","        \"avg_train_loss\": avg_loss,\n","        \"avg_val_loss\": avg_vloss,\n","        \"train_class_accuracy\": accuracy_class,\n","        \"train_age_accuracy\": accuracy_age,\n","        \"train_gender_accuracy\": accuracy_gender,\n","        \"val_class_accuracy\": vaccuracy[0],\n","        \"val_age_accuracy\": vaccuracy[1],\n","        \"val_gender_accuracy\": vaccuracy[2],\n","        \"val_class_precision\": vprecision[0],\n","        \"val_age_precision\": vprecision[1],\n","        \"val_gender_precision\": vprecision[2],\n","        \"val_class_recall\": vrecall[0],\n","        \"val_age_recall\": vrecall[1],\n","        \"val_gender_recall\": vrecall[2],\n","        \"val_class_f1_score\": vf1[0],\n","        \"val_age_f1_score\": vf1[1],\n","        \"val_gender_f1_score\": vf1[2]\n","    })\n","\n","    # Track best performance, and save the model's state\n","    if avg_vloss < best_vloss:\n","        best_vloss = avg_vloss\n","        model_no += 1\n","        if not os.path.exists(\"models\"):\n","            os.makedirs(\"models\")\n","        model_path = f\"models/{model_output}_no_{model_no}_epoch_{epoch+1}.pth\"\n","        torch.save(model.state_dict(), model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4560779,"sourceId":7791391,"sourceType":"datasetVersion"},{"datasetId":4561341,"sourceId":7792152,"sourceType":"datasetVersion"},{"datasetId":4609644,"sourceId":7858628,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
