{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Setup and define custom dataset class\n","\n","The custom dataset class finds each raw audio sample and corresponding label, encodes the label and returns the raw audio sample as mono-channel as well as the label.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-03-12T07:36:59.917232Z","iopub.status.busy":"2024-03-12T07:36:59.916787Z","iopub.status.idle":"2024-03-12T07:37:03.063858Z","shell.execute_reply":"2024-03-12T07:37:03.062795Z","shell.execute_reply.started":"2024-03-12T07:36:59.917192Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchaudio\n","\n","class AudioDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, label_encoder=None):\n","        # Initialize attributes\n","        self.data = data[\"uuid\"]\n","        self.label = data[\"status\"]\n","        self.label_encoder = label_encoder\n","        self.sample_rate = {}\n","\n","    def __len__(self):\n","        return len(self.label)\n","\n","    def __getitem__(self, idx):\n","        # Extract audio sample from idx\n","        audio_path = self.data[idx]\n","        print(\"audio_path\", audio_path)\n","\n","        # Load in audio\n","        audio_sample, sample_rate = torchaudio.load(audio_path)\n","        self.sample_rate[idx] = sample_rate\n","        \n","        # Extract audio label from idx and transform\n","        audio_label = [self.label[idx]]\n","        audio_label = self.label_encoder.transform(audio_label)\n","        \n","        # Check if audio sample is stereo -> convert to mono\n","        if audio_sample.shape[0] > 1:\n","            audio_sample = audio_sample.mean(dim=0, keepdim=True)\n","            \n","        #return audio_sample, torch.tensor(audio_label, dtype=torch.int64)\n","        return audio_sample, sample_rate\n","            \n","\n","    def __get_sample_rate__(self, idx):\n","        # If needed extract sample rate\n","        return self.sample_rate.get(idx)"]},{"cell_type":"markdown","metadata":{},"source":["# Custom collate function\n","\n","The following collate function will take batches of raw audio samples and zero pad them to match the largest sized audio sample.\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def pad_sequence(batch):\n","    # Make all tensor in a batch the same length by padding with zeros\n","    batch = [item.t() for item in batch]\n","    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.0)\n","    return batch.permute(0, 2, 1)\n","\n","\n","def collate_fn(batch):\n","    # A data tuple has the form:\n","    # waveform, label\n","\n","    # Separate audio samples and labels\n","    waveforms, labels = zip(*batch)\n","    \n","    # Pad the audio samples\n","    #padded_waveforms = pad_sequence(waveforms)\n","\n","    # Convert labels to tensor\n","    labels = torch.tensor(labels)\n","\n","    #return waveforms, labels\n","    #return padded_waveforms, labels\n","    return waveforms, labels"]},{"cell_type":"markdown","metadata":{},"source":["# Miscellaneous functions\n","\n","The following code block contains miscellaneous functions such as plotting of waveforms, spectograms, fbank, and preprocessing of the data.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-03-12T07:37:03.078050Z","iopub.status.busy":"2024-03-12T07:37:03.077748Z","iopub.status.idle":"2024-03-12T07:37:03.103055Z","shell.execute_reply":"2024-03-12T07:37:03.101896Z","shell.execute_reply.started":"2024-03-12T07:37:03.078025Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import librosa \n","\n","# Stolen from pytorch tutorial xd\n","def plot_waveform(waveform, sr, title=\"Waveform\", ax=None):\n","    waveform = np.array(waveform)\n","\n","    num_channels, num_frames = waveform.shape\n","    time_axis = torch.arange(0, num_frames) / sr\n","\n","    if ax is None:\n","        _, ax = plt.subplots()\n","    time_axis = np.linspace(0, len(waveform) / sr, num=len(waveform))\n","    ax.plot(time_axis, waveform, linewidth=1)\n","    ax.grid(True)\n","    ax.set_xlim([0, time_axis[-1]])\n","    if title:\n","        ax.set_title(title)\n","    ax.set_xlabel('Time (s)')\n","    ax.set_ylabel('Amplitude')\n","    plt.show()\n","\n","def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n","    if ax is None:\n","        _, ax = plt.subplots(1, 1)\n","    if title is not None:\n","        ax.set_title(title)\n","    ax.set_ylabel(ylabel)\n","    ax.imshow(\n","        librosa.power_to_db(specgram),\n","        origin=\"lower\",\n","        aspect=\"auto\",\n","        interpolation=\"nearest\",\n","    )\n","\n","def plot_fbank(fbank, title=None):\n","    fig, axs = plt.subplots(1, 1)\n","    axs.set_title(title or \"Filter bank\")\n","    axs.imshow(fbank, aspect=\"auto\")\n","    axs.set_ylabel(\"frequency bin\")\n","    axs.set_xlabel(\"mel bin\")\n","    \n","def preprocess_data(data_meta_path, data_dir_path, output_dir):\n","    # Read data file then remove every column other than the specified columns\n","    # Removes empty samples and filters through cough probability\n","    data = pd.read_csv(data_meta_path, sep=\",\")\n","    data = (\n","        data[[\"uuid\", \"cough_detected\", \"SNR\", \"age\", \"gender\", \"status\"]]\n","        .loc[data[\"cough_detected\"] >= 0.8]\n","        .dropna().reset_index(drop=True).sort_values(by='cough_detected')\n","    )\n","    data = data[(data[\"gender\"] != \"other\")]\n","    \n","    #Count the occurrences of each age value\n","    age_counts = data['age'].value_counts()\n","    \n","    # Filter out ages with fewer than 100 samples\n","    ages_to_keep = age_counts.index[age_counts >= 100]\n","\n","    # Filter the DataFrame based on the selected ages\n","    data = data[data['age'].isin(ages_to_keep)]\n","\n","    # Check if the following MP3 with uuid exists\n","    mp3_data = []\n","    non_exist = []\n","    for file in data[\"uuid\"]:\n","        if os.path.exists(os.path.join(data_dir_path, f\"{file}.mp3\")):\n","            mp3_data.append(os.path.join(data_dir_path, f\"{file}.mp3\"))\n","        else:\n","            non_exist.append(file)\n","        # elif os.path.exists(os.path.join(data_dir_path, f'{file}.ogg')):\n","        #    ogg_data.append(os.path.join(data_dir_path, f'{file}.ogg'))\n","\n","    # Remove entries with missing MP3 files from the original data\n","    data = data[~data[\"uuid\"].isin(non_exist)]\n","\n","    # Replace the uuids with the path to uuid\n","    data[\"uuid\"] = mp3_data\n","    \n","    # Save the data as csv\n","    data.to_csv(os.path.join(output_dir, \"filtered_audio_data.csv\"), index=False)\n","\n","    print(\"Finished processing!\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished processing!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","data_path = r\"misc_data/metadata_compiled.csv\"\n","data_dir_path = r\"../Dataset/MP3/\"\n","output_dir = r\"misc_data/\"\n","preprocess_data(data_path, data_dir_path, output_dir)"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset specific functions\n","\n","The following codeblock contains functions specially related to the dataset preprocessing.\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-12T07:37:03.107235Z","iopub.status.busy":"2024-03-12T07:37:03.106892Z","iopub.status.idle":"2024-03-12T07:37:03.898417Z","shell.execute_reply":"2024-03-12T07:37:03.897531Z","shell.execute_reply.started":"2024-03-12T07:37:03.107207Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","def preprocess_dataset(data, test_size):\n","    # Extract audio samples and labels\n","    X = data.drop(columns=[\"status\"])\n","    y = data[\"status\"]\n","\n","\n","    # Perform a stratified split\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=test_size, stratify=y, random_state=42\n","    )\n","\n","    # Combine audio samples and target labels for training and validation sets\n","    train_data = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n","    test_data = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n","\n","    return train_data, test_data\n","\n","def weighted_sample(data):\n","    # Find class distribution\n","    class_counts = data[\"status\"].value_counts()\n","    # print(class_counts)\n","\n","    # Check class weights\n","    class_weights = 1 / class_counts\n","    # print(class_weights)\n","\n","    # Adjust weighting to each sample\n","    sample_weights = [1 / class_counts[i] for i in data[\"status\"].values]\n","    # print(\"len sample weights:\",len(sample_weights))\n","\n","    return sample_weights\n","\n","def undersample(data, n, normalize=False):\n","    # Step 1: Identify majority class\n","    class_counts = data[\"status\"].value_counts()\n","    majority_class = class_counts.idxmax()\n","\n","    # Step 2: Calculate desired class distribution (e.g., balanced distribution)\n","    desired_class_count = n  # Target number of samples for each class\n","\n","    # Step 3: Select subset from majority class\n","    undersampled_data_majority = data[data[\"status\"] == majority_class].sample(\n","        n=desired_class_count\n","    )\n","\n","    # Combine with samples from minority classes\n","    undersampled_data_minority = data[~(data[\"status\"] == majority_class)]\n","\n","    # Combine undersampled majority class with minority classes\n","    undersampled_data = pd.concat(\n","        [undersampled_data_majority, undersampled_data_minority]\n","    )\n","\n","    # Shuffle the undersampled dataset\n","    undersampled_data = undersampled_data.sample(frac=1).reset_index(drop=True)\n","\n","    return undersampled_data\n","\n","def visualize_dataset(data, normalize, title):\n","    print(f\"{title} Distribution\")\n","    print(data[\"status\"].value_counts(normalize=normalize))\n","    print(data[\"gender\"].value_counts(normalize=normalize))\n","    print(data[\"age\"].value_counts(normalize=normalize).sort_index())\n","    print(\"Total samples\", len(data))\n","\n","    plt.figure(figsize=(6, 4))\n","    plt.title(f\"Histogram of Patient Status\\n- {title}\")\n","    plt.bar(data[\"status\"].value_counts().index, data[\"status\"].value_counts())\n","    plt.xticks(rotation=20, ha=\"right\", fontsize=8)\n","    plt.xlabel(\"Class\", fontsize=8)\n","    plt.ylabel(\"Frequency\", fontsize=8)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization of dataset and dataset loader\n","\n","This codeblock includes the initialization of the dataset as well as any processing needed, such as splitting it into training/testing datasets, as well as different sampling techniques, such as undersampling/weighted sampling.\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-03-12T07:37:03.900819Z","iopub.status.busy":"2024-03-12T07:37:03.900390Z","iopub.status.idle":"2024-03-12T07:37:04.126124Z","shell.execute_reply":"2024-03-12T07:37:04.124884Z","shell.execute_reply.started":"2024-03-12T07:37:03.900783Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Standard Distribution\n","status\n","healthy        6517\n","symptomatic    1532\n","COVID-19        483\n","Name: count, dtype: int64\n","gender\n","male      5909\n","female    2623\n","Name: count, dtype: int64\n","age\n","16.0    124\n","17.0    114\n","18.0    184\n","19.0    155\n","20.0    229\n","21.0    224\n","22.0    233\n","23.0    224\n","24.0    278\n","25.0    284\n","26.0    244\n","27.0    256\n","28.0    327\n","29.0    276\n","30.0    363\n","31.0    223\n","32.0    272\n","33.0    277\n","34.0    250\n","35.0    286\n","36.0    221\n","37.0    220\n","38.0    257\n","39.0    224\n","40.0    300\n","41.0    200\n","42.0    258\n","43.0    228\n","44.0    165\n","45.0    220\n","46.0    174\n","47.0    150\n","48.0    145\n","49.0    130\n","50.0    181\n","51.0     95\n","52.0    120\n","53.0    103\n","54.0    102\n","55.0    122\n","56.0     94\n","Name: count, dtype: int64\n","Total samples 8532\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 600x400 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'\\ntrain_undersampled_dataloader = DataLoader(\\n    train_dataset,\\n    batch_size=batch,\\n    shuffle=True,\\n    num_workers=workers,\\n    collate_fn=collate_fn,\\n    pin_memory=True,\\n)\\nval_undersampled_dataloader = DataLoader(\\n    val_dataset,\\n    batch_size=batch,\\n    shuffle=False,\\n    num_workers=workers,\\n    collate_fn=collate_fn,\\n    pin_memory=True,\\n)\\ntest_undersampled_dataloader = DataLoader(\\n    test_dataset,\\n    batch_size=batch,\\n    shuffle=False,\\n    num_workers=workers,\\n    collate_fn=collate_fn,\\n    pin_memory=True,\\n)\\n'"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["from torch.utils.data import WeightedRandomSampler\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","# Set seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Load data\n","#data = pd.read_csv(\"misc_data/audio_data.csv\")\n","data = pd.read_csv(\"misc_data/filtered_audio_data.csv\")\n","#data['uuid'] = data['uuid'].apply(lambda x: x.replace('../Dataset/MP3/', \"/kaggle/input/covid-19-audio-classification/MP3/\"))\n","\n","# Initialize LabelEncoder\n","le = LabelEncoder()\n","\n","# Fit and transform labels into encoded form\n","labels = [\"healthy\", \"symptomatic\", \"COVID-19\"]\n","encoded_labels = le.fit_transform(labels)\n","\n","visualize_dataset(data, None, \"Standard\")\n","\n","# Prepare standard dataset\n","train_data, test_data = preprocess_dataset(data, 0.3) # First split the original dataset into 70% training\n","val_data, test_data = preprocess_dataset(test_data, 0.5) # Second split the \"test_data\" into 50/50 validation and test (or technically 15/15)\n","\n","#filtered_train_data =  train_data[(train_data['gender'] != 'other')].sort_values(by='cough_detected')\n","#filtered_train_data.to_csv(\"filtered data.csv\", index=False)\n","\n","# Prepare and create undersampled version\n","#undersampled_data = undersample(data, 2000, True)\n","#visualize_dataset(undersampled_data, None, \"Standard\")\n","#visualize_dataset(undersampled_data, \"normalize\", \"Normalized\")\n","#train_undersampled_data, test_undersampled_data = preprocess_dataset(undersampled_data, 0.3)\n","#val_undersampled_data, test_undersampled_data = preprocess_dataset(test_undersampled_data, 0.5)\n","#visualize_dataset(train_undersampled_data, None, \"Train\")\n","#visualize_dataset(train_undersampled_data, \"normalize\", \"Train Normalized\")\n","#visualize_dataset(val_undersampled_data, None, \"Validation\")\n","#visualize_dataset(val_undersampled_data, \"normalize\", \"Validation Normalized\")\n","\n","# Prepare and create weighted sampler\n","#train_sample_weights = weighted_sample(train_data)\n","#val_sample_weights = weighted_sample(val_data)\n","#test_sample_weights = weighted_sample(test_data)\n","\n","#train_weighted_Sampler = WeightedRandomSampler(weights=train_sample_weights, num_samples=len(train_data), replacement=True)\n","#val_weighted_Sampler = WeightedRandomSampler(weights=val_sample_weights, num_samples=len(val_data), replacement=True)\n","#test_weighted_Sampler = WeightedRandomSampler(weights=test_sample_weights, num_samples=len(test_data), replacement=True)\n","\n","# Create AudioDataset instances for training and validation sets\n","# Standard dataset\n","train_dataset = AudioDataset(train_data, le)\n","val_dataset = AudioDataset(val_data, le)\n","test_dataset = AudioDataset(test_data, le)\n","\n","# Undersampled dataset\n","#train_undersampled_dataset = AudioDataset(train_undersampled_data, le)\n","#test_undersampled_dataset = AudioDataset(test_undersampled_data, le)\n","\n","# Create training and test dataloader instances\n","batch = 1\n","workers = 0\n","pin_memory = True\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n","#val_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n","#test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n","\n","\"\"\"\n","train_undersampled_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=batch,\n","    shuffle=True,\n","    num_workers=workers,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n",")\n","val_undersampled_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=batch,\n","    shuffle=False,\n","    num_workers=workers,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n",")\n","test_undersampled_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=batch,\n","    shuffle=False,\n","    num_workers=workers,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n",")\n","\"\"\"\n","\n","#train_weighted_dataloader = DataLoader(train_dataset, sampler=train_weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)\n","#val_weighted_dataloader = DataLoader(val_dataset, sampler=val_weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)\n","#test_weighted_dataloader = DataLoader(test_dataset, sampler=test_weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'timee' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtimee\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n","\u001b[1;31mNameError\u001b[0m: name 'timee' is not defined"]}],"source":["print(timee[0])"]},{"cell_type":"markdown","metadata":{},"source":["# Initialize and define MFCC feature extractor\n","\n","In the following codeblock the MFCC specific parameters are defined and initialized. The codeblock also includes a function that pads the extracted MFCC features in order to pass it to the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-12T07:37:04.128408Z","iopub.status.busy":"2024-03-12T07:37:04.127376Z","iopub.status.idle":"2024-03-12T07:37:08.419898Z","shell.execute_reply":"2024-03-12T07:37:08.418711Z","shell.execute_reply.started":"2024-03-12T07:37:04.128368Z"},"trusted":true},"outputs":[],"source":["from torchaudio.transforms import MFCC\n","from torchvision.transforms import Resize\n","import torch.nn.functional as F\n","\n","def MFCC_Features(data, Normalize=False, resize=False):\n","    # Extract MFCC features\n","    features = mfcc(data)\n","    \n","    # Hardcoded padding\n","    features = F.pad(features, (0,6000 - features.shape[3]), \"constant\", 0)\n","    \n","    \n","    # Normalize the features\n","    if Normalize == True:\n","        features = (features - features.mean()) / features.std()\n","    \n","    # Add two artificial channels filled with zeros\n","    if resize == True:\n","        #artificial_channels = torch.zeros(features.shape[0], 2, features.shape[2], features.shape[3])\n","        #features = torch.cat([features, artificial_channels], dim=1)\n","        features = Resize((224,224), antialias=True)(features)\n","    \n","    return features\n","\n","# Settings for MelSpectrogram computation\n","melkwargs = {\n","    \"n_mels\": 80,  # How many mel frequency filters are used\n","    \"n_fft\": 480,  # How many fft components are used for each feature\n","    \"win_length\": 480,  # How many frames are included in each window\n","    \"hop_length\": 160,  # How many frames the window is shifted for each component\n","    \"center\": False,  # Whether frams are padded such that the component of timestep t is centered at t\n","    \"f_max\": 10000,  # Maximum frequency to consider\n","    \"f_min\": 0,\n","}\n","\n","# Instantiate MFCC feature extractor\n","mfcc = MFCC(\n","    n_mfcc=22,  # Number of cepstrum components\n","    sample_rate=22000,  # Sample rate of input audio\n","    melkwargs=melkwargs)  # Keyword arguments for MelSpectogram"]},{"cell_type":"markdown","metadata":{},"source":["# Testing stuff\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def waveform_plot(signal, sr, title, threshold=None):\n","    # Calculate time axis\n","    time = np.arange(0, len(signal)) / sr\n","    \n","    # Calculate dBFS values\n","    if np.any(signal != 0) :\n","        db_signal = 20 * np.log10(np.abs(signal) / np.max(np.abs(signal)))\n","    else:\n","        db_signal = -60\n","    \n","    # Check if signal amplitude is zero\n","    #if np.max(np.abs(signal)) == 0:\n","    #    db_signal = -60  # Set a very low dB value if the signal amplitude is zero\n","    #else:\n","    #    # Calculate dBFS values\n","    #    db_signal = 20 * np.log10(np.abs(signal) / 1)\n","    \n","    \n","    # Calculate dBFS values with handling division by zero\n","    #db_signal = 20 * np.log10((np.abs(signal) + np.finfo(float).eps) / 1)\n","\n","    plt.figure(figsize=(10, 8))\n","    \n","    # Plot standard waveform\n","    plt.subplot(3,1,1) \n","    plt.plot(time, signal, color='b')\n","    plt.xlabel('Time (s)')\n","    plt.ylabel('Amplitude')\n","    plt.title(title)\n","    plt.grid(True)\n","    \n","    plt.subplot(3,1,2)\n","    # Plot waveform in dB scale\n","    plt.plot(time, db_signal, color='b')\n","    \n","    # Plot threshold level\n","    if threshold:\n","        plt.axhline(y=threshold, color='r', linestyle='--', label=f'{threshold} dBFS Threshold')\n","        plt.legend()\n","    \n","    plt.xlabel('Time (s)')\n","    plt.ylabel('Amplitude (dBFS)')\n","    plt.title(title)\n","    plt.grid(True)\n","    \n","    n_fft = 2048  # Length of the FFT window\n","    hop_length = 512  # Hop length for FFT\n","    S = np.abs(librosa.stft(signal.astype(float), n_fft=n_fft, hop_length=hop_length))\n","\n","    # Convert amplitude to dB scale (sound pressure level)\n","    S_db = librosa.amplitude_to_db(S, ref=np.max)\n","\n","    # Get frequency bins corresponding to FFT\n","    freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n","\n","    # Step 3: Plot the SPL values over frequency\n","    plt.subplot(3,1,3)\n","    plt.plot(freqs, np.mean(S_db, axis=1), color='b')\n","    plt.title('Sound Pressure Level (SPL) vs. Frequency')\n","    plt.xlabel('Frequency (Hz)')\n","    plt.ylabel('SPL (dB)')\n","    plt.grid(True)\n","    plt.xlim([20, 25000])  # Set frequency range for better visualization\n","    plt.xscale('log')  # Use log scale for frequency axis\n","    \n","    \n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-12T07:37:52.503252Z","iopub.status.busy":"2024-03-12T07:37:52.502843Z"},"trusted":true},"outputs":[],"source":["from pydub.silence import split_on_silence\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from pydub import AudioSegment\n","import pydub\n","import torch.optim as optim\n","import torchaudio\n","import librosa\n","import torch\n","import os\n","import numpy as np\n","import io\n","import time\n","\n","inputs, labels = next(iter(train_dataloader))\n","#print(inputs := np.array(inputs))\n","#print(labels := np.array(labels))\n","#print(inputs[0][0])\n","#print(labels[0])\n","\n","signal = np.array(inputs[0][0])\n","sr = np.array(labels[0])\n","thresh = -40\n","waveform_plot(signal, sr, \"Standard waveform\", thresh)\n","\n","#y, srr = librosa.load(\"../Dataset/MP3/46064dba-fcc4-42a0-8eb2-89785d549f44.mp3\", sr=None)\n","#plot_dB(y, srr)\n","\n","\n","print()\n","t1 = time.time()\n","aud = AudioSegment.from_file(\"../Dataset/MP3/46064dba-fcc4-42a0-8eb2-89785d549f44.mp3\")\n","\n","print(sample_rate := aud.frame_rate)\n","#aud, srr = librosa.load(\"../Dataset/MP3/46064dba-fcc4-42a0-8eb2-89785d549f44.mp3\", sr=None)\n","# make the audio in pydub audio segment format\n","#aud = AudioSegment(aud.tobytes(),frame_rate = 48000, sample_width = signal.dtype.itemsize, channels = 1)\n","#aud = AudioSegment(signal.tobytes(),frame_rate = 48000, sample_width = signal.dtype.itemsize, channels = 1)\n","#test = AudioSegment.from_numpy_array()\n","\n","#buffer = io.BytesIO()\n","#np.save(buffer,aud)\n","#buffer.seek(0)\n","\n","#aud = AudioSegment.from_file(buffer, format=\"raw\", frame_rate=srr, channels=1, sample_width=aud.dtype.itemsize)\n","\n","# use split on sience method to split the audio based on the silence, \n","# here we can pass the min_silence_len as silent length threshold in ms and intensity thershold\n","\n","audio_chunks = split_on_silence(\n","    aud,\n","    min_silence_len = 500,\n","    silence_thresh = thresh,\n","    keep_silence = 100)\n","\n","#audio chunks are combined here\n","# Putting the file back together\n","combined = AudioSegment.empty()\n","for chunk in audio_chunks:\n","    combined += chunk\n","    #print(chunk)\n","\n","#print(combined)\n","#combined.export(f'audiosegment_output.mp3', format = \"mp3\")\n","audio_processed = sum(audio_chunks)\n","\n","#audio_processed = sum(combined)\n","#audio_processed.export(\"processed_audio.mp3\", format=\"mp3\")\n","\n","\n","\n","#from pydub.playback import play\n","#play(aud)\n","#audio_processed = np.array(audio_processed)\n","audio_processed = np.array(audio_processed.get_array_of_samples(), dtype=np.float32)\n","audio_processed = audio_processed / np.max(np.abs(audio_processed))\n","print(\"Time to process:\", time.time()-t1)\n","print(\"original signal\", signal)\n","print(\"original signal shape\", signal.shape)\n","print(\"audio processed\", audio_processed)\n","print(\"audio processed shape\", audio_processed.shape)\n","\n","# Normalize the processed audio data\n","#max_abs_value = np.max(np.abs(audio_processed))\n","#normalized_audio_processed = audio_processed / np.max(np.abs(audio_processed))\n","\n","# Verify the shape of the normalized audio data\n","#print(\"Normalized audio processed shape:\", normalized_audio_processed.shape)\n","\n","# Verify the range of values in the normalized audio data\n","#print(\"Minimum value:\", np.min(normalized_audio_processed))\n","#print(\"Maximum value:\", np.max(normalized_audio_processed))\n","#output = AudioSegment(audio_processed.tobytes(), frame_rate=48000, sample_width = signal.dtype.itemsize, channels = 1)\n","#output.export(out_f=\"TEST.mp3\", format=\"mp3\")\n","#y, srr = librosa.load(\"audiosegment_output.mp3\", sr=None)\n","#y, srr = librosa.load(\"processed_audio.mp3\", sr=None)\n","#y, srr = librosa.load(\"TEST.mp3\", sr=None)\n","#aud = AudioSegment.from_file(\"audiosegment_output.mp3\")\n","#play(aud)\n","#waveform_plot(y, srr, \"Silence removed\")\n","waveform_plot(audio_processed, 48000, \"Silence removed\")\n","#waveform_plot(normalized_audio_processed, 48000, \"Silence removed\")\n","\n","\"\"\"\n","# Define parameters for silence detection\n","# Adjust these parameters based on your audio\n","min_silence_len = 100  # Minimum length of silence in milliseconds\n","silence_thresh = -60  # Silence threshold in dBFS\n","\n","# Convert NumPy array to AudioSegment\n","audio_segment = AudioSegment(signal.tobytes(), frame_rate=sr, sample_width=2, channels=1)\n","\n","# Split the audio based on silence\n","chunks = split_on_silence(audio_segment, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n","\n","# Concatenate non-silent chunks\n","output_signal = sum(chunks)\n","output_signal = np.array(output_signal.get_array_of_samples())\n","\n","\n","\n","\n","#features = MFCC_Features(inputs, Normalize=True, resize=False)\n","#plot_spectrogram(features[0][0], title=\"Normalized\")\n","#features = MFCC_Features(inputs, Normalize=True, resize=True)\n","#plot_spectrogram(features[0][0], title=\"Normalized Resized\")\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:37:09.051195Z","iopub.status.idle":"2024-03-12T07:37:09.051627Z","shell.execute_reply":"2024-03-12T07:37:09.051432Z","shell.execute_reply.started":"2024-03-12T07:37:09.051415Z"},"trusted":true},"outputs":[],"source":["timemr[e00000]"]},{"cell_type":"markdown","metadata":{},"source":["# Initializing and defining model\n","\n","The following codeblock contains the initialization of the ResNet50 model from the PyTorch library.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:37:09.053190Z","iopub.status.idle":"2024-03-12T07:37:09.054137Z","shell.execute_reply":"2024-03-12T07:37:09.053948Z","shell.execute_reply.started":"2024-03-12T07:37:09.053930Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from torchvision import models\n","import torch.nn as nn\n","\n","# Load in the pre-trained resnet model\n","model = models.vgg16_bn(weights=None, num_classes=3)\n","#model = models.resnet18(weights=None, num_classes=3)\n","#model = models.resnet50()\n","\n","# Modifying the first layer to be able to pass 1-channel image (spectrogram) for ResNet model\n","#num_features = model.fc.in_features\n","#model.fc = nn.Linear(num_features, 3)\n","model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1,1), padding=(1, 1))\n","\n","# Set the model to training mode and put it on GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device running on: {device}\")\n","\n","# Wrap your model with DataParallel\n","model = nn.DataParallel(model)\n","model.to(device); # add \";\" to keep from printing the network architecture"]},{"cell_type":"markdown","metadata":{},"source":["# Setup weights and bias logging\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:37:09.055824Z","iopub.status.idle":"2024-03-12T07:37:09.056221Z","shell.execute_reply":"2024-03-12T07:37:09.056046Z","shell.execute_reply.started":"2024-03-12T07:37:09.056029Z"},"trusted":true},"outputs":[],"source":["import wandb\n","\n","# Initialize wandb\n","#!wandb login --relogin 9be53a0c7076cae09612be80ee5e0e80d9dac79c\n","\n","# Defining training variables\n","lr = 0.001\n","step = 5\n","decay = 0.1\n","optim = \"adam\"\n","gamma = 0.5\n","epochs = 50\n","\n","# Defining weights and biases config\n","#wandb.init(\n","#    # set the wandb project where this run will be logged\n","#    project=\"mini-project\",\n","#    config={\n","#    \"architecture\": \"VGG-16 Batch Normalization\",\n","#    \"dataset\": \"COVID-19 Audio Classification\",\n","#    \"learning_rate\": lr,\n","#    \"step_size\": step,\n","#    \"weight_decay\": decay,\n","#    \"optimizer\": optim,\n","#    \"gamma\": gamma,\n","#    \"epochs\": epochs\n","#    }\n","#)"]},{"cell_type":"markdown","metadata":{},"source":["# Training loop\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:37:09.058133Z","iopub.status.idle":"2024-03-12T07:37:09.058540Z","shell.execute_reply":"2024-03-12T07:37:09.058338Z","shell.execute_reply.started":"2024-03-12T07:37:09.058323Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","from torch.optim.lr_scheduler import StepLR\n","from torch.cuda.amp import GradScaler\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchaudio\n","import torch\n","import os\n","\n","if optim == \"adam\":\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n","else: \n","    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=decay)\n","scheduler = StepLR(optimizer, step_size=step, gamma=0.5)\n","criterion = nn.CrossEntropyLoss()\n","#scaler = GradScaler()\n","log_interval = 20\n","best_vloss = float(\"inf\")\n","model_no = 0\n","\n","print(\"Currently: Training\")\n","for epoch in range(epochs):\n","    model.train() # Initiate training mode\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    \n","    # Training loop\n","    for i, (inputs, targets) in tqdm(enumerate(\n","       train_weighted_dataloader),\n","       total=len(train_weighted_dataloader),\n","       leave=True,\n","       desc=f\"Epoch {epoch+1}/{epochs} | Training\"\n","        ):\n","        #print(f\"========== BATCH {i} ========== \")\n","        #for idx, f in enumerate(features):\n","        #    print(f\"{idx+1} | {f.shape}\")\n","        #print(f\"========== BATCH {i} ========== \")\n","        \n","        features = MFCC_Features(inputs) # Compute the MFCC features\n","        features, targets = features.to(device), targets.to(device) # Load them onto GPU\n","        optimizer.zero_grad() # Zero the parameters\n","        outputs = model(features) # Retrieve the output from the model\n","        loss = criterion(outputs, targets) # Compute the loss\n","        #loss = F.cross_entropy(outputs, targets, reduction='mean')\n","        loss.backward() # Compute gradients of the loss\n","        optimizer.step() # Update weights\n","        \n","        running_loss += loss.item()\n","        \n","        # Calculate accuracy\n","        _, predicted = torch.max(outputs, 1)\n","        correct_predictions += (predicted == targets).sum().item()\n","        total_predictions += targets.size(0)\n","        \n","        if i % log_interval == 0:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i}/{len(train_weighted_dataloader)} | Training Loss: {loss.item():.4f}\")\n","    \n","    # Compute accuracy\n","    accuracy = correct_predictions / total_predictions\n","    \n","    print(f\"Training Accuracy: {accuracy:.4f}\")\n","    \n","    # Compute average training loss for the epoch\n","    avg_loss = running_loss / len(train_weighted_dataloader)\n","        \n","    # Validation loop\n","    running_vloss = 0.0\n","    vcorrect_predictions = 0\n","    vtotal_predictions = 0\n","    model.eval()\n","    with torch.no_grad(): # Disable gradient computation\n","        for j, (vinputs, vtargets) in tqdm(enumerate(\n","            val_weighted_dataloader),\n","            total=len(val_weighted_dataloader),\n","            leave=True,\n","            desc=f\"Epoch {epoch+1}/{epochs} | Validating\"):\n","            vfeatures = MFCC_Features(vinputs) # Compute the MFCC features\n","            vfeatures, vtargets = vfeatures.to(device), vtargets.to(device) # Load them onto GPU\n","            voutputs = model(vfeatures)\n","            vloss = criterion(voutputs, vtargets)\n","            #vloss = F.cross_entropy(voutputs, vtargets, reduction='mean')\n","            running_vloss += vloss.item()\n","            \n","            # Calculate accuracy\n","            _, vpredicted = torch.max(voutputs, 1)\n","            vcorrect_predictions += (vpredicted == vtargets).sum().item()\n","            vtotal_predictions += vtargets.size(0)\n","    \n","    # Compute average validation loss for the epoch\n","    avg_vloss = running_vloss / len(val_weighted_dataloader)\n","    \n","    # Compute accuracy\n","    vaccuracy = vcorrect_predictions / vtotal_predictions\n","    \n","    # Compute precision, recall, F1 score\n","    precision = precision_score(vtargets.cpu(), vpredicted.cpu(), average='macro',zero_division=0.0)\n","    recall = recall_score(vtargets.cpu(), vpredicted.cpu(), average='macro',zero_division=0.0)\n","    f1 = f1_score(vtargets.cpu(), vpredicted.cpu(), average='macro',zero_division=0.0)\n","    \n","    # Log metrics to wandb\n","    wandb.log({\"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n","    wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_loss,\"train_acc\": accuracy, \"val_loss\": avg_vloss, \"val_accuracy\": vaccuracy})\n","    \n","    print(f\"Epoch #{epoch+1} | Training Loss: {avg_loss:.4f} | Validation Loss: {avg_vloss:.4f} | Validation Accuracy: {vaccuracy:.4f}\\n           Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n","                      \n","    # Update learning rate\n","    #print_lr(is_verbose, group, lr, epoch=None)\n","    scheduler.step()\n","    \n","    print(f\"Epoch {epoch+1}, Learning Rate: {scheduler.get_last_lr()}\")\n","    \n","    # Track best performance, and save the model's state\n","    if avg_vloss < best_vloss:\n","        best_vloss = avg_vloss\n","        model_no += 1\n","        if not os.path.exists(\"/kaggle/working/models\"):\n","            os.makedirs('models')\n","        model_path = f\"/kaggle/working/models/VGG16_bn_weighted_model_no_{model_no}_epoch_{epoch+1}.pth\"\n","        torch.save(model.state_dict(), model_path)\n","\n","# Finish the run\n","wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4560779,"sourceId":7791391,"sourceType":"datasetVersion"},{"datasetId":4561341,"sourceId":7792152,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
