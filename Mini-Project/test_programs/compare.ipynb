{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-16T10:13:27.742041Z","iopub.status.busy":"2024-03-16T10:13:27.741664Z","iopub.status.idle":"2024-03-16T10:13:28.146152Z","shell.execute_reply":"2024-03-16T10:13:28.145119Z","shell.execute_reply.started":"2024-03-16T10:13:27.742003Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","#for dirname, _, filenames in os.walk(dir):\n","#    for i, filename in enumerate(filenames):\n","#        print(os.path.join(dirname, filename))\n","#        if i == 5: break\n","\n","#if not os.path.exists(\"/kaggle/working/models\"):\n","#    os.makedirs('models')\n","       \n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# Setup and define custom dataset class\n","\n","The custom dataset class finds each raw audio sample and corresponding label, encodes the label and returns the raw audio sample as mono-channel as well as the label."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:13:28.149855Z","iopub.status.busy":"2024-03-16T10:13:28.149364Z","iopub.status.idle":"2024-03-16T10:13:28.174182Z","shell.execute_reply":"2024-03-16T10:13:28.173234Z","shell.execute_reply.started":"2024-03-16T10:13:28.149828Z"},"trusted":true},"outputs":[],"source":["from pydub.silence import split_on_silence\n","\n","def remove_silence(audio_object, min_silence_ms=100, threshold_dBFS=-40, keep_silence=100, seek_step=1):\n","    # Check for loudness (DEBUGGING)\n","    #loudness_dBFS = audio_object.dBFS\n","    #print(\"Loudness (dBFS):\", loudness_dBFS)\n","\n","    # Attempt to split and remove silence from the audio signal\n","    audio_segments = split_on_silence(audio_object, min_silence_ms, threshold_dBFS, seek_step)\n","\n","    # Check if audio_segments is empty if yes return the original audio object as numpy array\n","    if not audio_segments:\n","\n","        # Get the array of samples from the audio segment\n","        org_audio = np.array(audio_object.get_array_of_samples(), dtype=np.float32)\n","\n","        # Normalize the samples if needed\n","        org_audio /= np.max(np.abs(org_audio))\n","\n","        return org_audio\n","\n","    # Add the different audio segments together\n","    audio_processed = sum(audio_segments)\n","\n","    # Return the samples from the processed audio, save as numpy array, and normalize it\n","    audio_processed = np.array(audio_processed.get_array_of_samples(), dtype=np.float32)\n","    audio_processed /= np.max(np.abs(audio_processed))\n","    #print(\"audio_processed\",audio_processed)\n","    #print(\"audio_processed.shape\",audio_processed.shape)\n","\n","    return audio_processed\n","\n","def encode_age(age):\n","    # Define age mapping\n","    age_mapping = {\"child\": 0, \"teen\": 1, \"adult\": 2, \"senior\": 3}\n","\n","    # Determine age range\n","    if age <= 12:  # Children from ages 0-12\n","        return age_mapping[\"child\"]\n","    elif age <= 19:  # Teenagers from ages 13-19\n","        return age_mapping[\"teen\"]\n","    elif age <= 50:  # Adults from ages 20-50\n","        return age_mapping[\"adult\"]\n","    else:  # Seniors (age > 50)\n","        return age_mapping[\"senior\"]"]},{"cell_type":"code","execution_count":3,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-03-16T10:13:28.176053Z","iopub.status.busy":"2024-03-16T10:13:28.175676Z","iopub.status.idle":"2024-03-16T10:13:32.782733Z","shell.execute_reply":"2024-03-16T10:13:32.781897Z","shell.execute_reply.started":"2024-03-16T10:13:28.176021Z"},"trusted":true},"outputs":[],"source":["from pydub import AudioSegment\n","import torchaudio\n","import torch\n","import numpy as np\n","\n","class AudioDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, args, label_encoder=None):\n","        # Initialize attributes\n","        self.data = data[\"uuid\"]\n","        self.label = data[\"status\"]\n","        self.age = data[\"age\"]\n","        self.gender = data[\"gender\"]\n","        self.SNR = data[\"SNR\"]\n","        self.label_encoder = label_encoder\n","        self.min_silence = args[0]\n","        self.threshold = args[1]\n","        self.keep_silence = args[2]\n","        self.sample_rate = {}\n","\n","    def __len__(self):\n","        return len(self.label)\n","\n","    def __getitem__(self, idx):\n","        # Extract audio sample from idx\n","        audio_path = self.data[idx]\n","        #print(\"Audio path\", audio_path)\n","\n","        # Load in audio\n","        #audio_samples, sample_ratess = torchaudio.load(audio_path)\n","        #print(\"original signal\", audio_samples)\n","        #print(\"original signal.shape\", audio_samples.shape)\n","        #self.sample_rate[idx] = sample_rate\n","        audio_object = AudioSegment.from_file(audio_path)\n","        audio_sample = remove_silence(audio_object, self.min_silence, self.threshold, self.keep_silence)\n","        #print(\"processed signal\", audio_sample)\n","        #print(\"processed signal.shape\", audio_sample.shape)\n","        self.sample_rate[idx] = audio_object.frame_rate\n","\n","\n","        # Extract audio label from idx and transform\n","        audio_label = [self.label[idx]]\n","        audio_label = self.label_encoder.transform(audio_label)\n","\n","        # Extract age, gender, and SNR from idx and encode the necessary features\n","        gender_mapping = {\"male\": 0, \"female\": 1}\n","        gender = np.array([gender_mapping[self.gender[idx]]])\n","        age = np.array(encode_age(self.age[idx]))\n","        snr = np.array([self.SNR[idx]])\n","\n","        # Check if audio sample is stereo -> convert to mono (remove_silence already turns it into 1 channel)\n","        #if len(audio_sample.shape) > 1 and audio_sample.shape[1] > 1:\n","            # Convert stereo audio to mono\n","            #audio_sample = audio_sample.mean(dim=0, keepdim=True)\n","\n","        return torch.tensor(audio_sample, dtype=torch.float32), torch.tensor(audio_label, dtype=torch.int64), torch.tensor(gender), torch.tensor(age), torch.tensor(snr)\n","\n","    def __get_sample_rate__(self, idx):\n","        # If needed extract sample rate\n","        return self.sample_rate.get(idx)"]},{"cell_type":"markdown","metadata":{},"source":["# Custom collate function\n","\n","The following collate function will take batches of raw audio samples and zero pad them to match the largest sized audio sample."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:13:32.785852Z","iopub.status.busy":"2024-03-16T10:13:32.785415Z","iopub.status.idle":"2024-03-16T10:13:32.794056Z","shell.execute_reply":"2024-03-16T10:13:32.793004Z","shell.execute_reply.started":"2024-03-16T10:13:32.785826Z"},"trusted":true},"outputs":[],"source":["def pad_sequence(batch):\n","    # Make all tensor in a batch the same length by padding with zeros\n","    batch = [item.t() for item in batch]\n","    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.0)\n","    #print(\"batch\", batch)\n","    #print(\"batch shape\", batch.shape)\n","    return batch.unsqueeze(1) # Add channel dimension for MFCC input\n","    #return batch.permute(0, 2, 1)\n","\n","\n","def collate_fn(batch):\n","    # A data tuple has the form:\n","    # waveform, label\n","\n","    # Separate audio samples and labels\n","    waveforms, labels, genders, ages, snrs = zip(*batch)\n","\n","    # Pad the audio samples (if needed)\n","    #padded_waveforms = pad_sequence(waveforms)\n","\n","    # Convert features and labels to tensor\n","    labels = torch.tensor(labels)\n","    genders = torch.tensor(genders)\n","    ages = torch.tensor(ages)\n","    snrs = torch.tensor(snrs)\n","\n","    # Extract tensors from tuples\n","    #waveforms = [item[0] for item in waveforms]\n","    #original = [item[0] for item in original]\n","    #print(\"collate waveform\",waveforms)\n","    #print(\"collate original waveform\",original)\n","\n","    # Convert to tensors\n","    #waveforms = np.array(waveforms, dtype=np.float32)\n","    #waveforms = torch.tensor(waveforms, dtype=torch.float32)\n","    #print(\"collate waveform tensor\",waveforms)\n","    #original = torch.tensor(original, dtype=torch.float32)\n","\n","    #waveforms = torch.tensor(waveforms, dtype=torch.float32)\n","    #original = torch.tensor(original, dtype=torch.float32)\n","\n","    #print(\"collate waveform\",waveforms)\n","    #print(\"collate waveform.shape\",waveforms.shape)\n","\n","    #print(\"collate original waveform\",original)\n","    #print(\"collate original waveform.shape\",original[0].shape)\n","\n","\n","    #return waveforms, labels, original\n","    return waveforms, labels, genders, ages, snrs\n","    #return padded_waveforms, labels, genders, ages, snrs"]},{"cell_type":"markdown","metadata":{},"source":["# Miscellaneous functions\n","\n","The following code block contains miscellaneous functions such as plotting of waveforms, spectograms, fbank, and preprocessing of the data."]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-03-16T10:13:32.795886Z","iopub.status.busy":"2024-03-16T10:13:32.795517Z","iopub.status.idle":"2024-03-16T10:13:32.824211Z","shell.execute_reply":"2024-03-16T10:13:32.823356Z","shell.execute_reply.started":"2024-03-16T10:13:32.795854Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import librosa\n","\n","def waveform_plot(signal, sr, title, threshold=None, plot=None):\n","    # Calculate time axis\n","    time = np.arange(0, len(signal)) / sr\n","\n","    plt.figure(figsize=(10, 8))\n","\n","    # Plot standard waveform\n","    plt.subplot(3,1,1)\n","    plt.plot(time, signal, color='b')\n","    plt.xlabel('Time (s)')\n","    plt.ylabel('Amplitude')\n","    plt.title(title)\n","    plt.grid(True)\n","    plt.show()\n","\n","    if plot:\n","        # Calculate dBFS values\n","        if np.any(signal != 0):\n","            db_signal = 20 * np.log10(np.abs(signal) / np.max(np.abs(signal)))\n","        else:\n","            db_signal = -60\n","\n","        plt.subplot(3,1,2)\n","        # Plot waveform in dB scale\n","        plt.plot(time, db_signal, color='b')\n","\n","        # Plot threshold level\n","        if threshold:\n","            plt.axhline(y=threshold, color='r', linestyle='--', label=f'{threshold} dBFS Threshold')\n","            plt.legend()\n","\n","        plt.xlabel('Time (s)')\n","        plt.ylabel('Amplitude (dBFS)')\n","        plt.title(title)\n","        plt.grid(True)\n","\n","        n_fft = 2048  # Length of the FFT window\n","        hop_length = 512  # Hop length for FFT\n","        S = np.abs(librosa.stft(signal.astype(float), n_fft=n_fft, hop_length=hop_length))\n","\n","        # Convert amplitude to dB scale (sound pressure level)\n","        S_db = librosa.amplitude_to_db(S, ref=np.max)\n","\n","        # Get frequency bins corresponding to FFT\n","        freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n","\n","        # Step 3: Plot the SPL values over frequency\n","        plt.subplot(3,1,3)\n","        plt.plot(freqs, np.mean(S_db, axis=1), color='b')\n","        plt.title('Sound Pressure Level (SPL) vs. Frequency')\n","        plt.xlabel('Frequency (Hz)')\n","        plt.ylabel('SPL (dB)')\n","        plt.grid(True)\n","        plt.xlim([20, 25000])  # Set frequency range for better visualization\n","        plt.xscale('log')  # Use log scale for frequency axis\n","\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","# Stolen from pytorch tutorial xd\n","def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", batch=0, idx=0, ax=None):\n","    if ax is None:\n","        fig, ax = plt.subplots(1, 1)\n","    if title is not None:\n","        ax.set_title(title)\n","    ax.set_ylabel(ylabel)\n","    im = ax.imshow(\n","        librosa.power_to_db(specgram),\n","        origin=\"lower\",\n","        aspect=\"auto\",\n","        interpolation=\"nearest\",\n","    )\n","    plt.colorbar(im, ax=ax, label='dB')\n","    #plt.close()\n","    plt.savefig(f\"test_outputs/batch{batch}_idx{idx}_{title}.png\")\n","\n","def plot_fbank(fbank, title=None):\n","    fig, axs = plt.subplots(1, 1)\n","    axs.set_title(title or \"Filter bank\")\n","    axs.imshow(fbank, aspect=\"auto\")\n","    axs.set_ylabel(\"frequency bin\")\n","    axs.set_xlabel(\"mel bin\")\n","\n","def preprocess_data(data_meta_path, data_dir_path, output_dir):\n","    # Read data file then remove every column other than the specified columns\n","    # Removes empty samples and filters through cough probability\n","    data = pd.read_csv(data_meta_path, sep=\",\")\n","    data = (\n","        data[[\"uuid\", \"cough_detected\", \"SNR\", \"age\", \"gender\", \"status\"]]\n","        .loc[data[\"cough_detected\"] >= 0.8]\n","        .dropna().reset_index(drop=True).sort_values(by='cough_detected')\n","    )\n","    data = data[(data[\"gender\"] != \"other\")]\n","\n","    #Count the occurrences of each age value\n","    age_counts = data['age'].value_counts()\n","\n","    # Filter out ages with fewer than 100 samples\n","    ages_to_keep = age_counts.index[age_counts >= 100]\n","\n","    # Filter the DataFrame based on the selected ages\n","    data = data[data['age'].isin(ages_to_keep)]\n","\n","    # Check if the following MP3 with uuid exists\n","    mp3_data = []\n","    non_exist = []\n","    for file in data[\"uuid\"]:\n","        if os.path.exists(os.path.join(data_dir_path, f\"{file}.mp3\")):\n","            mp3_data.append(os.path.join(data_dir_path, f\"{file}.mp3\"))\n","        else:\n","            non_exist.append(file)\n","        # elif os.path.exists(os.path.join(data_dir_path, f'{file}.ogg')):\n","        #    ogg_data.append(os.path.join(data_dir_path, f'{file}.ogg'))\n","\n","    # Remove entries with missing MP3 files from the original data\n","    data = data[~data[\"uuid\"].isin(non_exist)]\n","\n","    # Replace the uuids with the path to uuid\n","    data[\"uuid\"] = mp3_data\n","\n","    # Save the data as csv\n","    data.to_csv(os.path.join(output_dir, \"filtered_audio_data.csv\"), index=False)\n","\n","    print(\"Finished processing!\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:13:32.826219Z","iopub.status.busy":"2024-03-16T10:13:32.825402Z","iopub.status.idle":"2024-03-16T10:13:32.838520Z","shell.execute_reply":"2024-03-16T10:13:32.837719Z","shell.execute_reply.started":"2024-03-16T10:13:32.826193Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\ndata_path = r\"misc_data/metadata_compiled.csv\"\\ndata_dir_path = r\"../Dataset/MP3/\"\\noutput_dir = r\"misc_data/\"\\npreprocess_data(data_path, data_dir_path, output_dir)\\n'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","data_path = r\"misc_data/metadata_compiled.csv\"\n","data_dir_path = r\"../Dataset/MP3/\"\n","output_dir = r\"misc_data/\"\n","preprocess_data(data_path, data_dir_path, output_dir)\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Dataset specific functions\n","\n","The following codeblock contains functions specially related to the dataset preprocessing."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:13:32.840002Z","iopub.status.busy":"2024-03-16T10:13:32.839706Z","iopub.status.idle":"2024-03-16T10:13:33.448972Z","shell.execute_reply":"2024-03-16T10:13:33.448166Z","shell.execute_reply.started":"2024-03-16T10:13:32.839970Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","def preprocess_dataset(data, test_size):\n","    # Extract audio samples and labels\n","    X = data.drop(columns=[\"status\"])\n","    y = data[\"status\"]\n","\n","\n","    # Perform a stratified split\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=test_size, stratify=y, random_state=42\n","    )\n","\n","    # Combine audio samples and target labels for training and validation sets\n","    train_data = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n","    test_data = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n","\n","    return train_data, test_data\n","\n","def weighted_sample(data):\n","    # Find class distribution\n","    class_counts = data[\"status\"].value_counts()\n","    # print(class_counts)\n","\n","    # Check class weights\n","    class_weights = 1 / class_counts\n","    # print(class_weights)\n","\n","    # Adjust weighting to each sample\n","    sample_weights = [1 / class_counts[i] for i in data[\"status\"].values]\n","    # print(\"len sample weights:\",len(sample_weights))\n","\n","    return sample_weights\n","\n","def undersample(data, n, normalize=False):\n","    # Step 1: Identify majority class\n","    class_counts = data[\"status\"].value_counts()\n","    majority_class = class_counts.idxmax()\n","\n","    # Step 2: Calculate desired class distribution (e.g., balanced distribution)\n","    desired_class_count = n  # Target number of samples for each class\n","\n","    # Step 3: Select subset from majority class\n","    undersampled_data_majority = data[data[\"status\"] == majority_class].sample(\n","        n=desired_class_count\n","    )\n","\n","    # Combine with samples from minority classes\n","    undersampled_data_minority = data[~(data[\"status\"] == majority_class)]\n","\n","    # Combine undersampled majority class with minority classes\n","    undersampled_data = pd.concat(\n","        [undersampled_data_majority, undersampled_data_minority]\n","    )\n","\n","    # Shuffle the undersampled dataset\n","    undersampled_data = undersampled_data.sample(frac=1).reset_index(drop=True)\n","\n","    return undersampled_data\n","\n","def visualize_dataset(data, normalize, title):\n","    print(f\"{title} Distribution\")\n","    print(data[\"status\"].value_counts(normalize=normalize))\n","    print(\"Total samples\", len(data))\n","\n","    plt.figure(figsize=(6, 4))\n","    plt.title(f\"Histogram of Patient Status\\n- {title}\")\n","    plt.bar(data[\"status\"].value_counts().index, data[\"status\"].value_counts())\n","    plt.xticks(rotation=20, ha=\"right\", fontsize=8)\n","    plt.xlabel(\"Class\", fontsize=8)\n","    plt.ylabel(\"Frequency\", fontsize=8)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization of dataset and dataset loader\n","\n","This codeblock includes the initialization of the dataset as well as any processing needed, such as splitting it into training/testing datasets, as well as different sampling techniques, such as undersampling/weighted sampling."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:30:31.943346Z","iopub.status.busy":"2024-03-17T12:30:31.942893Z","iopub.status.idle":"2024-03-17T12:30:32.048519Z","shell.execute_reply":"2024-03-17T12:30:32.046951Z","shell.execute_reply.started":"2024-03-17T12:30:31.943312Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import WeightedRandomSampler\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import torch\n","import os\n","\n","# Set seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Load data\n","#data = pd.read_csv(\"/kaggle/input/covid-19-audio-classification/filtered_audio_data.csv\")\n","#data = pd.read_csv(\"/kaggle/input/filtered-csv/filtered_audio_data.csv\")\n","##data['uuid'] = data['uuid'].apply(lambda x: x.replace('../Dataset/MP3/', \"/kaggle/input/covid-19-audio-classification/MP3/\"))\n","#data['uuid'] = data['uuid'].str.replace('../Dataset/MP3/',\"/kaggle/input/covid-19-audio-classification/MP3/\")\n","\n","#print(data.head())\n","\n","data = pd.read_csv(\"misc_data/filtered_audio_data.csv\")\n","\n","# Initialize LabelEncoder\n","le = LabelEncoder()\n","\n","# Fit and transform labels into encoded form\n","labels = [\"healthy\", \"symptomatic\", \"COVID-19\"]\n","encoded_labels = le.fit_transform(labels)\n","\n","# Prepare standard dataset\n","train_data, test_data = preprocess_dataset(data, 0.3) # First split the original dataset into 70% training\n","val_data, test_data = preprocess_dataset(test_data, 0.5) # Second split the \"test_data\" into 50/50 validation and test (or technically 15/15)\n","\n","# Prepare and create undersampled version\n","#undersampled_data = undersample(data, 2000, True)\n","#visualize_dataset(undersampled_data, None, \"Standard\")\n","#visualize_dataset(undersampled_data, \"normalize\", \"Normalized\")\n","#train_undersampled_data, test_undersampled_data = preprocess_dataset(undersampled_data, 0.3)\n","#val_undersampled_data, test_undersampled_data = preprocess_dataset(test_undersampled_data, 0.5)\n","#visualize_dataset(train_undersampled_data, None, \"Train\")\n","#visualize_dataset(train_undersampled_data, \"normalize\", \"Train Normalized\")\n","#visualize_dataset(val_undersampled_data, None, \"Validation\")\n","#visualize_dataset(val_undersampled_data, \"normalize\", \"Validation Normalized\")\n","\n","# Prepare and create weighted sampler\n","train_sample_weights = weighted_sample(train_data)\n","val_sample_weights = weighted_sample(val_data)\n","test_sample_weights = weighted_sample(test_data)\n","\n","train_weighted_Sampler = WeightedRandomSampler(weights=train_sample_weights, num_samples=len(train_data), replacement=True)\n","val_weighted_Sampler = WeightedRandomSampler(weights=val_sample_weights, num_samples=len(val_data), replacement=True)\n","test_weighted_Sampler = WeightedRandomSampler(weights=test_sample_weights, num_samples=len(test_data), replacement=True)\n","\n","# Create AudioDataset instances for training and validation sets\n","# Standard dataset\n","min_silence = 500\n","threshold = -40\n","keep_silence = 250\n","args = [min_silence, threshold, keep_silence]\n","\n","train_dataset = AudioDataset(train_data, args, le)\n","val_dataset = AudioDataset(val_data, args, le)\n","test_dataset = AudioDataset(test_data, args, le)\n","\n","# Undersampled dataset\n","#train_undersampled_dataset = AudioDataset(train_undersampled_data, le)\n","#test_undersampled_dataset = AudioDataset(test_undersampled_data, le)\n","\n","# Create training and test dataloader instances\n","batch = 8\n","workers = 0\n","pin_memory = True\n","\n","#train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n","#val_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n","#test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=workers, collate_fn=collate_fn, pin_memory=pin_memory)\n","\n","\"\"\"\n","train_undersampled_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=batch,\n","    shuffle=True,\n","    num_workers=workers,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n",")\n","val_undersampled_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=batch,\n","    shuffle=False,\n","    num_workers=workers,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n",")\n","test_undersampled_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=batch,\n","    shuffle=False,\n","    num_workers=workers,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n",")\n","\"\"\"\n","\n","train_weighted_dataloader = DataLoader(train_dataset, sampler=train_weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)\n","val_weighted_dataloader = DataLoader(val_dataset, sampler=val_weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)\n","test_weighted_dataloader = DataLoader(test_dataset, sampler=test_weighted_Sampler, batch_size=batch, num_workers=workers, collate_fn=collate_fn, pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Initialize and define MFCC feature extractor\n","\n","In the following codeblock the MFCC specific parameters are defined and initialized. The codeblock also includes a function that pads the extracted MFCC features in order to pass it to the model."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:13:33.595482Z","iopub.status.busy":"2024-03-16T10:13:33.595185Z","iopub.status.idle":"2024-03-16T10:13:36.968713Z","shell.execute_reply":"2024-03-16T10:13:36.967711Z","shell.execute_reply.started":"2024-03-16T10:13:33.595457Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["g:\\Github\\deep-learning-course\\.venv\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (80) may be set too high. Or, the value for `n_freqs` (176) may be set too low.\n","  warnings.warn(\n"]}],"source":["from torchaudio.transforms import MFCC\n","from torchvision.transforms import Resize\n","import torch.nn.functional as F\n","\n","def MFCC_Features(data, padding=3000, normalize=False, resize=False, batch=0):\n","    \"\"\"\n","    Args:\n","    data: Input audio waveform\n","    max_length: Maximum length for padding\n","    normalize: Normalize the channel layer\n","    resize: Resize the spectrogram\n","    target_size: Target size for resizing\n","    \"\"\"\n","    # Extract MFCC features\n","    #features = mfcc(data)\n","    #print(\"features.shape\",features.shape)\n","    #features = [mfcc(waveform) for waveform in data]\n","    features = [torch.unsqueeze(mfcc(waveform), 0) for waveform in data] # Adding channels\n","    #features = [torch.unsqueeze(torch.unsqueeze(mfcc(waveform), 0), 0) for waveform in data] # Adding batch size and channels\n","\n","\n","    #for feature in features:\n","    #        print(\"Feature shape after MFCC:\", feature.shape)\n","\n","    #for i, feature in enumerate(features):\n","    #    x = torch.unsqueeze(torch.unsqueeze(feature,0),0)\n","    #    #x = torch.unsqueeze(feature,0)\n","    #    features[i] = x\n","\n","    #for feature in features:\n","            #print(\"Feature shape after adding channels:\", feature.shape)\n","\n","    # Hardcoded padding\n","    if padding:\n","        features = F.pad(features, (0, padding - features.shape[3]), \"constant\", 0)\n","\n","\n","    # Normalize the features for each sample\n","    if normalize == True:\n","        #for j, i in enumerate(features):\n","        #    plot_spectrogram(i[0], \"Before normalization\", batch = batch, idx = j)\n","        #features = (features - features.mean()) / features.std()\n","\n","        #print(\"features.shape during normalization step\", features.shape)\n","        for j, feature in enumerate(features):\n","            mean = feature.mean(dim=[1 ,2], keepdim=True)\n","            std = feature.std(dim=[1, 2], keepdim=True)\n","            features[j] = (feature - mean) / std\n","        #features = [(feature - feature.mean()) / feature.std() for feature in features]\n","        #for j, i in enumerate(features):\n","        #    #print(\"shape after normalization i\",i.shape)\n","        #    plot_spectrogram(i[0], \"After normalization\", batch = batch, idx = j)\n","\n","    #print(\"len normal\", len(features[0]))\n","\n","    # Add two artificial channels filled with zeros\n","    if resize == True:\n","        #artificial_channels = torch.zeros(features.shape[0], 2, features.shape[2], features.shape[3])\n","        #features = torch.cat([features, artificial_channels], dim=1)\n","        #for j, i in enumerate(features):\n","        #    plot_spectrogram(i[0], \"Before resizing\", batch = batch, idx = j)\n","        #features = Resize((224,224), antialias=True)(features)\n","\n","        # Print out the shape of each feature tensor\n","        #for feature in features:\n","        #    print(\"Feature shape before resizing:\", feature[0].shape)\n","\n","        features = [Resize((224, 224), antialias=True)(feature) for feature in features]\n","        #for feature in features:\n","        #    print(\"Feature shape after resizing:\", feature.shape)\n","\n","        #for j, i in enumerate(features):\n","        #    plot_spectrogram(i[0], \"After resizing\", batch = batch, idx = j)\n","\n","    features = torch.stack(features)\n","    #print(\"features after stacking\", features.shape)\n","    return features\n","\n","# Settings for MelSpectrogram computation\n","melkwargs = {\n","    \"n_mels\": 80,  # How many mel frequency filters are used\n","    \"n_fft\": 350,  # How many fft components are used for each feature\n","    \"win_length\": 350,  # How many frames are included in each window\n","    \"hop_length\": 100,  # How many frames the window is shifted for each component\n","    \"center\": False,  # Whether frams are padded such that the component of timestep t is centered at t\n","    \"f_max\": 11000,  # Maximum frequency to consider\n","    \"f_min\": 0,\n","}\n","\n","# Instantiate MFCC feature extractor\n","mfcc = MFCC(\n","    n_mfcc=22,  # Number of cepstrum components\n","    sample_rate=22000,  # Sample rate of input audio\n","    melkwargs=melkwargs)  # Keyword arguments for MelSpectogram"]},{"cell_type":"markdown","metadata":{},"source":["# Initializing and defining model\n","\n","The following codeblock contains the initialization of the ResNet50 model from the PyTorch library."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T16:16:04.383055Z","iopub.status.busy":"2024-03-16T16:16:04.382534Z","iopub.status.idle":"2024-03-16T16:16:04.872341Z","shell.execute_reply":"2024-03-16T16:16:04.871081Z","shell.execute_reply.started":"2024-03-16T16:16:04.383006Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device running on: cuda\n","ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=3, bias=True)\n",")\n","Only 1 GPU available!\n"]}],"source":["from torchvision import models\n","import torch.nn as nn\n","import torch\n","\n","# Load in the pre-trained resnet model\n","#model = models.vgg16_bn(weights=None, num_classes=3)\n","model = models.resnet18(weights=None, num_classes=3)\n","#model = models.resnet18()\n","#model = models.resnet50()\n","\n","# Modifying the first layer to be able to pass 1-channel image (spectrogram) for ResNet model\n","#model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1,1), padding=(1, 1)) VGG\n","#model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1,1), padding=(1, 1)) # RESNET\n","\n","# Set the model to training mode and put it on GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device running on: {device}\")\n","\n","model.to(device) # add \";\" to keep from printing the network architecture\n","print(model)\n","# Wrap your model with DataParallel if multiple GPUs are available\n","if torch.cuda.device_count() > 1:\n","    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model)\n","else:\n","    print(\"Only 1 GPU available!\")"]},{"cell_type":"markdown","metadata":{},"source":["# Setup weights and bias logging"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:13:37.450133Z","iopub.status.busy":"2024-03-16T10:13:37.449766Z","iopub.status.idle":"2024-03-16T10:13:38.431365Z","shell.execute_reply":"2024-03-16T10:13:38.430358Z","shell.execute_reply.started":"2024-03-16T10:13:37.450106Z"},"trusted":true},"outputs":[],"source":["import wandb\n","\n","# Initialize wandb\n","#!wandb login --relogin 9be53a0c7076cae09612be80ee5e0e80d9dac79c\n","\n","# Defining training variables\n","lr = 0.1\n","step = 5\n","decay = 0.1\n","optim = \"adam\"\n","gamma = 0.1\n","epochs = 50\n","\n","\n","## Defining weights and biases config\n","#wandb.init(\n","#    # set the wandb project where this run will be logged\n","#    project=\"mini-project\",\n","#    config={\n","#    \"architecture\": \"ResNet18\",\n","#    \"description\": \"Weighted, Normalized, and resized mel spectrograms to 224x224 + removing silent audio parts\",\n","#    \"dataset\": \"COVID-19 Audio Classification\",\n","#    \"learning_rate\": lr,\n","#    \"step_size\": step,\n","#    \"weight_decay\": decay,\n","#    \"optimizer\": optim,\n","#    \"gamma\": gamma,\n","#    \"epochs\": epochs\n","#    }\n","#)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training loop"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:13:38.433325Z","iopub.status.busy":"2024-03-16T10:13:38.433008Z","iopub.status.idle":"2024-03-16T10:13:38.446732Z","shell.execute_reply":"2024-03-16T10:13:38.445636Z","shell.execute_reply.started":"2024-03-16T10:13:38.433299Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["COVID-19 Healthy Symptomatic\n","[5.88954635 0.43635832 1.85696517]\n"]}],"source":["from sklearn.utils.class_weight import compute_class_weight\n","train_labels = train_data[\"status\"]\n","class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n","\n","print(\"COVID-19\",\"Healthy\", \"Symptomatic\")\n","print(class_weights)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:13:38.448853Z","iopub.status.busy":"2024-03-16T10:13:38.448446Z","iopub.status.idle":"2024-03-16T10:31:55.165991Z","shell.execute_reply":"2024-03-16T10:31:55.164558Z","shell.execute_reply.started":"2024-03-16T10:13:38.448819Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Currently: Training\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 | Training:   0%|          | 0/747 [00:05<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"Given groups=1, weight of size [64, 3, 7, 7], expected input[8, 1, 224, 224] to have 3 channels, but got 1 channels instead","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[13], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m features, targets \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Load them onto GPU\u001b[39;00m\n\u001b[0;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zero the parameters\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Retrieve the output from the model\u001b[39;00m\n\u001b[0;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets) \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#loss = F.cross_entropy(outputs, targets, reduction='mean')\u001b[39;00m\n","File \u001b[1;32mg:\\Github\\deep-learning-course\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mg:\\Github\\deep-learning-course\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mg:\\Github\\deep-learning-course\\.venv\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mg:\\Github\\deep-learning-course\\.venv\\Lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n","File \u001b[1;32mg:\\Github\\deep-learning-course\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mg:\\Github\\deep-learning-course\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mg:\\Github\\deep-learning-course\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mg:\\Github\\deep-learning-course\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[8, 1, 224, 224] to have 3 channels, but got 1 channels instead"]}],"source":["from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from torch.optim.lr_scheduler import StepLR\n","from torch.cuda.amp import GradScaler\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchaudio\n","import torch\n","import os\n","\n","if optim == \"adam\":\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n","else:\n","    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=decay)\n","scheduler = StepLR(optimizer, step_size=step, gamma=0.5)\n","class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n","criterion = nn.CrossEntropyLoss(weight=class_weights)\n","#scaler = GradScaler()\n","log_interval = 20\n","best_vloss = float(\"inf\")\n","model_no = 0\n","\n","print(\"Currently: Training\")\n","for epoch in range(epochs):\n","    model.train() # Initiate training mode\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    # Training loop\n","    for i, (inputs, targets, *_) in tqdm(enumerate(\n","       train_weighted_dataloader),\n","       total=len(train_weighted_dataloader),\n","       leave=True,\n","       desc=f\"Epoch {epoch+1}/{epochs} | Training\"\n","        ):\n","        #print(f\"========== BATCH {i} ========== \")\n","        #for idx, f in enumerate(features):\n","        #    print(f\"{idx+1} | {f.shape}\")\n","        #print(f\"========== BATCH {i} ========== \")\n","\n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True) # Compute the MFCC features\n","        features, targets = features.to(device), targets.to(device) # Load them onto GPU\n","        optimizer.zero_grad() # Zero the parameters\n","        outputs = model(features) # Retrieve the output from the model\n","        loss = criterion(outputs, targets) # Compute the loss\n","        #loss = F.cross_entropy(outputs, targets, reduction='mean')\n","        loss.backward() # Compute gradients of the loss\n","        optimizer.step() # Update weights\n","\n","        running_loss += loss.item()\n","\n","        # Calculate accuracy\n","        _, predicted = torch.max(outputs, 1)\n","        correct_predictions += (predicted == targets).sum().item()\n","        total_predictions += targets.size(0)\n","\n","        if i % log_interval == 0:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i}/{len(train_weighted_dataloader)} | Training Loss: {loss.item():.4f}\")\n","\n","    # Compute accuracy\n","    accuracy = correct_predictions / total_predictions\n","\n","    print(f\"Training Accuracy: {accuracy:.4f}\")\n","\n","    # Compute average training loss for the epoch\n","    avg_loss = running_loss / len(train_weighted_dataloader)\n","\n","    # Validation loop\n","    running_vloss = 0.0\n","    vcorrect_predictions = 0\n","    vtotal_predictions = 0\n","    model.eval()\n","    with torch.no_grad(): # Disable gradient computation\n","        for j, (vinputs, vtargets, *_) in tqdm(enumerate(\n","            val_weighted_dataloader),\n","            total=len(val_weighted_dataloader),\n","            leave=True,\n","            desc=f\"Epoch {epoch+1}/{epochs} | Validating\"):\n","            vfeatures = MFCC_Features(vinputs, padding=False, normalize=True, resize=True) # Compute the MFCC features\n","            vfeatures, vtargets = vfeatures.to(device), vtargets.to(device) # Load them onto GPU\n","            voutputs = model(vfeatures)\n","            vloss = criterion(voutputs.to(device), vtargets)\n","            #vloss = F.cross_entropy(voutputs, vtargets, reduction='mean')\n","            running_vloss += vloss.item()\n","\n","            # Calculate accuracy\n","            _, vpredicted = torch.max(voutputs, 1)\n","            vcorrect_predictions += (vpredicted == vtargets).sum().item()\n","            vtotal_predictions += vtargets.size(0)\n","\n","    # Compute average validation loss for the epoch\n","    avg_vloss = running_vloss / len(val_weighted_dataloader)\n","\n","    # Compute accuracy\n","    vaccuracy = vcorrect_predictions / vtotal_predictions\n","\n","    # Compute precision, recall, F1 score\n","    precision = precision_score(vtargets.cpu(), vpredicted.cpu(), average='macro',zero_division=0.0)\n","    recall = recall_score(vtargets.cpu(), vpredicted.cpu(), average='macro',zero_division=0.0)\n","    f1 = f1_score(vtargets.cpu(), vpredicted.cpu(), average='macro',zero_division=0.0)\n","\n","    # Log metrics to wandb\n","    wandb.log({\"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n","    wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_loss,\"train_acc\": accuracy, \"val_loss\": avg_vloss, \"val_accuracy\": vaccuracy})\n","\n","    print(f\"Epoch #{epoch+1} | Training Loss: {avg_loss:.4f}  |  Training Accuracy: {accuracy:.4f} | Validation Loss: {avg_vloss:.4f} | Validation Accuracy: {vaccuracy:.4f}\\n           Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n","\n","    # Update learning rate\n","    #print_lr(is_verbose, group, lr, epoch=None)\n","    scheduler.step()\n","\n","    print(f\"Epoch {epoch+1}, Learning Rate: {scheduler.get_last_lr()}\")\n","\n","    # Track best performance, and save the model's state\n","    if avg_vloss < best_vloss:\n","        best_vloss = avg_vloss\n","        model_no += 1\n","        model_path = f\"/kaggle/working/models/ResNet18_weighted_model_no_{model_no}_epoch_{epoch+1}.pth\"\n","        torch.save(model.state_dict(), model_path)\n","\n","# Finish the run\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:31:55.167077Z","iopub.status.idle":"2024-03-16T10:31:55.167411Z","shell.execute_reply":"2024-03-16T10:31:55.167266Z","shell.execute_reply.started":"2024-03-16T10:31:55.167250Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4560779,"sourceId":7791391,"sourceType":"datasetVersion"},{"datasetId":4561341,"sourceId":7792152,"sourceType":"datasetVersion"},{"datasetId":4609644,"sourceId":7858628,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
