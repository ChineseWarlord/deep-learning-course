{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importing necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.601957Z","iopub.status.busy":"2024-03-19T13:49:40.601590Z","iopub.status.idle":"2024-03-19T13:49:40.612398Z","shell.execute_reply":"2024-03-19T13:49:40.611297Z","shell.execute_reply.started":"2024-03-19T13:49:40.601921Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Torch-related imports\n","from torch.utils.data import WeightedRandomSampler, DataLoader\n","from torch.optim.lr_scheduler import StepLR\n","from torchvision.transforms import Resize\n","from torchaudio.transforms import MFCC\n","from torch.cuda.amp import GradScaler\n","from torchvision import models\n","from torchinfo import summary\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torchaudio\n","import torch\n","\n","# Sklearn-related imports\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.utils.class_weight as class_weight\n","import sklearn.model_selection as model_selection\n","import sklearn.preprocessing as preprocessing\n","import sklearn.metrics as metrics\n","\n","# Audio processing imports\n","from pydub.silence import split_on_silence\n","from pydub import AudioSegment\n","import librosa\n","\n","# Miscellaneous imports\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","import numpy as np\n","import wandb\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset, dataloader, and model parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.614593Z","iopub.status.busy":"2024-03-19T13:49:40.614298Z","iopub.status.idle":"2024-03-19T13:49:40.661372Z","shell.execute_reply":"2024-03-19T13:49:40.660473Z","shell.execute_reply.started":"2024-03-19T13:49:40.614569Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Set seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Path to data\n","#data = pd.read_csv(\"/kaggle/input/filtered-csv/filtered_audio_data.csv\")\n","#data[\"uuid\"] = data[\"uuid\"].str.replace(\"../Dataset/MP3/\", \"/kaggle/input/covid-19-audio-classification/MP3/\")\n","data = pd.read_csv(\"misc_data/filtered_audio_data.csv\")\n","\n","# Class labels\n","labels = [\"healthy\", \"symptomatic\", \"COVID-19\"]\n","\n","# Silence arguments\n","min_silence = 500\n","threshold_dBFS = -40\n","keep_silence = 250\n","\n","# Dataloader and dataset arguments\n","batch = 3\n","workers = 0\n","pin_memory = True\n","dataset_type = \"undersampled\"\n","undersampling = 500\n","\n","# Settings for MelSpectrogram computation\n","melkwargs = {\n","    \"n_mels\": 60,  # How many mel frequency filters are used\n","    \"n_fft\": 350,  # How many fft components are used for each feature\n","    \"win_length\": 350,  # How many frames are included in each window\n","    \"hop_length\": 100,  # How many frames the window is shifted for each component\n","    \"center\": False,  # Whether frams are padded such that the component of timestep t is centered at t\n","    \"f_max\": 11000,  # Maximum frequency to consider\n","    \"f_min\": 0,\n","}\n","n_mfcc = 22\n","sample_rate = 22000\n","\n","# Model type\n","\"\"\"\n","Models:\n","    \"resnet18\":\n","    \"resnet34\":\n","    \"resnet50\":\n","    \"vgg_bn\":\n","    \"multi_resnet\":\n","    \"modified_multi_resnet\"\n","    \"modified_multi_resnet_spectral\"\n","\"\"\"\n","model_type = \"modified_multi_resnet_spectral\"\n","model_arch = \"resnet18\"\n","model_output = \"modified_multi_resnet18_spectral_undersampled\"\n","\n","# Model training and Weights and Biases variables\n","lr = 0.001\n","step = 10\n","decay = 0.0001\n","optimizer_type = \"adam\"\n","gamma = 0.1\n","epochs = 50\n","arch = \"Multi Input ResNet18 Spectral\"\n","desc = \"This model is trained on MFCC features, numeric features, and spectral features.\"\n","dataset = \"COVID-19 Audio Classification\"\n","weighted = False"]},{"cell_type":"markdown","metadata":{},"source":["# Setup and define custom dataset class\n","\n","The custom dataset class finds each raw audio sample and corresponding label, encodes the label and returns the raw audio sample as mono-channel as well as the label."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.663726Z","iopub.status.busy":"2024-03-19T13:49:40.663377Z","iopub.status.idle":"2024-03-19T13:49:40.672399Z","shell.execute_reply":"2024-03-19T13:49:40.671620Z","shell.execute_reply.started":"2024-03-19T13:49:40.663699Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def remove_silence(audio_object, min_silence_ms=100, threshold_dBFS=-40, keep_silence=100, seek_step=1):\n","    # Check for loudness (DEBUGGING)\n","    # loudness_dBFS = audio_object.dBFS\n","    # print(\"Loudness (dBFS):\", loudness_dBFS)\n","\n","    # Attempt to split and remove silence from the audio signal\n","    audio_segments = split_on_silence(audio_object, min_silence_ms, threshold_dBFS, seek_step)\n","\n","    # Check if audio_segments is empty if yes return the original audio object as numpy array\n","    if not audio_segments:\n","\n","        # Get the array of samples from the audio segment\n","        org_audio = np.array(audio_object.get_array_of_samples(), dtype=np.float32)\n","\n","        # Normalize the samples if needed\n","        org_audio /= np.max(np.abs(org_audio))\n","\n","        return org_audio\n","\n","    # Add the different audio segments together\n","    audio_processed = sum(audio_segments)\n","\n","    # Return the samples from the processed audio, save as numpy array, and normalize it\n","    audio_processed = np.array(audio_processed.get_array_of_samples(), dtype=np.float32)\n","    audio_processed /= np.max(np.abs(audio_processed))\n","\n","    return audio_processed\n","\n","\n","def encode_age(age):\n","    # Define age mapping\n","    age_mapping = {\"child\": 0, \"teen\": 1, \"adult\": 2, \"senior\": 3}\n","\n","    # Determine age range\n","    if age <= 12:  # Children from ages 0-12\n","        return age_mapping[\"child\"]\n","    elif age <= 19:  # Teenagers from ages 13-19\n","        return age_mapping[\"teen\"]\n","    elif age <= 50:  # Adults from ages 20-50\n","        return age_mapping[\"adult\"]\n","    else:  # Seniors (age > 50)\n","        return age_mapping[\"senior\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-03-19T13:49:40.674707Z","iopub.status.busy":"2024-03-19T13:49:40.674380Z","iopub.status.idle":"2024-03-19T13:49:40.687256Z","shell.execute_reply":"2024-03-19T13:49:40.686440Z","shell.execute_reply.started":"2024-03-19T13:49:40.674683Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class AudioDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, args, label_encoder=None):\n","        # Initialize attributes\n","        self.data = data[\"uuid\"]\n","        self.label = data[\"status\"]\n","        self.age = data[\"age\"]\n","        self.gender = data[\"gender\"]\n","        self.SNR = data[\"SNR\"]\n","        self.label_encoder = label_encoder\n","        self.min_silence = args[0]\n","        self.threshold = args[1]\n","        self.keep_silence = args[2]\n","        self.sample_rate = {}\n","\n","    def __len__(self):\n","        return len(self.label)\n","\n","    def __getitem__(self, idx):\n","        # Extract audio sample from idx\n","        audio_path = self.data[idx]\n","\n","        # Load in audio\n","        audio_object = AudioSegment.from_file(audio_path)\n","        audio_sample = remove_silence(audio_object, self.min_silence, self.threshold, self.keep_silence)\n","        self.sample_rate[idx] = audio_object.frame_rate\n","\n","        # Extract audio label from idx and transform\n","        audio_label = [self.label[idx]]\n","        audio_label = self.label_encoder.transform(audio_label)\n","\n","        # Extract age, gender, and SNR from idx and encode the necessary features\n","        gender_mapping = {\"male\": 0, \"female\": 1}\n","        gender = np.array([gender_mapping[self.gender[idx]]], dtype=np.int8)\n","        age = np.array(encode_age(self.age[idx]), dtype=np.int8)\n","        snr = np.array([self.SNR[idx]])\n","\n","        # Check if audio sample is stereo -> convert to mono (remove_silence already turns it into 1 channel)\n","        # if len(audio_sample.shape) > 1 and audio_sample.shape[1] > 1:\n","        # Convert stereo audio to mono\n","        # audio_sample = audio_sample.mean(dim=0, keepdim=True)\n","\n","        return (\n","            torch.tensor(audio_sample, dtype=torch.float32),\n","            torch.tensor(audio_label, dtype=torch.int32),\n","            torch.tensor(gender, dtype=torch.int32),\n","            torch.tensor(age, dtype=torch.int32),\n","            torch.tensor(snr, dtype=torch.float32),\n","        )\n","\n","    def __get_sample_rate__(self, idx):\n","        # If needed extract sample rate\n","        return self.sample_rate.get(idx)"]},{"cell_type":"markdown","metadata":{},"source":["# Custom collate function\n","\n","The following collate function will take batches of raw audio samples and zero pad them to match the largest sized audio sample."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.711700Z","iopub.status.busy":"2024-03-19T13:49:40.711382Z","iopub.status.idle":"2024-03-19T13:49:40.719349Z","shell.execute_reply":"2024-03-19T13:49:40.718421Z","shell.execute_reply.started":"2024-03-19T13:49:40.711671Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def pad_sequence(batch):\n","    # Make all tensor in a batch the same length by padding with zeros\n","    batch = [item.t() for item in batch]\n","    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.0)\n","\n","    return batch.unsqueeze(1)  # Add channel dimension for MFCC input\n","\n","\n","def collate_fn(batch):\n","    # A data tuple has the form:\n","    # waveform, label\n","\n","    # Separate audio samples and labels\n","    waveforms, labels, genders, ages, snrs = zip(*batch)\n","\n","    # Pad the audio samples (if needed)\n","    # padded_waveforms = pad_sequence(waveforms)\n","\n","    # Convert labels to tensor\n","    labels = torch.tensor(labels, dtype=torch.int32)\n","\n","    # Stack numeric features into a tensor and normalize them (if needed)\n","    # scaler = StandardScaler()\n","    genders = torch.tensor(genders)\n","    ages = torch.tensor(ages)\n","    snrs = torch.tensor(snrs)\n","    numeric_features = torch.stack((genders, ages, snrs), dim=1)\n","    # numeric_features = torch.tensor(scaler.fit_transform(numeric_features))\n","\n","    return waveforms, labels, numeric_features"]},{"cell_type":"markdown","metadata":{},"source":["# Miscellaneous functions\n","\n","The following code block contains miscellaneous functions such as plotting of waveforms, spectograms, fbank, and preprocessing of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-03-19T13:49:40.721847Z","iopub.status.busy":"2024-03-19T13:49:40.721427Z","iopub.status.idle":"2024-03-19T13:49:40.743883Z","shell.execute_reply":"2024-03-19T13:49:40.742959Z","shell.execute_reply.started":"2024-03-19T13:49:40.721817Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def waveform_plot(signal, sr, title, threshold=None, plot=None):\n","    # Calculate time axis\n","    time = np.arange(0, len(signal)) / sr\n","\n","    # Plot standard waveform\n","    plt.figure(figsize=(10, 8))\n","    plt.subplot(3, 1, 1)\n","    plt.plot(time, signal, color=\"b\")\n","    plt.xlabel(\"Time (s)\")\n","    plt.ylabel(\"Amplitude\")\n","    plt.title(title)\n","    plt.grid(True)\n","    plt.show()\n","\n","    if plot:\n","        # Calculate dBFS values\n","        if np.any(signal != 0):\n","            db_signal = 20 * np.log10(np.abs(signal) / np.max(np.abs(signal)))\n","        else:\n","            db_signal = -60\n","\n","        plt.subplot(3, 1, 2)\n","        # Plot waveform in dB scale\n","        plt.plot(time, db_signal, color=\"b\")\n","\n","        # Plot threshold level\n","        if threshold:\n","            plt.axhline(y=threshold, color=\"r\", linestyle=\"--\", label=f\"{threshold} dBFS Threshold\")\n","            plt.legend()\n","\n","        plt.xlabel(\"Time (s)\")\n","        plt.ylabel(\"Amplitude (dBFS)\")\n","        plt.title(title)\n","        plt.grid(True)\n","\n","        n_fft = 2048  # Length of the FFT window\n","        hop_length = 512  # Hop length for FFT\n","        S = np.abs(librosa.stft(signal.astype(float), n_fft=n_fft, hop_length=hop_length))\n","\n","        # Convert amplitude to dB scale (sound pressure level)\n","        S_db = librosa.amplitude_to_db(S, ref=np.max)\n","\n","        # Get frequency bins corresponding to FFT\n","        freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n","\n","        # Step 3: Plot the SPL values over frequency\n","        plt.subplot(3, 1, 3)\n","        plt.plot(freqs, np.mean(S_db, axis=1), color=\"b\")\n","        plt.title(\"Sound Pressure Level (SPL) vs. Frequency\")\n","        plt.xlabel(\"Frequency (Hz)\")\n","        plt.ylabel(\"SPL (dB)\")\n","        plt.grid(True)\n","        plt.xlim([20, 25000])  # Set frequency range for better visualization\n","        plt.xscale(\"log\")  # Use log scale for frequency axis\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","\n","# Stolen from pytorch tutorial xd\n","def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", batch=0, idx=0, ax=None):\n","    if ax is None:\n","        fig, ax = plt.subplots(1, 1)\n","    if title is not None:\n","        ax.set_title(title)\n","    ax.set_ylabel(ylabel)\n","    im = ax.imshow(\n","        librosa.power_to_db(specgram),\n","        origin=\"lower\",\n","        aspect=\"auto\",\n","        interpolation=\"nearest\",\n","    )\n","    plt.colorbar(im, ax=ax, label=\"dB\")\n","    # plt.close()\n","    plt.savefig(f\"test_outputs/batch{batch}_idx{idx}_{title}.png\")\n","\n","\n","def plot_fbank(fbank, title=None):\n","    fig, axs = plt.subplots(1, 1)\n","    axs.set_title(title or \"Filter bank\")\n","    axs.imshow(fbank, aspect=\"auto\")\n","    axs.set_ylabel(\"frequency bin\")\n","    axs.set_xlabel(\"mel bin\")\n","\n","\n","def preprocess_data(data_meta_path, data_dir_path, output_dir):\n","    # Read data file then remove every column other than the specified columns\n","    # Removes empty samples and filters through cough probability\n","    data = pd.read_csv(data_meta_path, sep=\",\")\n","    data = data[[\"uuid\", \"cough_detected\", \"SNR\", \"age\", \"gender\", \"status\"]].loc[data[\"cough_detected\"] >= 0.8].dropna().reset_index(drop=True).sort_values(by=\"cough_detected\")\n","    data = data[(data[\"gender\"] != \"other\")]\n","\n","    # Count the occurrences of each age value\n","    age_counts = data[\"age\"].value_counts()\n","\n","    # Filter out ages with fewer than 100 samples\n","    ages_to_keep = age_counts.index[age_counts >= 100]\n","\n","    # Filter the DataFrame based on the selected ages\n","    data = data[data[\"age\"].isin(ages_to_keep)]\n","\n","    # Check if the following MP3 with uuid exists\n","    mp3_data = []\n","    non_exist = []\n","    for file in data[\"uuid\"]:\n","        if os.path.exists(os.path.join(data_dir_path, f\"{file}.mp3\")):\n","            mp3_data.append(os.path.join(data_dir_path, f\"{file}.mp3\"))\n","        else:\n","            non_exist.append(file)\n","\n","    # Remove entries with missing MP3 files from the original data\n","    data = data[~data[\"uuid\"].isin(non_exist)]\n","\n","    # Replace the uuids with the path to uuid\n","    data[\"uuid\"] = mp3_data\n","\n","    # Save the data as csv\n","    data.to_csv(os.path.join(output_dir, \"filtered_audio_data.csv\"), index=False)\n","\n","    print(\"Finished processing!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.745170Z","iopub.status.busy":"2024-03-19T13:49:40.744916Z","iopub.status.idle":"2024-03-19T13:49:40.759858Z","shell.execute_reply":"2024-03-19T13:49:40.758764Z","shell.execute_reply.started":"2024-03-19T13:49:40.745148Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["\"\"\"\n","data_path = r\"misc_data/metadata_compiled.csv\"\n","data_dir_path = r\"../Dataset/MP3/\"\n","output_dir = r\"misc_data/\"\n","preprocess_data(data_path, data_dir_path, output_dir)\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Dataset specific functions\n","\n","The following codeblock contains functions specially related to the dataset preprocessing."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.761160Z","iopub.status.busy":"2024-03-19T13:49:40.760876Z","iopub.status.idle":"2024-03-19T13:49:40.776219Z","shell.execute_reply":"2024-03-19T13:49:40.775309Z","shell.execute_reply.started":"2024-03-19T13:49:40.761137Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def preprocess_dataset(data, test_size):\n","    # Extract audio samples and labels\n","    X = data.drop(columns=[\"status\"])\n","    y = data[\"status\"]\n","\n","    # Perform a stratified split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n","\n","    # Combine audio samples and target labels for training and validation sets\n","    train_data = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n","    test_data = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n","\n","    return train_data, test_data\n","\n","\n","def weighted_sample(data):\n","    # Find class distribution\n","    class_counts = data[\"status\"].value_counts()\n","\n","    # Adjust weighting to each sample\n","    sample_weights = [1 / class_counts[i] for i in data[\"status\"].values]\n","\n","    return sample_weights\n","\n","\n","def undersample(data, minority_class_label, n):\n","    # Identify minority class\n","    minority_class = minority_class_label\n","\n","    # Calculate desired class distribution (e.g., balanced distribution)\n","    desired_class_count = n  # Target number of samples for each class\n","\n","    # Select subset from minority class\n","    undersampled_data_minority = data[data[\"status\"] == minority_class].sample(n=desired_class_count)\n","\n","    # Combine with samples from majority classes\n","    undersampled_data_majority = data[~(data[\"status\"] == minority_class)]\n","\n","    # Combine undersampled minority class with majority classes\n","    undersampled_data = pd.concat([undersampled_data_majority, undersampled_data_minority])\n","\n","    # Shuffle the undersampled dataset\n","    undersampled_data = undersampled_data.sample(frac=1).reset_index(drop=True)\n","\n","    return undersampled_data\n","\n","\n","def visualize_dataset(data, normalize, title):\n","    print(f\"{title} Distribution\")\n","    print(data[\"status\"].value_counts(normalize=normalize))\n","    print(\"Total samples\", len(data))\n","\n","    plt.figure(figsize=(6, 4))\n","    plt.title(f\"Histogram of Patient Status\\n- {title}\")\n","    plt.bar(data[\"status\"].value_counts().index, data[\"status\"].value_counts())\n","    plt.xticks(rotation=20, ha=\"right\", fontsize=8)\n","    plt.xlabel(\"Class\", fontsize=8)\n","    plt.ylabel(\"Frequency\", fontsize=8)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization of dataset and dataset loader\n","\n","This codeblock includes the initialization of the dataset as well as any processing needed, such as splitting it into training/testing datasets, as well as different sampling techniques, such as undersampling/weighted sampling."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.778590Z","iopub.status.busy":"2024-03-19T13:49:40.778278Z","iopub.status.idle":"2024-03-19T13:49:40.838232Z","shell.execute_reply":"2024-03-19T13:49:40.837445Z","shell.execute_reply.started":"2024-03-19T13:49:40.778565Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def select_dataset(dataset_type, train_data, val_data, test_data, args, batch, workers, undersampling=500):\n","    if dataset_type == \"undersampled\":\n","        # Prepare and create undersampled version\n","        undersampled_data = undersample(data, \"healthy\", undersampling)\n","        undersampled_data = undersample(undersampled_data, \"symptomatic\", undersampling)\n","\n","        # train_undersampled_data, test_undersampled_data = preprocess_dataset(undersampled_data, 0.3) # ORIGINAL\n","        train_undersampled_data, val_undersampled_data = preprocess_dataset(undersampled_data, 0.3)\n","        # val_undersampled_data, test_undersampled_data = preprocess_dataset(test_undersampled_data, 0.5)\n","\n","        # Undersampled dataset\n","        train_dataset = AudioDataset(train_undersampled_data, args, le)\n","        val_dataset = AudioDataset(val_undersampled_data, args, le)\n","        test_dataset = AudioDataset(val_undersampled_data, args, le)  # CHANGE THIS!\n","\n","        train_dataloader = DataLoader(\n","            train_dataset,\n","            batch_size=batch,\n","            shuffle=True,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        val_dataloader = DataLoader(\n","            val_dataset,\n","            batch_size=batch,\n","            shuffle=False,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            batch_size=batch,\n","            shuffle=False,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","\n","        return train_dataloader, val_dataloader, test_dataloader\n","\n","    elif dataset_type == \"weighted\":\n","        # Prepare and create weighted sampler\n","        train_sample_weights = weighted_sample(train_data)\n","        val_sample_weights = weighted_sample(val_data)\n","        test_sample_weights = weighted_sample(test_data)\n","\n","        train_weighted_Sampler = WeightedRandomSampler(weights=train_sample_weights, num_samples=len(train_data), replacement=True)\n","        val_weighted_Sampler = WeightedRandomSampler(weights=val_sample_weights, num_samples=len(val_data), replacement=True)\n","        test_weighted_Sampler = WeightedRandomSampler(weights=test_sample_weights, num_samples=len(test_data), replacement=True)\n","\n","        # Create dataset and dataloader instances\n","        train_dataset = AudioDataset(train_data, args, le)\n","        val_dataset = AudioDataset(val_data, args, le)\n","        test_dataset = AudioDataset(test_data, args, le)\n","\n","        train_dataloader = DataLoader(\n","            train_dataset,\n","            sampler=train_weighted_Sampler,\n","            batch_size=batch,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        val_dataloader = DataLoader(\n","            val_dataset,\n","            sampler=val_weighted_Sampler,\n","            batch_size=batch,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","        test_dataloader = DataLoader(\n","            test_dataset,\n","            sampler=test_weighted_Sampler,\n","            batch_size=batch,\n","            num_workers=workers,\n","            collate_fn=collate_fn,\n","            pin_memory=True,\n","        )\n","\n","        return train_dataloader, val_dataloader, test_dataloader\n","\n","\n","# Initialize LabelEncoder\n","le = LabelEncoder()\n","\n","# Fit and transform labels into encoded form\n","encoded_labels = le.fit_transform(labels)\n","\n","# Silence removal arguments\n","args = [min_silence, threshold_dBFS, keep_silence]\n","\n","# Prepare standard dataset\n","train_data, test_data = preprocess_dataset(data, 0.3)  # First split the original dataset into 70% training\n","val_data, test_data = preprocess_dataset(test_data, 0.5)  # Second split the \"test_data\" into 50/50 validation and test (technically 15/15)\n","\n","# Initialize dataloaders\n","train_dataloader, val_dataloader, _ = select_dataset(dataset_type, train_data, val_data, test_data, args, batch, workers, undersampling)"]},{"cell_type":"markdown","metadata":{},"source":["# Initialize and define MFCC feature extractor\n","\n","In the following codeblock the MFCC specific parameters are defined and initialized. The codeblock also includes a function that pads the extracted MFCC features in order to pass it to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.839656Z","iopub.status.busy":"2024-03-19T13:49:40.839356Z","iopub.status.idle":"2024-03-19T13:49:40.850693Z","shell.execute_reply":"2024-03-19T13:49:40.849660Z","shell.execute_reply.started":"2024-03-19T13:49:40.839631Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def MFCC_Features(data, padding=False, normalize=False, resize=False):\n","    \"\"\"\n","    Args:\n","    data: Input audio waveform\n","    max_length: Maximum length for padding\n","    normalize: Normalize the channel layer\n","    resize: Resize the spectrogram\n","    target_size: Target size for resizing\n","    \"\"\"\n","    # Extract MFCC features\n","    # features = mfcc(data)\n","    features = [torch.unsqueeze(mfcc(waveform), 0) for waveform in data]  # Adding channels\n","    # features = [torch.unsqueeze(torch.unsqueeze(mfcc(waveform), 0), 0) for waveform in data] # Adding batch size and channels\n","\n","    # Hardcoded padding (WIP)\n","    if padding:\n","        features = F.pad(features, (0, padding - features.shape[3]), \"constant\", 0)\n","\n","    # Normalize the features for each sample\n","    if normalize == True:\n","        for j, feature in enumerate(features):\n","            mean = feature.mean(dim=[1, 2], keepdim=True)\n","            std = feature.std(dim=[1, 2], keepdim=True)\n","            features[j] = (feature - mean) / std\n","\n","    # Resize mel spectrograms\n","    if resize == True:\n","        features = [Resize((224, 224), antialias=True)(feature) for feature in features]\n","\n","    # Stack each feature as [batch_size, channels, features, length]\n","    features = torch.stack(features)\n","\n","    return features\n","\n","\n","# Instantiate MFCC feature extractor\n","mfcc = MFCC(n_mfcc=n_mfcc, sample_rate=sample_rate, melkwargs=melkwargs)"]},{"cell_type":"markdown","metadata":{},"source":["# Spectral features extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.852535Z","iopub.status.busy":"2024-03-19T13:49:40.852121Z","iopub.status.idle":"2024-03-19T13:49:40.871647Z","shell.execute_reply":"2024-03-19T13:49:40.870755Z","shell.execute_reply.started":"2024-03-19T13:49:40.852506Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def spectral_centroid(S=None, sr=22050, nfft=2048, h_length=512):\n","    return librosa.feature.spectral_centroid(S=S, sr=sr, n_fft=nfft, hop_length=h_length)\n","\n","\n","def root_mean_square(S=None, f_length=2048, h_length=512):\n","    return librosa.feature.rms(S=S, frame_length=f_length, hop_length=h_length)\n","\n","\n","def zero_crossing_rate(signal, f_length=2048, h_length=512):\n","    return librosa.feature.zero_crossing_rate(y=signal, frame_length=f_length, hop_length=h_length)\n","\n","\n","def dynamic_parameters(audio_sample, sr=48000):\n","    duration_seconds = len(audio_sample) / sr\n","\n","    if duration_seconds <= 0.5:  # Very short audio (less than 0.5 seconds)\n","        n_fft = 512\n","        hop_length = 128\n","        frame_length = 512\n","    elif 0.5 < duration_seconds <= 1:  # Short audio (0.5 - 1 second)\n","        n_fft = 1024\n","        hop_length = 256\n","        frame_length = 1024\n","    elif 1 < duration_seconds <= 5:  # Medium-length audio (1-5 seconds)\n","        n_fft = 2048\n","        hop_length = 512\n","        frame_length = 2048\n","    else:  # Long audio (longer than 5 seconds)\n","        n_fft = 4096\n","        hop_length = 1024\n","        frame_length = 4096\n","    return n_fft, hop_length, frame_length\n","\n","\n","def spectral_features(y):\n","    #ZCR_features = []\n","    #RMS_features = []\n","    #SC_features = []\n","    spectral_features = []\n","    for sample in y:\n","        sample = np.asarray(sample)\n","\n","        # Return n_fft, hop_length, frame_length based on length of sample\n","        n_fft, hop_length, frame_length = dynamic_parameters(sample)\n","\n","        # Compute magnitude spectrum of sample\n","        S, _ = librosa.magphase(librosa.stft(y=sample, n_fft=n_fft, hop_length=hop_length, win_length=n_fft, window=\"hann\", center=True, pad_mode=\"constant\"))\n","\n","        # Compute ZCR, RMS, and SC for additional feature extraction\n","        ZCR = zero_crossing_rate(signal=sample, f_length=frame_length, h_length=hop_length)\n","        RMS = root_mean_square(S=S, f_length=frame_length, h_length=hop_length)\n","        SC = spectral_centroid(S=S, sr=48000, nfft=n_fft, h_length=hop_length)\n","\n","        # Convert into tensors and normalize\n","        ZCR = torch.tensor(ZCR, dtype=torch.float32)\n","        RMS = torch.tensor(RMS, dtype=torch.float32)\n","        SC = torch.tensor(SC, dtype=torch.float32)\n","        ZCR = (ZCR - ZCR.mean()) / ZCR.std()\n","        RMS = (RMS - RMS.mean()) / RMS.std()\n","        SC = (SC - SC.mean()) / SC.std()\n","\n","        # Append the features for the current signal to the list\n","        #ZCR_features.append(ZCR)\n","        #RMS_features.append(RMS)\n","        #SC_features.append(SC)\n","        spectral_features.append((ZCR,RMS,SC))\n","\n","    # Convert list of tuples of spectral features into tuple of lists\n","    #ZCR, RMS, SC = zip(*spectral_features)\n","   \n","    # Compute the maximum length of features the combined features\n","    max_len = max(max(zcr.shape[1], rms.shape[1], sc.shape[1]) for zcr, rms, sc in spectral_features)\n","    \n","    # Pad each feature vector to the max length\n","    ZCR = [F.pad(zcr, (0, max_len - zcr.shape[1]), value=0.0) for zcr in ZCR]\n","    RMS = [F.pad(rms, (0, max_len - rms.shape[1]), value=0.0) for rms in RMS]\n","    SC = [F.pad(sc, (0, max_len - sc.shape[1]), value=0.0) for sc in SC]\n","    \n","    # Stack each list of tensors to create new shape:\n","    # [batch_size, feature_size, feature_length]\n","    ZCR = torch.stack(ZCR, dim=0)\n","    RMS = torch.stack(RMS, dim=0)\n","    SC = torch.stack(SC, dim=0)\n","    #print(\"ZCR shape:\", ZCR.shape)\n","    #print(\"RMS shape:\", RMS.shape)\n","    #print(\"SC shape:\", SC.shape)\n","\n","\n","    # Stack the individual spectral features to create new shape:\n","    # [batch_size, spectral_features, spectral_features_length]\n","    #combined_features = torch.stack((ZCR, RMS, SC), dim=1).permute(0,2,1,3)\n","    #print(\"combined features shape\", combined_features.shape)\n","    #combined_features = torch.stack((ZCR.squeeze(1), RMS.squeeze(1), SC.squeeze(1)), dim=1)\n","    #print(\"combined features shape squeezing\", combined_features.shape)\n","\n","    #tensor_list = torch.stack(RMS_features, dim=0)\n","    #print(\"tensor list RMS:\", tensor_list.shape)\n","\n","    #return combined_features\n","    return ZCR, RMS, SC"]},{"cell_type":"markdown","metadata":{},"source":["# Network architectures"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.874580Z","iopub.status.busy":"2024-03-19T13:49:40.874169Z","iopub.status.idle":"2024-03-19T13:49:40.908073Z","shell.execute_reply":"2024-03-19T13:49:40.907070Z","shell.execute_reply.started":"2024-03-19T13:49:40.874552Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class MultiInputResNet(nn.Module):\n","    def __init__(self, weights=None, num_classes=3, model_arch=\"resnet18\"):\n","        super(MultiInputResNet, self).__init__()\n","        if model_arch == \"resnet18\":\n","            self.resnet = models.resnet18(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet34\":\n","            self.resnet = models.resnet34(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet50\":\n","            self.resnet = models.resnet50(weights=weights, num_classes=num_classes)\n","\n","        # Adjust the first convolutional layer to match number of channels\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","        # Add additional branch to handle numeric features\n","        self.numeric_features = nn.Sequential(\n","            nn.Linear(3, 64),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","            nn.Linear(64, 512),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","        )\n","\n","        # Adding a fully connected layer\n","        self.fc = nn.Linear(512 + 3, num_classes)  # 512 from ResNet + 3 from numeric features\n","\n","    def forward(self, mfcc, numeric):\n","        # Process MFCC input through ResNet\n","        mfcc_resnet_output = self.resnet(mfcc)\n","\n","        # Process numeric features through additional branch\n","        numeric_output = self.numeric_features(numeric)\n","\n","        # Concatenate the outputs from both branches\n","        combined_features = torch.cat((mfcc_resnet_output, numeric_output), dim=1)\n","\n","        # return raw scores/logits\n","        output = self.fc(combined_features)\n","\n","        # Apply softmax activation to get probabilities\n","        # output_probs = F.softmax(output, dim=1)\n","\n","        return output\n","\n","\n","class MultiInputResNet_spectral(nn.Module):\n","    def __init__(self, weights=None, num_classes=3, model_arch=\"resnet18\"):\n","        super(MultiInputResNet_spectral, self).__init__()\n","        if model_arch == \"resnet18\":\n","            self.resnet = models.resnet18(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet34\":\n","            self.resnet = models.resnet34(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet50\":\n","            self.resnet = models.resnet50(weights=weights, num_classes=num_classes)\n","\n","        # Adjust the first convolutional layer to match number of channels\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","        # Add additional branch to handle numeric features\n","        self.numeric_features = nn.Sequential(\n","            nn.Linear(3, 64),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","            nn.Linear(64, 512),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","        )\n","\n","        # Add additional branch to handle spectral features\n","        self.spectral_features = nn.Sequential(\n","            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1), \n","            nn.ReLU(inplace=True), \n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1), \n","            nn.ReLU(inplace=True), \n","            nn.MaxPool1d(kernel_size=2), \n","            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1), \n","            nn.ReLU(inplace=True), \n","            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), \n","            nn.ReLU(inplace=True), \n","            nn.AdaptiveAvgPool1d(1)\n","        )\n","\n","        # Adding a fully connected layer\n","        self.fc = nn.Linear(512 + 3 + 512 + 512 + 512, num_classes)  # 512 from ResNet, 512 from each spectral feature + 3 from numeric features\n","\n","        # Adding fully connected layers for age and gender prediction\n","        self.fc_age = nn.Linear(3, 4)\n","        self.fc_gender = nn.Linear(3, 2)\n","\n","    def forward(self, mfcc, numeric, ZCR, RMS, SC):\n","        # Process MFCC input through ResNet\n","        mfcc_resnet_output = self.resnet(mfcc)\n","\n","        # Process numeric features through the additional branch\n","        numeric_output = self.numeric_features(numeric)\n","        numeric_output2 = F.relu(numeric)\n","\n","        # Process each spectral features through the additional branch\n","        ZCR_output = self.spectral_features(ZCR).squeeze(dim=2)\n","        RMS_output = self.spectral_features(RMS).squeeze(dim=2)\n","        SC_output = self.spectral_features(SC).squeeze(dim=2)\n","        #print(\"ZCR_output shape:\", ZCR_output.shape)\n","        #print(\"RMS_output shape:\", RMS_output.shape)\n","        #print(\"SC_output shape:\", SC_output.shape)\n","\n","        \n","        #ZCR_output = []\n","        #RMS_output = []\n","        #SC_output = []\n","        #for zcr, rms, sc in zip(ZCR, RMS, SC):\n","        #    ZCR_output.append(self.spectral_features(zcr))\n","        #    RMS_output.append(self.spectral_features(rms))\n","        #    SC_output.append(self.spectral_features(sc))\n","\n","        ## Concatenate the outputs from all branches\n","        #ZCR_output = torch.stack(ZCR_output, dim=0).squeeze(dim=2)\n","        #RMS_output = torch.stack(RMS_output, dim=0).squeeze(dim=2)\n","        #SC_output = torch.stack(SC_output, dim=0).squeeze(dim=2)     \n","        combined_features = torch.cat((mfcc_resnet_output, numeric_output, ZCR_output, RMS_output, SC_output), dim=1)\n","\n","        # Classification output\n","        output_class = self.fc(combined_features)\n","        \n","        # Age and gender predictions\n","        output_age = self.fc_age(numeric_output2)\n","        output_gender = self.fc_gender(numeric_output2)\n","\n","        # Apply softmax activation to get probabilities\n","        # output_probs = F.softmax(output, dim=1)\n","\n","        return output_class, output_age, output_gender\n","\n","\n","class Modified_MultiInputResNet(nn.Module):\n","    def __init__(self, weights=None, num_classes=3, model_arch=\"resnet18\"):\n","        super(Modified_MultiInputResNet, self).__init__()\n","        if model_arch == \"resnet18\":\n","            self.resnet = models.resnet18(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet34\":\n","            self.resnet = models.resnet34(weights=weights, num_classes=num_classes)\n","        elif model_arch == \"resnet50\":\n","            self.resnet = models.resnet50(weights=weights, num_classes=num_classes)\n","\n","        # Adjust the first convolutional layer to match number of channels\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","        # Add additional branch to handle numeric features\n","        self.numeric_features = nn.Sequential(\n","            nn.Linear(3, 64),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","            nn.Linear(64, 512),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(0.5),\n","        )\n","\n","        # Adding a fully connected layer\n","        self.fc = nn.Linear(512 + 3, num_classes)  # 512 from ResNet + 3 from numeric features\n","\n","        # Adding fully connected layers for age and gender prediction\n","        self.fc_age = nn.Linear(3, 4)\n","        self.fc_gender = nn.Linear(3, 2)\n","\n","    def forward(self, mfcc, numeric):\n","        # Process MFCC input through ResNet\n","        mfcc_resnet_output = self.resnet(mfcc)\n","\n","        # Process numeric features through additional branch\n","        numeric_output = self.numeric_features(numeric)\n","        numeric_output2 = F.relu(numeric)\n","\n","        # Concatenate the outputs from both branches\n","        combined_features = torch.cat((mfcc_resnet_output, numeric_output), dim=1)\n","\n","        output_class = self.fc(combined_features)\n","        output_age = self.fc_age(numeric_output2)\n","        output_gender = self.fc_gender(numeric_output2)\n","\n","        ## Apply softmax activation to get probabilities\n","        # output_probs_class = F.softmax(output_class, dim=1)\n","        # output_probs_age = F.softmax(output_age, dim=1)\n","        # output_probs_gender = F.softmax(output_gender, dim=1)\n","\n","        return output_class, output_age, output_gender"]},{"cell_type":"markdown","metadata":{},"source":["# Setup weights and bias logging"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:49:40.909385Z","iopub.status.busy":"2024-03-19T13:49:40.909119Z","iopub.status.idle":"2024-03-19T13:50:18.386776Z","shell.execute_reply":"2024-03-19T13:50:18.385713Z","shell.execute_reply.started":"2024-03-19T13:49:40.909362Z"},"trusted":true},"outputs":[],"source":["#Initialize wandb\n","#!wandb login --relogin 9be53a0c7076cae09612be80ee5e0e80d9dac79c\n","\n","#Defining weights and biases config\n","#wandb.init(\n","#   # set the wandb project where this run will be logged\n","#   project=\"mini-project\",\n","#   config={\n","#   \"architecture\": arch,\n","#   \"dataset\": dataset,\n","#   \"description\": desc,\n","#   \"learning_rate\": lr,\n","#   \"step_size\": step,\n","#   \"weight_decay\": decay,\n","#   \"optimizer\": optimizer_type,\n","#   \"gamma\": gamma,\n","#   \"epochs\": epochs\n","#   }\n","#)"]},{"cell_type":"markdown","metadata":{},"source":["# Compute class weights for loss function (if using weighted)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:50:18.388807Z","iopub.status.busy":"2024-03-19T13:50:18.388394Z","iopub.status.idle":"2024-03-19T13:50:18.956019Z","shell.execute_reply":"2024-03-19T13:50:18.955161Z","shell.execute_reply.started":"2024-03-19T13:50:18.388770Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","train_labels = train_data[\"status\"]\n","class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n","\n","print(\"COVID-19\",\"Healthy\", \"Symptomatic\")\n","print(class_weights)"]},{"cell_type":"markdown","metadata":{},"source":["# Training and validation functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:50:18.958239Z","iopub.status.busy":"2024-03-19T13:50:18.957653Z","iopub.status.idle":"2024-03-19T13:50:19.612302Z","shell.execute_reply":"2024-03-19T13:50:19.611338Z","shell.execute_reply.started":"2024-03-19T13:50:18.958211Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","\n","\n","def initialize_training_setup(model, optimizer_type, weighted=False):\n","    if optimizer_type == \"adam\":\n","        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n","\n","    else:\n","        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=decay)\n","\n","    if weighted == True:\n","        criterion = nn.CrossEntropyLoss(weight=class_weights)\n","    else:\n","        criterion = nn.CrossEntropyLoss()\n","\n","    return optimizer, criterion\n","\n","\n","def train_epoch_multi(model, device, epoch, optimizer, criterion):\n","    model.train()\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    print(\"Currently: Training\")\n","    for i, (inputs, targets, numeric) in tqdm(\n","        enumerate(train_dataloader),\n","        total=len(train_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Training\",\n","    ):\n","        # Training loop\n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        features, targets, numeric = features.to(device), targets.to(device), numeric.to(device)  # Load them onto GPU\n","        optimizer.zero_grad()  # Zero the parameters\n","        outputs = model(features, numeric)  # Retrieve the output from the model\n","        loss = criterion(outputs, targets)  # Compute the loss\n","        loss.backward()  # Compute gradients of the loss\n","        optimizer.step()  # Update weights\n","        running_loss += loss.item()\n","        # Calculate correct predictions\n","        _, predicted = torch.max(outputs, 1)\n","        correct_predictions += (predicted == targets).sum().item()\n","        total_predictions += targets.size(0)\n","\n","        # Print statistics for every 10th mini-batch\n","        if i % 10 == 9:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i}/{len(train_dataloader)} | Training Loss: {loss.item():.4f}\")\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    accuracy = correct_predictions / total_predictions\n","    print(f\"Training Accuracy: {accuracy:.4f}\")\n","    avg_loss = running_loss / len(train_dataloader)\n","    return avg_loss, accuracy\n","\n","\n","def train_epoch_modified(model, device, epoch, optimizer, criterion):\n","    model.train()\n","    running_loss = 0.0\n","    correct_predictions_class = 0\n","    correct_predictions_age = 0\n","    correct_predictions_gender = 0\n","    total_predictions_class = 0\n","    total_predictions_age = 0\n","    total_predictions_gender = 0\n","\n","    print(\"Currently: Training\")\n","    for i, (inputs, targets, numeric) in tqdm(\n","        enumerate(train_dataloader),\n","        total=len(train_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Training\",\n","    ):\n","        # Convert features and target labels into long\n","        class_targets = torch.round(targets).to(torch.long)\n","        gender_targets = torch.round(numeric[:, 0]).to(torch.long)\n","        age_targets = torch.round(numeric[:, 1]).to(torch.long)\n","\n","        # Load them onto GPU\n","        class_targets = class_targets.to(device)\n","        gender_targets = gender_targets.to(device)\n","        age_targets = age_targets.to(device)\n","\n","        # Training loop\n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        features, numeric = features.to(device), numeric.to(device)  # Load them onto GPU\n","        optimizer.zero_grad()  # Zero the parameters\n","        output_class, output_age, output_gender = model(features, numeric)  # Retrieve the output from the model\n","\n","        # Compute the loss for each output separately\n","        loss_class = criterion(output_class, class_targets)\n","        loss_gender = criterion(output_gender, gender_targets)\n","        loss_age = criterion(output_age, age_targets)\n","\n","        # Compute total loss (you can also use weighted sum of losses)\n","        total_loss = loss_class + loss_age + loss_gender\n","        total_loss.backward()  # Compute gradients of the loss\n","        optimizer.step()  # Update weights\n","\n","        running_loss += total_loss.item()\n","\n","        # Compute correct predictions and update statistics for class prediction\n","        _, predicted_class = torch.max(output_class, 1)\n","        correct_predictions_class += (predicted_class == class_targets).sum().item()\n","        total_predictions_class += class_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for age prediction\n","        _, predicted_age = torch.max(output_age, 1)\n","        correct_predictions_age += (predicted_age == age_targets).sum().item()\n","        total_predictions_age += age_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for gender prediction\n","        _, predicted_gender = torch.max(output_gender, 1)\n","        correct_predictions_gender += (predicted_gender == gender_targets).sum().item()\n","        total_predictions_gender += gender_targets.size(0)\n","\n","        # Print statistics for every 10th mini-batch\n","        if i % 10 == 9:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i+1}/{len(train_dataloader)} | Training Loss: {total_loss.item():.4f}\")\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    accuracy_class = correct_predictions_class / total_predictions_class\n","    accuracy_age = correct_predictions_age / total_predictions_age\n","    accuracy_gender = correct_predictions_gender / total_predictions_gender\n","    print(f\"Training Accuracy Class: {accuracy_class:.4f}\")\n","    print(f\"Training Accuracy Age: {accuracy_age:.4f}\")\n","    print(f\"Training Accuracy Gender: {accuracy_gender:.4f}\")\n","    avg_loss = running_loss / len(train_dataloader)\n","\n","    return avg_loss, accuracy_class, accuracy_age, accuracy_gender\n","\n","\n","def train_epoch(model, device, epoch, optimizer, criterion):\n","    model.train()\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    print(\"Currently: Training\")\n","    for i, (inputs, targets) in tqdm(\n","        enumerate(train_dataloader),\n","        total=len(train_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Training\",\n","    ):\n","        # Training loop\n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        features, targets = features.to(device), targets.to(device)  # Load them onto GPU\n","        optimizer.zero_grad()  # Zero the parameters\n","        outputs = model(features)  # Retrieve the output from the model\n","        loss = criterion(outputs, targets)  # Compute the loss\n","        loss.backward()  # Compute gradients of the loss\n","        optimizer.step()  # Update weights\n","        running_loss += loss.item()\n","\n","        # Calculate correct predictions\n","        _, predicted = torch.max(outputs, 1)\n","        correct_predictions += (predicted == targets).sum().item()\n","        total_predictions += targets.size(0)\n","\n","        # Print statistics for every 10th mini-batch\n","        if i % 10 == 9:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i}/{len(train_dataloader)} | Training Loss: {loss.item():.4f}\")\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    accuracy = correct_predictions / total_predictions\n","    print(f\"Training Accuracy: {accuracy:.4f}\")\n","    avg_loss = running_loss / len(train_dataloader)\n","    return avg_loss, accuracy\n","\n","\n","def train_epoch_spectral(model, device, epoch, optimizer, criterion):\n","    model.train()\n","    running_loss = 0.0\n","    correct_predictions_class = 0\n","    correct_predictions_age = 0\n","    correct_predictions_gender = 0\n","    total_predictions_class = 0\n","    total_predictions_age = 0\n","    total_predictions_gender = 0\n","\n","    print(\"Currently: Training\")\n","    for i, (inputs, targets, numeric) in tqdm(\n","        enumerate(train_dataloader),\n","        total=len(train_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Training\",\n","    ):\n","        # Convert features and target labels into long\n","        class_targets = torch.round(targets).to(torch.long)\n","        gender_targets = torch.round(numeric[:, 0]).to(torch.long)\n","        age_targets = torch.round(numeric[:, 1]).to(torch.long)\n","\n","        # Load them onto GPU\n","        class_targets = class_targets.to(device)\n","        gender_targets = gender_targets.to(device)\n","        age_targets = age_targets.to(device)\n","\n","        # Training loop\n","        features = MFCC_Features(inputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        features, numeric= features.to(device), numeric.to(device) # Load them onto GPU\n","        ZCR, RMS, SC = spectral_features(inputs)\n","        ZCR, RMS, SC = ZCR.to(device), RMS.to(device), SC.to(device)   \n","        optimizer.zero_grad()  # Zero the parameters\n","        output_class, output_age, output_gender = model(features, numeric, ZCR, RMS, SC)  # Retrieve the output from the model\n","\n","        # Compute the loss for each output separately\n","        loss_class = criterion(output_class, class_targets)\n","        loss_gender = criterion(output_gender, gender_targets)\n","        loss_age = criterion(output_age, age_targets)\n","\n","        # Compute total loss (you can also use weighted sum of losses)\n","        total_loss = loss_class + loss_age + loss_gender\n","        total_loss.backward()  # Compute gradients of the loss\n","        optimizer.step()  # Update weights\n","\n","        running_loss += total_loss.item()\n","\n","        # Compute correct predictions and update statistics for class prediction\n","        _, predicted_class = torch.max(output_class, 1)\n","        correct_predictions_class += (predicted_class == class_targets).sum().item()\n","        total_predictions_class += class_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for age prediction\n","        _, predicted_age = torch.max(output_age, 1)\n","        correct_predictions_age += (predicted_age == age_targets).sum().item()\n","        total_predictions_age += age_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for gender prediction\n","        _, predicted_gender = torch.max(output_gender, 1)\n","        correct_predictions_gender += (predicted_gender == gender_targets).sum().item()\n","        total_predictions_gender += gender_targets.size(0)\n","\n","        # Print statistics for every 10th mini-batch\n","        if i % 5 == 4:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {i+1}/{len(train_dataloader)} | Training Loss: {total_loss.item():.4f}\")\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    accuracy_class = correct_predictions_class / total_predictions_class\n","    accuracy_age = correct_predictions_age / total_predictions_age\n","    accuracy_gender = correct_predictions_gender / total_predictions_gender\n","    print(f\"Training Accuracy Class: {accuracy_class:.4f}\")\n","    print(f\"Training Accuracy Age: {accuracy_age:.4f}\")\n","    print(f\"Training Accuracy Gender: {accuracy_gender:.4f}\")\n","    avg_loss = running_loss / len(train_dataloader)\n","\n","    return avg_loss, accuracy_class, accuracy_age, accuracy_gender\n","\n","\n","# TODO CREATE SEPARATE VALIDATION LOOP FOR EACH MODEL TYPE!\n","def validate_epoch_multi(model, device, epoch, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    print(\"Currently: Validating\")\n","    for j, (vinputs, vtargets, vnumeric) in tqdm(\n","        enumerate(val_dataloader),\n","        total=len(val_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Validating\",\n","    ):\n","\n","        # Validation loop\n","        vfeatures = MFCC_Features(vinputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        vfeatures, vtargets, vnumeric = (\n","            vfeatures.to(device),\n","            vtargets.to(device),\n","            vnumeric.to(device),\n","        )  # Load them onto GPU\n","        voutputs = model(vfeatures, vnumeric)\n","        vloss = criterion(voutputs, vtargets)\n","\n","        running_loss += vloss.item()\n","\n","        # Calculate correct predictions\n","        _, vpredicted = torch.max(voutputs, 1)\n","        correct_predictions += (vpredicted == vtargets).sum().item()\n","        total_predictions += vtargets.size(0)\n","\n","    # Compute and return average validation loss, accuracy, precision, recall, and F1 score\n","    avg_vloss = running_loss / len(val_dataloader)\n","\n","    # Compute accuracy\n","    vaccuracy = correct_predictions / total_predictions\n","\n","    # Compute precision, recall, F1 score\n","    precision = precision_score(vtargets.cpu(), vpredicted.cpu(), average=\"macro\", zero_division=0.0)\n","    recall = recall_score(vtargets.cpu(), vpredicted.cpu(), average=\"macro\", zero_division=0.0)\n","    f1 = f1_score(vtargets.cpu(), vpredicted.cpu(), average=\"macro\", zero_division=0.0)\n","\n","    return avg_vloss, vaccuracy, precision, recall, f1\n","\n","\n","def validate_epoch_modified(model, device, epoch, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    correct_predictions_class = 0\n","    correct_predictions_age = 0\n","    correct_predictions_gender = 0\n","    total_predictions_class = 0\n","    total_predictions_age = 0\n","    total_predictions_gender = 0\n","\n","    print(\"Currently: Validating\")\n","    for j, (vinputs, vtargets, vnumeric) in tqdm(\n","        enumerate(val_dataloader),\n","        total=len(val_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Validating\",\n","    ):\n","        class_targets = torch.round(vtargets).to(torch.long)\n","        gender_targets = torch.round(vnumeric[:, 0]).to(torch.long)\n","        age_targets = torch.round(vnumeric[:, 1]).to(torch.long)\n","\n","        class_targets = class_targets.to(device)\n","        gender_targets = gender_targets.to(device)\n","        age_targets = age_targets.to(device)\n","\n","        # Validation loop\n","        vfeatures = MFCC_Features(vinputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        vfeatures, vnumeric = (vfeatures.to(device), vnumeric.to(device))  # Load them onto GPU\n","        voutput_class, voutput_age, voutput_gender = model(vfeatures, vnumeric)\n","\n","        # Compute the loss for each output separately\n","        vloss_class = criterion(voutput_class, class_targets)\n","        vloss_gender = criterion(voutput_gender, gender_targets)\n","        vloss_age = criterion(voutput_age, age_targets)\n","\n","        # Compute total loss (you can also use weighted sum of losses)\n","        total_loss = vloss_class + vloss_age + vloss_gender\n","\n","        running_loss += total_loss.item()\n","\n","        # Compute correct predictions and update statistics for class prediction\n","        _, predicted_class = torch.max(voutput_class, 1)\n","        correct_predictions_class += (predicted_class == class_targets).sum().item()\n","        total_predictions_class += class_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for age prediction\n","        _, predicted_age = torch.max(voutput_age, 1)\n","        correct_predictions_age += (predicted_age == age_targets).sum().item()\n","        total_predictions_age += age_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for gender prediction\n","        _, predicted_gender = torch.max(voutput_gender, 1)\n","        correct_predictions_gender += (predicted_gender == gender_targets).sum().item()\n","        total_predictions_gender += gender_targets.size(0)\n","\n","    # Compute and return average validation loss, accuracy, precision, recall, and F1 score\n","    avg_vloss = running_loss / len(val_dataloader)\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    vaccuracy_class = correct_predictions_class / total_predictions_class\n","    vaccuracy_age = correct_predictions_age / total_predictions_age\n","    vaccuracy_gender = correct_predictions_gender / total_predictions_gender\n","\n","    # Compute precision, recall, F1 score\n","    precision_class = precision_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","    precision_gender = precision_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","    precision_age = precision_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","    recall_class = recall_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","    recall_gender = recall_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","    recall_age = recall_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","    f1_class = f1_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","    f1_gender = f1_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","    f1_age = f1_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","\n","    # return avg_vloss, vaccuracy, precision, recall, f1\n","    metrics = ((vaccuracy_class, vaccuracy_age, vaccuracy_gender), (precision_class, precision_age, precision_gender), (recall_class, recall_age, recall_gender), (f1_class, f1_age, f1_gender))\n","\n","    return avg_vloss, metrics\n","\n","\n","def validate_epoch_spectral(model, device, epoch, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    correct_predictions_class = 0\n","    correct_predictions_age = 0\n","    correct_predictions_gender = 0\n","    total_predictions_class = 0\n","    total_predictions_age = 0\n","    total_predictions_gender = 0\n","\n","    print(\"Currently: Validating\")\n","    for j, (vinputs, vtargets, vnumeric) in tqdm(\n","        enumerate(val_dataloader),\n","        total=len(val_dataloader),\n","        leave=True,\n","        desc=f\"Epoch {epoch+1}/{epochs} | Validating\",\n","    ):\n","        class_targets = torch.round(vtargets).to(torch.long)\n","        gender_targets = torch.round(vnumeric[:, 0]).to(torch.long)\n","        age_targets = torch.round(vnumeric[:, 1]).to(torch.long)\n","\n","        class_targets = class_targets.to(device)\n","        gender_targets = gender_targets.to(device)\n","        age_targets = age_targets.to(device)\n","\n","        # Validation loop\n","        vfeatures = MFCC_Features(vinputs, padding=False, normalize=True, resize=True)  # Compute the MFCC features\n","        vfeatures, vnumeric = vfeatures.to(device), vnumeric.to(device)  # Load them onto GPU\n","        ZCR, RMS, SC = spectral_features(vinputs)\n","        ZCR, RMS, SC = ZCR.to(device), RMS.to(device), SC.to(device)\n","        voutput_class, voutput_age, voutput_gender = model(vfeatures, vnumeric, ZCR, RMS, SC)\n","\n","        # Compute the loss for each output separately\n","        vloss_class = criterion(voutput_class, class_targets)\n","        vloss_gender = criterion(voutput_gender, gender_targets)\n","        vloss_age = criterion(voutput_age, age_targets)\n","\n","        # Compute total loss (you can also use weighted sum of losses)\n","        total_loss = vloss_class + vloss_age + vloss_gender\n","\n","        running_loss += total_loss.item()\n","\n","        # Compute correct predictions and update statistics for class prediction\n","        _, predicted_class = torch.max(voutput_class, 1)\n","        correct_predictions_class += (predicted_class == class_targets).sum().item()\n","        total_predictions_class += class_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for age prediction\n","        _, predicted_age = torch.max(voutput_age, 1)\n","        correct_predictions_age += (predicted_age == age_targets).sum().item()\n","        total_predictions_age += age_targets.size(0)\n","\n","        # Compute correct predictions and update statistics for gender prediction\n","        _, predicted_gender = torch.max(voutput_gender, 1)\n","        correct_predictions_gender += (predicted_gender == gender_targets).sum().item()\n","        total_predictions_gender += gender_targets.size(0)\n","\n","    # Compute and return average validation loss, accuracy, precision, recall, and F1 score\n","    avg_vloss = running_loss / len(val_dataloader)\n","\n","    # Compute and return average training loss and accuracy for the epoch\n","    vaccuracy_class = correct_predictions_class / total_predictions_class\n","    vaccuracy_age = correct_predictions_age / total_predictions_age\n","    vaccuracy_gender = correct_predictions_gender / total_predictions_gender\n","\n","    # Compute precision, recall, F1 score\n","    precision_class = precision_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","    precision_gender = precision_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","    precision_age = precision_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","    recall_class = recall_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","    recall_gender = recall_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","    recall_age = recall_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","    f1_class = f1_score(class_targets.cpu(), predicted_class.cpu(), average=\"macro\", zero_division=0.0)\n","    f1_gender = f1_score(gender_targets.cpu(), predicted_gender.cpu(), average=\"macro\", zero_division=0.0)\n","    f1_age = f1_score(age_targets.cpu(), predicted_age.cpu(), average=\"macro\", zero_division=0.0)\n","\n","    # return avg_vloss, vaccuracy, precision, recall, f1\n","    metrics = ((vaccuracy_class, vaccuracy_age, vaccuracy_gender), (precision_class, precision_age, precision_gender), (recall_class, recall_age, recall_gender), (f1_class, f1_age, f1_gender))\n","\n","    return avg_vloss, metrics"]},{"cell_type":"markdown","metadata":{},"source":["# Training and validating model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:50:19.613891Z","iopub.status.busy":"2024-03-19T13:50:19.613594Z","iopub.status.idle":"2024-03-19T13:50:20.696077Z","shell.execute_reply":"2024-03-19T13:50:20.695141Z","shell.execute_reply.started":"2024-03-19T13:50:19.613864Z"},"trusted":true},"outputs":[],"source":["def select_model(model_type):\n","    if model_type == \"resnet18\":\n","        model = models.resnet18(weights=None, num_classes=3)\n","        model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"resnet34\":\n","        model = models.resnet50(weights=None, num_classes=3)\n","        model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"resnet50\":\n","        model = models.resnet50(weights=None, num_classes=3)\n","        model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"vgg_bn\":\n","        model = models.vgg16_bn(weights=None, num_classes=3)\n","        model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        return model\n","    elif model_type == \"multi_resnet\":\n","        model = MultiInputResNet(weights=None, num_classes=3, model_arch=model_arch)\n","        return model\n","    elif model_type == \"modified_multi_resnet\":\n","        model = Modified_MultiInputResNet(weights=None, num_classes=3, model_arch=model_arch)\n","        return model\n","    elif model_type == \"modified_multi_resnet_spectral\":\n","        model = MultiInputResNet_spectral(weights=None, num_classes=3, model_arch=model_arch)\n","        return model\n","\n","\n","# Initialize model\n","model = select_model(model_type)\n","\n","# Set the model to training mode and put it on GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device(\"cpu\")\n","print(f\"Device running on: {device}\")\n","\n","model.to(device)\n","\n","# Wrap model with DataParallel if multiple GPUs are available\n","if torch.cuda.device_count() > 1:\n","    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model)\n","else:\n","    print(\"Only 1 GPU available!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:50:20.698086Z","iopub.status.busy":"2024-03-19T13:50:20.697721Z","iopub.status.idle":"2024-03-19T13:52:01.048536Z","shell.execute_reply":"2024-03-19T13:52:01.046975Z","shell.execute_reply.started":"2024-03-19T13:50:20.698052Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Initialize training setup\n","optimizer, criterion = initialize_training_setup(model, optimizer_type, weighted)\n","scheduler = StepLR(optimizer, step_size=step, gamma=gamma)\n","best_vloss = float(\"inf\")\n","model_no = 0\n","\n","# Training and validation loop\n","for epoch in tqdm(range(epochs), total=epochs, leave=True, desc=f\"Epoch | \"):\n","    # avg_loss, accuracy = train_epoch(model, model_type, device, epoch, optimizer, criterion)\n","    # avg_loss, accuracy_class, accuracy_age, accuracy_gender = train_epoch_modified(model, device, epoch, optimizer, criterion)\n","    avg_loss, accuracy_class, accuracy_age, accuracy_gender = train_epoch_spectral(model, device, epoch, optimizer, criterion)\n","    # avg_vloss, vaccuracy, precision, recall, f1 = validate_epoch(model, device, epoch, criterion)\n","    # avg_vloss, metrics = validate_epoch_modified(model, device, epoch, criterion)\n","    avg_vloss, metrics = validate_epoch_spectral(model, device, epoch, criterion)\n","\n","    # Print and log metrics\n","    # MultiResnet model\n","    # print(f\"Epoch #{epoch+1} | Training Loss: {avg_loss:.4f} | Validation Loss: {avg_vloss:.4f} | Validation Accuracy: {vaccuracy:.4f}\")\n","    # print(f\"Epoch #{epoch+1} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n","    # print(f\"Epoch #{epoch+1} | Learning Rate: {scheduler.get_last_lr()}\")\n","\n","    # Extract metrics\n","    vaccuracy = metrics[0]\n","    precision = metrics[1]\n","    recall = metrics[2]\n","    f1 = metrics[3]\n","\n","    # ModifiedMultiResnet model\n","    print(f\"Epoch #{epoch+1} | Training Loss: {avg_loss:.4f} | Validation Loss: {avg_vloss:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class Accuracy: {vaccuracy[0]:.4f} | Age Accuracy: {vaccuracy[1]:.4f} | Gender Accuracy: {vaccuracy[2]:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class Precision: {precision[0]:.4f} | Precision: {precision[1]:.4f} | Precision: {precision[2]:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class Recall: {recall[0]:.4f} | Age Recall: {recall[1]:.4f} | Gender Recall: {recall[2]:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Class F1 score: {f1[0]:.4f} | Age F1 score: {f1[1]:.4f} | Gender F1 Score: {f1[2]:.4f}\")\n","    print(f\"Epoch #{epoch+1} | Learning Rate: {scheduler.get_last_lr()}\")\n","\n","    # Update learning rate\n","    scheduler.step()\n","\n","    # Log metrics to wandb\n","    # MultiResnet model\n","    # wandb.log({\n","    #        \"epoch\": epoch + 1,\n","    #        \"train_loss\": avg_loss,\n","    #        \"train_acc\": accuracy,\n","    #        \"val_loss\": avg_vloss,\n","    #        \"val_accuracy\": vaccuracy,\n","    #    })\n","    # wandb.log({\"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n","\n","    # ModifiedMultiResnet model\n","    wandb.log({\n","        \"epoch\": epoch + 1,\n","        \"train_loss\": avg_loss,\n","        \"class_accuracy\": vaccuracy[0],\n","        \"age_accuracy\": vaccuracy[1],\n","        \"gender_accuracy\": vaccuracy[2],\n","        \"class_precision\": precision[0],\n","        \"age_precision\": precision[1],\n","        \"gender_precision\": precision[2],\n","        \"class_recall\": recall[0],\n","        \"age_recall\": recall[1],\n","        \"gender_recall\": recall[2],\n","        \"class_f1_score\": f1[0],\n","        \"age_f1_score\": f1[1],\n","        \"gender_f1_score\": f1[2]\n","    })\n","\n","    # Track best performance, and save the model's state\n","    if avg_vloss < best_vloss:\n","        best_vloss = avg_vloss\n","        model_no += 1\n","        if not os.path.exists(\"models\"):\n","            os.makedirs(\"models\")\n","        model_path = f\"models/{model_output}_no_{model_no}_epoch_{epoch+1}.pth\"\n","        torch.save(model.state_dict(), model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:52:01.049632Z","iopub.status.idle":"2024-03-19T13:52:01.050084Z","shell.execute_reply":"2024-03-19T13:52:01.049864Z","shell.execute_reply.started":"2024-03-19T13:52:01.049844Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4560779,"sourceId":7791391,"sourceType":"datasetVersion"},{"datasetId":4561341,"sourceId":7792152,"sourceType":"datasetVersion"},{"datasetId":4609644,"sourceId":7858628,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
